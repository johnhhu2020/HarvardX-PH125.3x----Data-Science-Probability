---
title: 'HarvardX PH125.3x -- Data Science: Probability'
author: "John HHU"
date: '2022-05-30'
output: html_document
---






## Course  /  Section 1: Discrete Probability  /  Section 1 Overview


# Section 1 Overview


Section 1 introduces you to Discrete Probability. Section 1 is divided into three parts:

[][*        Introduction to Discrete Probability*]
            *A discrete probability distribution counts occurrences that have countable or finite outcomes.*
[][*        Combinations and Permutations*]
            *When the order doesn't matter, it is a Combination. When the order does matter it is a Permutation*]
[][*        Addition Rule and Monty Hall*]
            *https://www.varsitytutors.com/hotmath/hotmath_help/topics/addition-rule-of-probability*
            *https://www.mathgoodies.com/lessons/vol6/addition_rules*
            *Google Monty Hall problem*

After completing Section 1, you will be able to:

        apply basic probability theory to categorical data.
        perform a Monte Carlo simulation to approximate the results of repeating an experiment over and over, including simulating the outcomes in the Monty Hall problem.
        distinguish between: sampling with and without replacement, events that are and are not independent, and combinations and permutations.
        apply the multiplication and addition rules, as appropriate, to calculate the probably of multiple events occurring.
        use sapply() instead of a for loop to perform element-wise operations on a function.

There are 3 assignments that use the DataCamp platform for you to practice your coding skills. There are also some quick probability calculations for you to perform directly on the edX platform as well, and there is a longer set of problems at the end of section 1.

This section corresponds to the following section of the course textbook.
https://rafalab.github.io/dsbook/probability.html#discrete-probability

We encourage you to use R to interactively test out your answers and further your learning.







## Course  /  Section 1: Discrete Probability  /  1.1 Introduction to Discrete Probability


# Discrete Probability


We start by covering some basic principles related to categorical data.  This subset of probability is referred to as [][*discrete probability*].  ***It will help us understand the probability theory we will later introduce for numeric and continuous data, which is more common in data science applications***.  Discrete probability is more useful in card games and we use these as examples.  The word probability is used in everyday language.  For example, Google's auto complete of, what are the chances of, gives us getting pregnant, having twins, and rain tomorrow.  Answering questions about probability is often hard, if not impossible.  Here, we discuss a mathematical definition of probability that does permit us to give precise answers to certain questions.  

For example, if I have two red beads and three blue beads inside an urn and I pick one at random, what is the probability of picking a red one?  Our intuition tells us that the answer is 2/5, or 40%.  A precise definition can be given by noting that ***there are five possible outcomes of which two satisfy the condition necessary for the event "pick a red bead."***  Because each of the five outcomes has the same chance of occurring, we conclude that the probability is 0.4 for red and 0.6 for blue.  

[][*A more tangible way to think about the probability of an event is as a proportion of times the event occurs when we repeat the experiment over and over independently and under the same conditions*].  Before we continue, let's introduce some notation.  We use the notation probability of A to denote the probability of an event A happening.  We use the very general term event to refer to things that can happen when something happens by chance.  

For example, in our previous example, the event was picking a red bead.  In a political poll, in which we call 100 likely voters
at random, an example of an event is calling 48 Democrats and 52 Republicans.  [][***In data science applications, we will often deal with continuous variables*].  In these cases, events will often be things like, ***is this person taller than 6 feet?***  In this case, we write events in a more mathematical form.  For example, x greater than 6.  We'll see more of these examples later.  Here, we focus on categorical data and discrete probability.


[][Textbook link]

This video corresponds to the textbook section on discrete probability External link.
https://rafalab.github.io/dsbook/probability.html#discrete-probability


[][Key points]

    The probability of an event is the proportion of times the event occurs when we repeat the experiment independently under the same conditions.
    Pr(A) = probability of event A
    An event is defined as an outcome that can occur when when something happens by chance.
    We can determine probabilities related to discrete variables (picking a red bead, choosing 48 Democrats and 52 Republicans from 100 likely voters) and continuous variables (height over 6 feet).



![](C:/Users/qp/Pictures/discrete probability.png)

![](C:/Users/qp/Pictures/googles auto complete of what are the chance of ....png)

![](C:/Users/qp/Pictures/having rains tomorrow.png)

![](C:/Users/qp/Pictures/picking beads rnadomly from a urn.png)

![](C:/Users/qp/Pictures/a precise definition giving that there are 5 possible outcomes of which 2 satisfy the condition necessary.png)

![](C:/Users/qp/Pictures/A more tangible way to think about the probability of an event is as a condition of times the events occurs.png)

![](C:/Users/qp/Pictures/paobability of a notation.png)

![](C:/Users/qp/Pictures/term event to refering things can happen when something happens by chance.png)

![in data science we will often deal with continuous variables.png](C:/Users/qp/Pictures/in data science we will often deal with continuous variables.png)

![](C:/Users/qp/Pictures/events sample in data science dealing with continuous variable.png)

![](C:/Users/qp/Pictures/then we write it in a more mathematical form.png)









# Monte Carlo Simulations


Computers provide a way to actually perform the simple random experiments, such as the one we did before.  Pick a bead at random from a bag or an urn with 3 blue beads and 2 red ones.  [][*Random number generators*] permit us to mimic the process of picking at random.  An example in R is the sample() function.  We demonstrate its use showing you some code.  First, use the rep() function to generate the urn.  We create an urn with 2 red and 3 blues.  You can see when we type beads we see this.  Now, we can use a sample() function to pick one at random.  If we type sample beads comma 1, in this case, we get a blue.  This line of code produces one random outcome.  Now, we want to repeat this experiment over and over.  

However, it is, of course, impossible to repeat forever.  Instead, we repeat the experiment a large enough number of times to make the results practically equivalent to doing it over and over forever.  [][*This is an example of a Monte Carlo simulation*].  Note that much of what mathematical and theoretical statisticians study--something we do not cover in this course--relates to providing rigorous definitions of *practically equivalent*, as well as studying how close a large number of experiment gets us to what happens in the limit, the limit meaning if we did it forever.  [][********Later in this module, we provide a practical approach to deciding what is large enough********].  

To perform our first Monte Carlo simulation, we use the replicate() function.  This permits us to repeat the same task any number of times we want.  Here, we repeat the random event 10,000 times.  We set B to be 10,000, then we use the replicate() function to sample from the beads 10,000 times.  We can now see if, in fact, our definition is in agreement with this Monte Carlo simulation approximation.  *We can use table(), for example, to see the distribution.  And then we can use prop.table() to give us the proportions*.  And we see that, in fact, the Monte Carlo simulation gives a very good approximation with 0.5962 for blue and 0.4038 for red.  We didn't get exactly 0.6 and exactly 0.4, but statistical theory tells us that, if we make B large enough, we can get as close as we want to those numbers.  We just covered a simple and not very useful example of Monte Carlo simulations.  

But we will use Monte Carlo simulation to estimate probabilities in cases in which it is harder to compute the exact ones.  Before we go into more complex examples, we still use simple ones to demonstrate the computing tools available in R.  Let's start by noting that we don't actually have to use replicate() in this particular example.  This is because the function sample() has an argument that permits us to pick more than one element from the urn.  However, by default, this selection occurs without replacement.  After a bead is selected, it is not put back in the urn.  Note what happens when we ask to randomly select 5 beads.  Let's do it over and over again.  Let's do it three times.  This results in a rearrangement that always has three blue and two red beads.  If we asked for six beads, then we get an error.  It tells us you don't have enough beads in here to get six.  

This is because it's doing it without replacement.  However, this function, the sample function, can be used directly--again, without the replicate--to repeat the same experiment of picking 1 out of 5 beads over and over under the same conditions.  To do this, we sample with replacement.  After we pick the bead we put it back in the urn.  We can tell sample to do this by changing the *replace = * argument which defaults to false to true.  We do it like this.  And when we do this, we see that we get very similar answers to what  we got using the replicate function.  


[][Textbook link]

This video corresponds to the textbook section on Monte Carlo simulations.
https://rafalab.github.io/dsbook/probability.html#monte-carlo-simulations


[][Key points]

    Monte Carlo simulations model the probability of different outcomes by repeating a random process a large enough number of times that the results are similar to what would be observed if the process were repeated forever.

    The sample() function draws random outcomes from a set of options.
    The replicate() function repeats lines of code a set number of times. It is used with sample() and similar functions to run Monte Carlo simulations.

Video code

Note that your exact outcome values from the Monte Carlo simulation will differ because the sampling is random.

beads <- rep(c("red", "blue"), times = c(2,3))    # create an urn with 2 red, 3 blue
beads    # view beads object
sample(beads, 1)    # sample 1 bead at random

B <- 10000    # number of times to draw 1 bead
events <- replicate(B, sample(beads, 1))    # draw 1 bead, B times
tab <- table(events)    # make a table of outcome counts
tab    # view count table
prop.table(tab)    # view table of outcome proportions



```{r}
beads = rep(c("red", "blue"), times = c(2, 3) )


sample(beads, 100, replace = T)


tab = table(sample(beads, 10000, replace = T))
prop.table(tab)
```



![](C:/Users/qp/Pictures/computer provides us the random number enerators.png)

![](C:/Users/qp/Pictures/sample function in r helps us apply a random experiment.png)

![](C:/Users/qp/Pictures/use rep function to create a urn includes 2 red and 3 blue.png)


![](C:/Users/qp/Pictures/and now we can produce 1 random outcome.png)

![](C:/Users/qp/Pictures/monte carlo simulation.png)

![](C:/Users/qp/Pictures/replicate function premits us to repeat same task any number of time we want.png)

![](C:/Users/qp/Pictures/now we repeat the random event 10 thousand times and check the outcome.png)

![](C:/Users/qp/Pictures/we can use table function to see the distribution.png)

![](C:/Users/qp/Pictures/and use the prop.table function to see the proportions.png)

![](C:/Users/qp/Pictures/default sample function repeat offers us without replacement experiment.png)

![](C:/Users/qp/Pictures/sample one by one and we get this, without replacement.png)

![](C:/Users/qp/Pictures/and now we don't have enough beads in urn.png)

![](C:/Users/qp/Pictures/now check what happens on your own what happends if we change default replace arg from FALSE to T.png)









# Setting the Random Seed


The set.seed() function

Before we continue, we will briefly explain the following important line of code:

set.seed(1986) 

Throughout this book, we use random number generators. This implies that many of the results presented can actually change by chance, which then suggests that a frozen version of the book may show a different result than what you obtain when you try to code as shown in the book. This is actually fine since the results are random and change from time to time. However, if you want to to ensure that results are exactly the same every time you run them, you can set R’s random number generation seed to a specific number. Above we set it to 1986. We want to avoid using the same seed every time. A popular way to pick the seed is the year - month - day. For example, we picked 1986 on December 20, 2018:  2018 − 12 − 20 = 1986.

You can learn more about setting the seed by looking at the documentation:

?set.seed

In the exercises, we may ask you to set the seed to assure that the results you obtain are exactly what we expect them to be.
Important note on seeds in R 3.5 versus R 3.6 and later

When R updated to version 3.6 in early 2019, the default method for setting the seed changed. This means that exercises, videos, textbook excerpts and other code you encounter online may yield a different result based on your version of R.

[][*If you are running R 3.6 or later, you can revert to the original seed setting behavior by adding the argument sample.kind="Rounding". For example:*]


set.seed(1)
set.seed(1, sample.kind="Rounding")    # will make R 3.6 generate a seed as in R 3.5

Using the sample.kind="Rounding" argument will generate a message:

**non-uniform 'Rounding' sampler used**

This is not a warning or a cause for alarm - it is a confirmation that R is using the alternate seed generation method, and you should expect to receive this message in your console.

If you use R 3.6 or later, you should always use the second form of set.seed() in this course series (outside of DataCamp assignments). Failure to do so may result in an otherwise correct answer being rejected by the grader. In most cases where a seed is required, you will be reminded of this fact.


```{r}
set.seed(1986, sample.kind="Rounding")
```







# Using the mean Function for Probability


An important application of the mean() function

[][*In R, applying the mean() function to a logical vector returns the proportion of elements that are TRUE*]. It is very common to use the mean() function in this way to calculate probabilities and we will do so throughout the course.

Suppose you have the vector beads from a previous video:

beads <- rep(c("red", "blue"), times = c(2,3))
beads
[1] "red" "red" "blue" "blue" "blue"

To find the probability of drawing a blue bead at random, you can run:

mean(beads == "blue")
[1] 0.6

This code is broken down into steps inside R. First, R evaluates the logical statement beads == "blue", which generates the vector:

FALSE FALSE TRUE TRUE TRUE

When the mean function is applied, R coerces the logical values to numeric values, changing TRUE to 1 and FALSE to 0:

0 0 1 1 1

The mean of the zeros and ones thus gives the proportion of TRUE values. As we have learned and will continue to see, probabilities are directly related to the proportion of events that satisfy a requirement.






# Probability Distributions


[][***Defining a distribution for categorical outcomes is relatively straight forward***].  We simply assign a probability to each category.  In cases that can be thought of as beads in an urn, for each bead type, the proportion defines the distribution.  Another example comes from polling.  If you're are randomly calling likely voters from a population that has 44% Democrat, 44% Republican, 10% undecided, and 2% green, these proportions define the probability for each group.  For this example, the probability distribution is simply these four proportions.  

Again, categorical data makes it easy to define probability distributions.  However, later in applications that are more common in data science,  we will learn about probability distributions for continuous variables.  In this case, it'll get a little bit more complex.  But for now, we're going to stick to discrete probabilities before we move on.  


[][Textbook link]

This video corresponds to the textbook section on probability distributions.
https://rafalab.github.io/dsbook/probability.html#discrete-probability-distributions



[][Key points]

    The probability distribution for a variable describes the probability of observing each possible outcome.
    For discrete categorical variables, the probability distribution is defined by the proportions for each group.



![for each bead type, the proportion defines the distribution](C:/Users/qp/Pictures/beads in a urn for each bead type.png)

![categorical data makes it easy to define probability distribution](C:/Users/qp/Pictures/this proportion defines the probability for each group.png)

(  *A discrete probability distribution counts occurrences that have countable or finite outcomes. This is in contrast to a continuous distribution, where outcomes can fall anywhere on a continuum. Common examples of discrete distribution include the binomial, Poisson, and Bernoulli distributions.*  )









# Independence


We say that two events are independent if the outcome of one does not affect the other.  This classic example are coin tosses.  Every time we toss a fair coin, the probability of seeing heads is one half, regardless of what previous tosses have revealed.  The same is true when we pick beads from an urn, with replacement.  In the example we saw earlier, the probability of red was 0.40, regardless of previous draws.  Many examples of events that are not independent come from card games.  When we deal the first card, the probability of getting, say a King, is 1 in 13.  This is because there are 13 possibilities.  You can get an ace, a two, a three, a four, et cetera, 10, Jack, Queen, or King.  Now, if we deal a King for the first card, and I don't replace it, then the probability of getting a King in the second card is less, because there are only three Kings left.  The probability is 3 out of not 52, because we already dealt one card, but out of 51.  These events are, therefore, not independent.  

[][*The first outcome affects the second*].  To see an extreme case of non-independent events, consider an example of drawing five beads at random, without replacement, from an urn.  Three are blue, two are red.  I'm going to generate data like this using the sample() function and assign it to x.  You can't see the outcomes.  Now, if I ask you to guess the color of the first bead, what do you guess?  Since there's more blue beads, there's actually a 0.6 chance of seeing blue.  That's probably what you guess.  **But now I'm going to show you the outcomes of the other four**.  The second, third, fourth, and fifth outcomes you can see here.  You can see that the three blue beads have already come out.  This affects the probability of the first.  They are not independent.  So would you still guess blue?  Of course not.  Now you know that the probability of red is 1.  

These events are not independent.  The probabilities change once you see the other outcomes.  When events are not independent, conditional probabilities are useful and necessary to make correct calculations.  We already saw an example of a conditional probability.  We computed the probability that a second dealt card is a King, given that the first was a King.  In probability, we use the following notation.  We use this dash like this as a shorthand for given that or conditional on, these are synonyms.  Note that, [][*when two events, say A and B, are independent, we have the following equation.  The probability of A given B is equal to the probability of A*].  It doesn't matter what B is.  The probability A is unchanged.  This is the mathematical way of saying it.  And in fact, this can be considered the mathematical definition of independence.  

All right now, if we want to know the probability of two events, say A and B, occurring, we can use the multiplication rule.  **So the probability of A and B is equal to the probability of A multiplied by the probability of B, given that A already happened**.  Let's use blackjack as an example.  In blackjack, you get assigned to random cards, without replacement.  Then you can ask for more.  The goal is to get closer to 21 than the dealer, without going over.  Face cards are worth 10 points, so is the 10 card, that's worth 10 points too.  And aces are worth either 11 or 1.  So if you get an ace and a face card, you win automatically.  

So, in blackjack, to calculate the chances of getting 21 in the following way, first we get an ace.  And then we get a face card or a 10.  We compute the probability of the first being an ace.  And then multiply by the probability of a face card or a 10, given that the first card was an ace.  The calculation is 1 over 13 chance of getting an ace, times chance of getting a card with value 10, given that we already saw an ace, which is 16 out of 51.  We've already taken one card out.  This is approximately 2%.  The multiplicative rule also applies to more than two events.  We can use induction to expand for more than two.  So the probability of A and B and C is equal to the probability of A times our probability of B, given that A happen, times the probability of C, that A and B happen.  

When we have independent events, the multiplication rule becomes simpler.  We simply multiply of three probabilities.  But we have to be very careful when we use the multiplicative rule in practice.  We're assuming independence.  And this can result in very different and incorrect probability calculations when we don't actually have independence.  This can have dire consequences.  For example, in a trial, if an expert doesn't really know the multiplication rule and how to use it.  So let's use an example.  This is loosely based on something that actually happened.  Imagine a court case in which the suspect was described to have a mustache and a beard.  And the prosecution brings in an expert to argue that because 1 in 10 men have beards, and 1 in 5 men has mustaches, using the multiplication rule, this means that only 2% of men have both beards and mustaches, 1/10 times 1/5.  2% is a pretty unlikely event.  However, to multiply like this, we need to assume independence.  And in this case, it's clearly not true.  The conditional probability of a man having a mustache, conditional on them having a beard, is quite high.  It's about 95%.  So the correct calculation actually gives us a much higher probability.  It's 9%, so there's definitely reasonable doubt.  


[][Textbook link]

This video corresponds to the textbook section on independence, conditional probability and the multiplication rule.
https://rafalab.github.io/dsbook/probability.html#independence



[][Key points]

    Conditional probabilities compute the probability that an event occurs given information about dependent events. For example, the probability of drawing a second king given that the first draw is a king is:
    
[][       Pr(Card 2 is kind | Card 1 is king) = 3/51]
    
[][       If two events A and B are independent, Pr(A|B) = Pr(A)]
    .
    To determine the probability of multiple events occurring, we use the multiplication rule.

Equations


The **multiplication rule for independent events** is:

[][       Pr(A and B and C) = Pr(A) x Pr(B) x Pr(c)]

The multiplication rule for dependent events considers the conditional probability of both events occurring:

[][       Pr(A and B) = Pr(A) x Pr(B|A)]

We can expand the multiplication rule for dependent events to more than 2 events:

[][       Pr(A and B and C) = Pr(A) x Pr(B|A) x Pr(C| A and B)]



![](C:/Users/qp/Pictures/definition of independent.png)

![](C:/Users/qp/Pictures/toss coin is a example of independent.png)

![with replacement](C:/Users/qp/Pictures/pick bead from urn is also a independent.png)

![The first outcome affects the second](C:/Users/qp/Pictures/not independent event example are card games.png)

![not independent events are without replacement card games.png](C:/Users/qp/Pictures/not independent events are without replacement card games.png)

![compute the probability that the second dealt card is king, given that the first was king](C:/Users/qp/Pictures/when events are not independent, conditional probabilities are useful and necessary.png)

![](C:/Users/qp/Pictures/in probability we use following notation to represent conditional probability.png)

![](C:/Users/qp/Pictures/notice when 2 events are independent, we have following equation.png)

![](C:/Users/qp/Pictures/applies the multiplication rule to calculate the probability of 2 events occurring.png)

![so the probability of a and b is equal to the probability of a multiplied by the probability of b given that a already happened.png](C:/Users/qp/Pictures/so the probability of a and b is equal to the probability of a multiplied by the probability of b given that a already happened.png)

![](C:/Users/qp/Pictures/blackjack game winning rate is you first get ace card then get 10 or face card.png)

![](C:/Users/qp/Pictures/the multiplicative rule also applies to more then two events.png)

![](C:/Users/qp/Pictures/for independent events, the multiplication rule becomes simpler.png)

![](C:/Users/qp/Pictures/how to know if the events are independent or not.png)

![](C:/Users/qp/Pictures/having mustache and beard.png)

![](C:/Users/qp/Pictures/1 in 10 men has beard and 1 in 5 men has mustache.png)

![assume independence, not true at all.png](C:/Users/qp/Pictures/assume independence, not true at all.png)

![](C:/Users/qp/Pictures/The conditional probability of a man having a mustache, conditional on them.png)



# Question from last video about beard and mustaches?

question posted 2 days ago by john_hhu2020

Sorry I don't get it, I thought it was 95%, since the instructor mentioned below in the video:

    Pr(mustache | beard) = 0.95

I don't know how does this comes out (this answer or the answer on the book, since it mentioned this: "1/10×95/100=0.095" under this hypothesis: "Say the conditional probability of a man having a mustache conditional on him having a beard is .95"). Say if we applies the multiplicative rule here:

    Pr(m and B) = Pr(m) x Pr(b|m)

and since we have 10 sample, Pr(m) = 1/5, Pr(b) = 1/10, then what is the probability of Pr(b|m)? And whats the difference between Pr(b and m) and Pr(b|m), as I cant image the situation of these.

The question is: in this case, what is the difference between Pr(b and m) and Pr(b|m) ???
This post is visible to everyone.
0 responses

    john_hhu2020

    less than a minute ago

# =================================================================================================================================
I see, so the conditionally probability means a situation change: both Numerator and Denominator was changed due to the first event have happened.
Thus the Pr(mustache | beard) must be given, and only then we can calculate the Pr(mustache and beard), and this Pr(mustache and beard) means the probability of guy met both conditions in the first condition - the Numerator and Denominator haven't changed. Anyone willing to help please write something, especially the instructor









# Assessment: Introduction to Discrete Probability



Assessment due Jun 15, 2022 10:43 AWST


## Probability of cyan
1/1 point (graded)

One ball will be drawn at random from a box containing: 3 cyan balls, 5 magenta balls, and 7 yellow balls.
What is the probability that the ball will be cyan?
correct

Loading
You have used 1 of 5 attempts Some problems have options such as save, reset, hints, or show answer. These options follow the Submit button.
Correct (1/1 point)


## Probability of not cyan
1/1 point (graded)

One ball will be drawn at random from a box containing: 3 cyan balls, 5 magenta balls, and 7 yellow balls.
What is the probability that the ball will not be cyan?
correct

Loading
You have used 1 of 5 attempts Some problems have options such as save, reset, hints, or show answer. These options follow the Submit button.
Correct (1/1 point)


## Sampling without replacement
1/1 point (graded)

Instead of taking just one draw, consider taking two draws. You take the second draw without returning the first draw to the box. We call this sampling without replacement.
What is the probability that the first draw is cyan and that the second draw is not cyan?

Provide at least 3 significant digits.
correct

Loading
You have used 1 of 5 attempts Some problems have options such as save, reset, hints, or show answer. These options follow the Submit button.
Correct (1/1 point)


## Sampling with replacement
1/1 point (graded)

Now repeat the experiment, but this time, after taking the first draw and recording the color, return it back to the box and shake the box. We call this sampling with replacement.
What is the probability that the first draw is cyan and that the second draw is not cyan?
correct

Loading
You have used 1 of 5 attempts Some problems have options such as save, reset, hints, or show answer. These options follow the Submit button.
Correct (1/1 point)

Have a question about these assessments? Search the discussion forum BEFORE posting below.

Some reminders:

    Please be specific in the title and body of your post regarding which question you're asking about to facilitate answering your question.
    Posting snippets of code is okay, but posting full code solutions is not.
    If you do post snippets of code, please format it as code for readability. If you're not sure how to do this, there are instructions in a pinned post in the discussion forum.

Discussion: Assessment: Introduction to discrete probability'
Topic: Section 1 / Assessment: Introduction to discrete probability






a box containing: 3 cyan balls, 5 magenta balls, and 7 yellow balls. 

# What is the probability that the first draw is cyan and that the second draw is not cyan?

Pr(A) Cyan

Pr(B) not Cyan


Pr(A and B) = Pr(A) x Pr(B|A)
            = 3/15 x 12/14
            = 1/5 x 6/7
            = 6/35


3/15 x 12/15
1/5 x 4/5
4/25









# Introduction to DataCamp


DataCamp

You are about the take the first DataCamp assessment. In this course we will be using the DataCamp platform for some assessments. DataCamp provides an R console and and a script editor right here on your browser. Here we give a brief DataCamp tutorial. If you are already familiar with DataCamp you can skip this section and proceed to the next section

To start a DataCamp assessment, you will click on the button that says [][Click here to start the assessment], which looks like this: *Click here to start the assessment* picture of start the assessment button. You will see a button like this one in the next section: [][*Assessment: Introduction to Discrete Probability*].

The DataCamp interface has four panels. They are:

    The Information Panel: General information about the assessment.
    The Instructions Panel: Exercise instructions. The multiple choice questions appear here when applicable.
    The Editor: Here is where you type and edit your answers in the form of an R script. Example code also appears here. The editor also includes reminders of the instructions.  Note that # denotes comments. These are not run as code, instead, they tell others what your code is about! 
    R console: This is where R commands get executed. You can send commands from the editor to the console but you can also type in commands directly to test out code.

Here is a screenshot of what DataCamp looks like:

Image of DataCamp Platform

![](C:/Users/qp/Pictures/datacamp interface.png)

There are two ways to send commands from the editor to the console:

1) If you hit the  Submit Answer button, the entire code in the editor gets executed and your answer is evaluated. Remember, after you click Submit Answer in an assessment, your code will be evaluated. If you do not take the hint, you get unlimited tries.

2) If your cursor is on the editor and you hit command-return on a Mac or control-return on Windows, that line gets executed in the console. You do not submit an answer when you do this. This is a good way to test your script before you submit.

Tip: DataCamp suggests useful keyboard shortcuts after most exercises.

![](C:/Users/qp/Pictures/datacamp exercise pannel.png)

After submit answer on the platform










# DataCamp Assessment: Introduction to Discrete Probability



Assessment due Jun 15, 2022 10:43 AWST

This assessment covers the basics of discrete probability. There are two parts to this assessment: the first part is a set of questions on the edX platform, and the second part is a set of coding questions on the DataCamp platform.

In this assessment, you will learn about the fundamentals of discrete probability through looking at examples of sampling from an urn with and without replacement.

By clicking OK, you agree to DataCamp's privacy policy: https://www.datacamp.com/privacy-policy. Note that you might need to disable your pop-up blocker, or allow "www.datacamp.com" in your pop-up blocker allowed list. When you have completed the exercises, return to edX to continue your learning.

**Assessment: Introduction to discrete probability (External resource) (4.5 points possible)**
By clicking OK, you agree to DataCamp's privacy policy: https://www.datacamp.com/privacy-policy.
https://courses.edx.org/courses/course-v1:HarvardX+PH125.3x+1T2022/xblock/block-v1:HarvardX+PH125.3x+1T2022+type@lti_consumer+block@f8d6dedea9284f2bb67b368dc05d0a38/handler/lti_launch_handler
You must be logged in to DataCamp to view your lab scores. To log in, simply click on one of the launch links above (even if you're not planning to work on a lab right now, but just want to view your scores). Then, refresh this page to view your scores.

 

Have a question about this assessment? Search the discussion forum to see if someone else has asked or answered your question BEFORE posting below.

Some reminders:

    Please be specific in the title and body of your post regarding which question you're asking about to facilitate answering your question.
    Posting snippets of code is okay, but posting full code solutions is not.
    If you do post snippets of code, please format it as code for readability. If you're not sure how to do this, there are instructions in a pinned post in the general discussion forum.

Discussion: DataCamp Assessment: Introduction to discrete probability
Topic: Section 1 / DataCamp Assessment: Introduction to discrete probability



## Exercise 1. Probability of cyan - generalized

In the edX exercises for this section, we calculated some probabilities by hand. Now we'll calculate those probabilities using R.

One ball will be drawn at random from a box containing: 3 cyan balls, 5 magenta balls, and 7 yellow balls.

What is the probability that the ball will be cyan?
Instructions
70 XP

    Define a variable p as the probability of choosing a cyan ball from the box.
    Print the value of p.

Hint

    Calculate the proportion of balls in the box that are cyan and assign that value to the variable p.
    To print the contents of a variable, write the variable name in a new line of code.

```{r}
cyan <- 3
magenta <- 5
yellow <- 7

# Assign a variable `p` as the probability of choosing a cyan ball from the box
#p <- rep(c("cyan", "magenta", "yellow"), times=c(3, 5, 7))
p <- cyan/(cyan + magenta + yellow)

# Print the variable `p` to the console
p
```



## Exercise 2. Probability of not cyan - generalized

We defined the variable p as the probability of choosing a cyan ball from a box containing: 3 cyan balls, 5 magenta balls, and 7 yellow balls.

What is the probability that the ball you draw from the box will NOT be cyan?
Instructions
100 XP

    Using the probability of choosing a cyan ball, p, calculate the probability of choosing any other ball.

```{r}
# `p` is defined as the probability of choosing a cyan ball from a box containing: 3 cyan balls, 5 magenta balls, and 7 yellow balls.
# Using variable `p`, calculate the probability of choosing any ball that is not cyan from the box

1-p
```


## Exercise 3. Sampling without replacement - generalized

Instead of taking just one draw, consider taking two draws. You take the second draw without returning the first draw to the box. We call this sampling without replacement.

What is the probability that the first draw is cyan and that the second draw is not cyan?
Instructions
100 XP

    Calculate the conditional probability p_2 of choosing a ball that is not cyan after one cyan ball has been removed from the box.
    Calculate the joint probability of both choosing a cyan ball on the first draw and a ball that is not cyan on the second draw using p_1 and p_2.

```{r}
cyan <- 3
magenta <- 5
yellow <- 7

# The variable `p_1` is the probability of choosing a cyan ball from the box on the first draw.
p_1 <- cyan / (cyan + magenta + yellow)

# Assign a variable `p_2` as the probability of not choosing a cyan ball on the second draw without replacement.

# Calculate the probability that the first draw is cyan and the second draw is not cyan using `p_1` and `p_2`.



# The variable `p_1` is the probability of choosing a cyan ball from the box on the first draw.
p_1 <- cyan / (cyan + magenta + yellow)
p_1

# Assign a variable `p_2` as the probability of not choosing a cyan ball on the second draw without replacement.
p_2 <- (magenta + yellow)/(cyan + magenta + yellow - 1)
p_2

# Calculate the probability that the first draw is cyan and the second draw is not cyan using `p_1` and `p_2`.
p_1 * p_2
```


## Exercise 4. Sampling with replacement - generalized

Now repeat the experiment, but this time, after taking the first draw and recording the color, return it back to the box and shake the box. We call this sampling with replacement.

What is the probability that the first draw is cyan and that the second draw is not cyan?
Instructions
100 XP

    Calculate the probability p_2 of choosing a ball that is not cyan on the second draw, with replacement.
    Next, use p_1 and p_2 to calculate the probability of choosing a cyan ball on the first draw and a ball that is not cyan on the second draw (after replacing the first ball).

```{r}
cyan <- 3
magenta <- 5
yellow <- 7

# The variable 'p_1' is the probability of choosing a cyan ball from the box on the first draw.
p_1 <- cyan / (cyan + magenta + yellow)

# Assign a variable 'p_2' as the probability of not choosing a cyan ball on the second draw with replacement.
p_2 <- 1- p_1

# Calculate the probability that the first draw is cyan and the second draw is not cyan using `p_1` and `p_2`.
p_1 * p_2

```
```{r}
# Solution =====================================================================================================================
cyan <- 3
magenta <- 5
yellow <- 7

# The variable 'p_1' is the probability of choosing a cyan ball from the box on the first draw.
p_1 <- cyan / (cyan + magenta + yellow)

# Assign a variable 'p_2' as the probability of not choosing a cyan ball on the second draw with replacement.
p_2 <- 1 - (cyan) / (cyan + magenta + yellow)

# Calculate the probability that the first draw is cyan and the second draw is not cyan using `p_1` and `p_2`.
p_1 * p_2
```



## End of Assessment: Introduction to Discrete Probability

This is the end of the programming assignment for this section. Please DO NOT click through to additional assessments from this page. Please DO answer the question on this page. If you do click through, your scores may NOT be recorded.

Click on "Awesome" to get the "points" for this question and then return to the course on edX.

You can close this window and return to Data Science: Probability.
Answer the question
50XP
Possible Answers

    Awesome
    press
    1
    Nope
    press
    2









## Course  /  Section 1: Discrete Probability  /  1.2 Combinations and Permutations


# Combinations and Permutations


In our very first example, we imagine an urn with 5 beads--3 blue, 2 red.  To review, to compute the probability distribution of 1 draw, we simply listed out all the possibilities-- there were 5--and then for each event, we counted how many of these possibilities were associated with that event.  So for example, for the blue beads the probability is 0.6.  **For more complicated examples, however, these computations are not necessarily straightforward**.  [][*For example, what does the probability that if I draw 5 cards without replacement, I get all cards of the same suit, what is called a flush in poker?*]  

Discrete probability teaches us how to make these computations using mathematics.  Here we focus on how to use R code.  So we're going to use card games as examples.  So let's start by constructing a deck of cards using R.  For this, we will use the function [][expand.grid()] and the function [][Paste()].  We *use Paste to create strings by joining smaller strings*.  For example, if we have the number and the suit for a card, in 2 different variables we can create the card name using Paste like this.  It also works on pairs of vectors.  It performs the operation element-wise.  So if we type this line of code, we get the following result.  *The function expand.grid gives us all the combinations of 2 lists* ([][*the outcome of expand.grid() function is a data.frame, be careful with this*]).  So for example, if you have blue and black pants and white, gray, and plaid shirt, all your combinations can be computed using the expand.grid function like this.  You can see all 6 combinations.  

So here's how we generate a deck of cards.  We define the four suits, we define the 13 numbers, and then we create the deck using expand.grid() and then pasting together the 2 columns that expand.grid() creates.  Now that we have a deck constructed, we can now start answering questions about probability.  Let's start by doing something simple.  *Let's double-check that the probability of a king in the first card is 1 in 13 with R code*.  We simply [][*compute the proportion of possible outcomes that satisfy our condition*].  So we create a vector that contains the four ways we can get a king.  That's going to be the kings variable.  And then we simply check what proportion of the deck is one of these cards and we get the answer that we expect--0.076 dot dot dot, which is 1 in 13.  

Now, ******how about the conditional probability of the second card being a king, given that the first was a king?******  Earlier we deduced that if 1 king is already out, then there's 51 left.  So the probability is 3 in 51.  But let's confirm by listing out all possible outcomes.  To do this, we're going to use [][the combinations() and permutations() functions that are available from the gtools package].  The permutations function computes for any list of size n all the different ways we can select R items.  So here's an example--here all the ways we can choose 2 numbers from the list 1, 2, 3, 4, 5.  *Notice that the order matters with the permutations() function*.  So 3, 1 is different than 1, 3, So it appears in our permutations.  Also notice that 1, 1; 2, 2; and 3, 3 don't appear, because once we pick a number, it can't appear again (no replacement).  Optionally for this function permutations, we can add a vector.  So for example, if you want to see 5 random 7-digit phone numbers out of all possible phone numbers, you could type code like this.  Here we're defining a vector of digits that goes from 0 to 9 rather than 1 through 10.  So these four lines of code generate all phone numbers, picks 5 at random, and then shows them to you.  

***To compute all possible ways that we can choose 2 cards when the order matters***, we simply type the following piece of code.  [][**Here we use permutations() function.  There's 52 cards, we're going to choose 2, and we're going to select them out of the vector that includes our card names, which we called deck earlier**].  This is going to be a matrix with 2 dimensions, 2 columns, and in this case, it's going to have 2,652 rows (*possibilities*).  Those are all the permutations.  Now, we're going to define the first card and the second card by grabbing the first and second columns using this simple piece of code.  And now we can, *for example, check how many cases have a first card that is a king--that's 204*.  [][***And now to find the conditional probability, we ask what fraction of these 204 have also a king in the second card***].  So this case we type the following piece of code.  We add all the cases that have king in the first, king in the second,  and divide by the cases that have a king in the first.  
# ======================================== Read it, Read it, Read it ================================================================================

And now we get the answer 0.058 dot dot dot, which is exactly 3 out of 51, which we had already deduced.  Note that the code we just saw is equivalent to this piece of code where we compute the proportions instead of the totals.  And this also gives us the answer that we want, 3 out of 51.  This is an R version of the multiplication rule, which tells us the probability of B, given A, is equal to proportion of A and B, or the probability of A and B, divided by the proportion of A or the probability of A.  [][*Now, what if the order does not matter?*]  For example, in blackjack, if you get an ace and a face card or a 10, it's called a natural 21, and you win automatically.  If we want to compute the probability of this happening, we want to enumerate the combinations, not permutations,  since the order doesn't matter.  So if we get an A and a king, king and an A, it's still a 21.  We don't want to count that twice.  So **notice the difference between the permutations functions**, which lists all permutations, and the combination function, where order does not matter.  

This means that 2, 1 doesn't appear because 1, 2 already appeared.  Similarly, 3, 1 and 3, 2 don't appear.  So to compute the probability of a natural 21 in blackjack, we can do this.  We can define a vector that includes all the aces, a vector that includes all the face cards, then we generate all the combinations of picking 2 cards out of 52, and then we simply count.  [][*How often do we get aces and a face card?*]  And we get the answer 0.048 dot, dot, dot.  Now, notice that in the previous piece of code we assumed that the aces come first.  This is only because we know the way that combination generates and enumerates possibilities.  But if we want to be safe, we can instead type this code, which considers both possibilities.  We get the same answer, and again, this is because we know how combinations works and how it lists the possibilities.  

Instead of using combinations to deduce the exact probability of a natural 21, **we can also use a Monte Carlo to estimate this probability**.  In this case, we draw two cards over and over and keep track of how many 21's we get.  We can use the function sample to draw a card without a replacement like this.  Here's 1 hand.  We didn't get a 21 there.  And then check if 1 card is an ace and the other is a face card or a 10.  Now we simply repeat this over and over and we get a very good approximation--in this case, 0.0488.



[][Textbook link]

Here is a link to the textbook section on combinations and permutations External link.
https://rafalab.github.io/dsbook/probability.html#combinations-and-permutations


[][Key points]

    paste() joins two strings and inserts a space in between.
    expand.grid() gives the combinations of 2 vectors or lists.
    permutations(n,r) from the gtools package lists the different ways that r items can be selected from a set of n options when order matters.
    combinations(n,r) from the gtools package lists the different ways that r items can be selected from a set of n options when order does not matter.

Code: Introducing paste() and expand.grid()

# joining strings with paste
number <- "Three"
suit <- "Hearts"
paste(number, suit)

# joining vectors element-wise with paste
paste(letters[1:5], as.character(1:5))

# generating combinations of 2 vectors with expand.grid
expand.grid(pants = c("blue", "black"), shirt = c("white", "grey", "plaid"))

Code: Generating a deck of cards

suits <- c("Diamonds", "Clubs", "Hearts", "Spades")
numbers <- c("Ace", "Deuce", "Three", "Four", "Five", "Six", "Seven", "Eight", "Nine", "Ten", "Jack", "Queen", "King")
deck <- expand.grid(number = numbers, suit = suits)
deck <- paste(deck$number, deck$suit)

# probability of drawing a king
kings <- paste("King", suits)
mean(deck %in% kings)

Code: Permutations and combinations

Correction: The code shown does not generate all 7 digit phone numbers because phone numbers can have repeated digits. It generates all possible 7 digit numbers without repeats.

library(gtools)
permutations(5,2)    # ways to choose 2 numbers in order from 1:5
all_phone_numbers <- permutations(10, 7, v = 0:9)
n <- nrow(all_phone_numbers)
index <- sample(n, 5)
all_phone_numbers[index,]

permutations(3,2)    # order matters
combinations(3,2)    # order does not matter

Code: Probability of drawing a second king given that one king is drawn

hands <- permutations(52,2, v = deck)
first_card <- hands[,1]
second_card <- hands[,2]
sum(first_card %in% kings)

sum(first_card %in% kings & second_card %in% kings) / sum(first_card %in% kings)

Code: Probability of a natural 21 in blackjack

aces <- paste("Ace", suits)
facecard <- c("King", "Queen", "Jack", "Ten")
facecard <- expand.grid(number = facecard, suit = suits)
facecard <- paste(facecard$number, facecard$suit)

hands <- combinations(52, 2, v=deck) # all possible hands

# probability of a natural 21 given that the ace is listed first in `combinations`
mean(hands[,1] %in% aces & hands[,2] %in% facecard)

# probability of a natural 21 checking for both ace first and ace second
mean((hands[,1] %in% aces & hands[,2] %in% facecard)|(hands[,2] %in% aces & hands[,1] %in% facecard))

Code: Monte Carlo simulation of natural 21 in blackjack

Note that your exact values will differ because the process is random and the seed is not set.

# code for one hand of blackjack
hand <- sample(deck, 2)
hand

# code for B=10,000 hands of blackjack
B <- 10000
results <- replicate(B, {
    hand <- sample(deck, 2)
    (hand[1] %in% aces & hand[2] %in% facecard) | (hand[2] %in% aces & hand[1] %in% facecard)
})
mean(results)




```{r}
number <- "Three"
suit <- "Hearts"

paste(number, suit)



paste(letters[1:5], as.character(1:5))
```

```{r}
expand.grid(pants = c("blue", "black"), shirt = c("white", "gray", "plaid"))
```

# This is cool, we can image what we want and applying our thought into code, R or Python, no difference
# ===================================================================================================================================
```{r}
suits <- c("Diamonds", "Clubs", "Hearts", "Spades")
numbers <- c("Ace", "Deuce", "Three", "Four", "Five", "Six", "Seven", "Eight", "Nine", "Ten", "Jack", "Queen", "King")


deck <- expand.grid(suit = suits, number = numbers)
length(deck[, 1])
head(deck)

deck <- paste(deck$suit, deck$number)
length(deck)


deck
```

```{r}
kings <- paste(suits, "King")

kings


mean(deck %in% kings)

#mean(length(kings)/length(deck))


# this is how I think calculating probability of second card being king given that the first card is already a king
# ============================================================================================================================
mean((length(kings)-1)/(length(deck)-1))
```

```{r}
library(gtools)


head(permutations(5, 2))
length(permutations(52, 2)[, 1])    # Dont forget after permutation or combinations, you'll have two dimensional df =================================
length(combinations(52, 2)[, 1])

data.frame(permutations(52, 2, v = deck))


all_phone_numbers <- permutations(10, 7, v=0:9)
n <- nrow(all_phone_numbers)

index <- sample(n, 5)

all_phone_numbers[index,]
```

```{r}
library(gtools)

suits <- c("Diamonds", "Clubs", "Hearts", "Spades")
numbers <- c("Ace", "Deuce", "Three", "Four", "Five", "Six", "Seven", "Eight", "Nine", "Ten", "Jack", "Queen", "King")


deck <- expand.grid(suit = suits, number = numbers)
deck <- paste(deck$suit, deck$number)
deck


hands <- permutations(52, 2, v = deck)     # ===========================================================================

length(hands[, 1])



n = nrow(hands)

index <- sample(n, 6)      # ===========================================================================================

hands[index, ]



first_card <- hands[, 1]
second_card <- hands[, 2]

head(second_card)

length(unique(first_card))



# first_card %in% kings      # Thank God I tried this code, and find the causation of wrong code
sum(first_card %in% kings)


sum(first_card %in% kings & second_card %in% kings)/sum(first_card %in% kings)
```

# =============================================================================================================================================
![how to think this equation in the right way](C:/Users/qp/Pictures/Screenshot_20220612-121142_Gallery.jpg)

![Yes, this image has to be here](C:/Users/qp/Pictures/20220608_225119.jpg)

```{r}
combinations(3, 2)

permutations(3, 2)
```

```{r}
suits


aces <- paste("Ace", suits)
aces

facedcard <- c("Ten", "Jack", "Queen", "King")
facedcard_combine <- expand.grid(faces = facedcard, suit = suits)

facedcard_combine <- paste(facedcard_combine$faces, facedcard_combine$suit)


facedcard_combine



suits <- c("Diamonds", "Clubs", "Hearts", "Spades")
numbers <- c("Ace", "Deuce", "Three", "Four", "Five", "Six", "Seven", "Eight", "Nine", "Ten", "Jack", "Queen", "King")


deck <- expand.grid(suit = suits, number = numbers)
deck <- paste(deck$number, deck$suit)


all_hands <- permutations(52, 2, v = deck)      # =========================================================================================
head(all_hands)                                 # First finish this approach, and Notice in Blackjack the natural 21 means cards, not order



# I'll apply the conditional probability equations here: Pr(A and B) = Pr(A) * Pr(B|A)
natural21 <- mean(all_hands[, 1] %in% aces) * sum(all_hands[, 1] %in% aces & all_hands[, 2] %in% facedcard_combine)/sum(all_hands[, 1] %in% aces)
natural21
```

```{r}
library(gtools)


suits <- c("Clubs", "Diamonds", "Hearts", "Spades")

aces <- paste("Ace", suits)
aces


facedcard <- c("Ten", "Jack", "Queen", "King")

facedcard_combine <- expand.grid(faces = facedcard, suit = suits)

facedcard_combine <- paste(facedcard_combine$faces, facedcard_combine$suit)
facedcard_combine



suits <- c("Diamonds", "Clubs", "Hearts", "Spades")
numbers <- c("Ace", "Deuce", "Three", "Four", "Five", "Six", "Seven", "Eight", "Nine", "Ten", "Jack", "Queen", "King")


deck <- expand.grid(suit = suits, number = numbers)
deck <- paste(deck$number, deck$suit)




hands <- combinations(52, 2, v = deck)
length(hands[, 2])


mean(hands[, 1] %in% aces & hands[, 2] %in% facedcard_combine)
mean(hands[, 1] %in% aces & hands[, 2] %in% facedcard_combine) + mean(hands[, 1] %in% facedcard_combine & hands[, 2] %in% aces)
# ===============================================================================================================================
# Here we should notice how the combination function works

mean((hands[, 1] %in% aces & hands[, 2] %in% facedcard_combine) | (hands[, 1] %in% facedcard_combine & hands[, 2] %in% aces))
```

```{r}
hands <- sample(deck, 2)

hands




B <- 100000
result <- replicate(B, {
  hand <- sample(deck, 2)
  (hand[1] %in% aces & hand[2] %in% facedcard_combine) | 
    hand[2] %in% aces & hand[1] %in% facedcard_combine})

mean(result)
```





![](C:/Users/qp/Pictures/our very first example is 5 beads in a urn.png)

![](C:/Users/qp/Pictures/here is the probability of randomly picking a blue beads from the urn.png)

![](C:/Users/qp/Pictures/r function expand.grid.png)

![](C:/Users/qp/Pictures/r function paste.png)

![](C:/Users/qp/Pictures/we use paste function to create strings be joining smaller strings.png)

![](C:/Users/qp/Pictures/the paste function performs the operation elements wise.png)

![](C:/Users/qp/Pictures/expand.grid gives us all the combination of two lists.png)

![](C:/Users/qp/Pictures/using what we just leant r functions, expand.grid and paste to generate a deck of cards.png)

![now, how about the second card being a king, given that the first card is king ??? Think about it](C:/Users/qp/Pictures/this is the first king probability.png)

![Note: these functions are available in gtools package](C:/Users/qp/Pictures/we are going to use the combinations function and permutations function.png)

![](C:/Users/qp/Pictures/what does a permutations function does.png)

![Notice: here the order maters, and once we pick a number, it cant appear again](C:/Users/qp/Pictures/here we list all the way we can choose 2 numbers from the list 1 2 3 4 5.png)

![](C:/Users/qp/Pictures/we can add vector for this permutations function.png)

![](C:/Users/qp/Pictures/a sample permutations.png)

![](C:/Users/qp/Pictures/and we will define grabbing first card and the second card using this code.png)

![](C:/Users/qp/Pictures/check how many cases have first card as king, but I failed running this code.png)

![](C:/Users/qp/Pictures/notice how we calculate the conditional probability.png)

![](C:/Users/qp/Pictures/using proportions will also gives us the answers.png)

![](C:/Users/qp/Pictures/multiplication rule equation.png)

![](C:/Users/qp/Pictures/combinations.png)

![](C:/Users/qp/Pictures/here is the permutations function.png)

![](C:/Users/qp/Pictures/and here is the outcome from a combination function.png)

![](C:/Users/qp/Pictures/compute the natural 21 in blackjack.png)

![But be careful how combination function wroks, as this code seems counterintuitive](C:/Users/qp/Pictures/using thie piece of code to compute the probability of natural 21, where aces or faced card order doesn't matter.png)

![](C:/Users/qp/Pictures/we get the same answer, and this is because combinations function works and how it list the probabilities.png)

![](C:/Users/qp/Pictures/use the sample function to simulation draw a card without replacement with default replace vector.png)

![](C:/Users/qp/Pictures/here we sue monte carlo simulation with this code to draw two card from the deck without replacement.png)

![](C:/Users/qp/Pictures/and we can repeat this over and over and then we get a very good approximation of probability.png)









# The Birthday Problem


Suppose you're in a classroom with 50 people.  If we assume this is a randomly selected group, what is the chance that at least two people have the same birthday?  Although it is somewhat advanced, we can actually deduce this mathematically, and we do this later, but now, we're going to use Monte Carlo simulations.  For simplicity, we assumed that nobody was born on February 29th.  This actually doesn't change the answer much.  

All right, first, note that birthdays can be represented as numbers between 1 and 365.  So a sample of 50 random birthdays can be obtained simply using the sample function, like this.  To check if, in this particular set of 50 people we have at least two with the same birthday, we can use the function duplicated, which returns true whenever an element of a vector has already appeared in that vector.  So here's an example.  If I type duplicated 1, 2, 3, 1, 4, 3, 5, , we get true's for the 1 and the 3 the second time they appear.  So to check if two birthdays were the same, we simply use any and duplicated functions, like this.  And in the case that we just generated, we actually do have this happening.  We did have two people, at least two people, with the same birthday.  We get true.  Now, to estimate the probability, we're going to repeat this experiment.  We're going to run a Monte Carlo simulation over and over again.  

So what we do is we do it 10,000 times.  We use the replicate function like this.  And when we're done, we get that the probability of having two people, at least two people, with the same birthday in a group of 50 people is about 98%.  Were you expecting this to be so high?  People tend to underestimate these probabilities, so [][*it might be an opportunity for you to bet and make some money off of your friends*] (key point of this course, please keep in mind).  To get an intuition as to why this is so high, think of what happens as you get close to 365 people.  At this stage, we run out of days, and the probability is 1.  In the next video, we're going to actually compute this probability for different group sizes and see how fast it gets close to 1.  


[][Textbook link]

Here is a link to the textbook section on the birthday problem External link.
https://rafalab.github.io/dsbook/probability.html#birthday-problem


[][Key points]

    duplicated() takes a vector and returns a vector of the same length with TRUE for any elements that have appeared previously in that vector.
    We can compute the probability of shared birthdays in a group of people by modeling birthdays as random draws from the numbers 1 through 365. We can then use this sampling model of birthdays to run a Monte Carlo simulation to estimate the probability of shared birthdays.

Code: The birthday problem

# checking for duplicated bdays in one 50 person group
n <- 50
bdays <- sample(1:365, n, replace = TRUE)    # generate n random birthdays
any(duplicated(bdays))    # check if any birthdays are duplicated

# Monte Carlo simulation with B=10000 replicates
B <- 10000
results <- replicate(B, {    # returns vector of B logical values
    bdays <- sample(1:365, n, replace = TRUE)
    any(duplicated(bdays))
})
mean(results)    # calculates proportion of groups with duplicated bdays




```{r}
birthdays <- sample(1:365, 50, replace = T)

length(unique(birthdays))      # So we do have the same birthday classmates in a 50 sized class, should repeat it many times and simulate the %


mean(birthdays %in% unique(birthdays))
```

```{r}
birthdaycheck <- replicate(100000, {
  same <- sample(365, 50, replace = T)
  50 - length(unique(same)) >=1             # I haven't learn the course, did I doing it wrong or something?  Why its so high ???
})


mean(birthdaycheck)
```




![](C:/Users/qp/Pictures/in a random selected class of 50 people, the chance of at least two people share birthday.png)

![](C:/Users/qp/Pictures/using r to generate randomly selected 50 birthdays.png)

![](C:/Users/qp/Pictures/duplicated function.png)

![](C:/Users/qp/Pictures/a example of duplicated function.png)

![](C:/Users/qp/Pictures/we can use any function combined with duplicated function to check if duplicate happened in list.png)

![](C:/Users/qp/Pictures/the probability of at least two classmates sharing same birthday in 50 people sized class is freaking high.png)









# sapply


Say you want to use what you've just learned about the birthday problem to bet with friends about two people having the same birthday in a group of people.  [][*When are the chances larger than 50%?  Larger than 75%? *]  Let's create a lookup table.  We can quickly create a function to compute this for any group.  We write the function like this.  We'll call it compute prob, and we'll basically make the calculations for the probability of two people having the same birthday.  We will use a small Monte Carlo simulation to do it.  Now that we've done this, we want to compute this function, we want to apply this function to several values of n, let's say from 1 to 60.  

Let's define n as a sequence starting at 1 and ending at 60.  Now, we can use a for loop to apply this function to each value in n, but it turns out that *for loops are rarely the preferred approach in R*.  In general, we try to perform operations on entire vectors.  Arithmetic operations, for example, operate on vectors in an element wise fashion.  We saw this when we learned about R.  So if we type x equals 1 through 10, now X is the vector starting at 1 and ending at 10, and we compute the square root of x, it actually computes the square root for each element.  

Equally, if we define y to be 1 through 10, and then multiply x by y, it multiplies each element 1 by 1--1 times 1, 2 times 2, et cetera.  So there's really no need for for loops.  [][*But not all functions work this way*].  You can't just send a vector to any function in R.  For example, the function we just wrote does not work element-wise since it's expecting a scalar, it's expecting an n.  This piece of code does not do what we want.  If we type compute prob and send it the vector n, we will not get what we want.  We will get just one number.  What we can do instead is use the function sapply().  **sapply permits us to perform element-wise operations on any function**.  Here's how it works.  We'll define a simple example for the vector 1 through 10.  If we want to apply the square roots of each element of x, we can simply type sapply x comma square root, and it'll apply square root to each element of x.  Of course, we don't need to do this because square root already does that, but we are using it as a simple example.  So for our case, we can simply type prob equals sapply n-- n is our vector--and then the function we define compute prob.  

And this will assign to each element of prob the probability of two people having the same birthday for that n.  And now we can very quickly make a plot.  We plot the probability of two people having the same birthday against the size of the group.  Now, let's compute the exact probabilities rather than use Monte Carlo simulations.  The function we just defined uses a Monte Carlo simulation, but we can use what we've learned about probability theory to compute the exact value.  Not only do we get the exact answer using math, but the computations are much faster since we don't have to generate experiments.  We simply get a number.  To make the math simpler for this particular problem, [][*instead of computing the probability of it happening, we'll compute the probability of it not happening, and then we can use the multiplication rule*].  

*==============You need to break your thought to tensor level, and pushing it forward one by one, then you'll have the solution ==============*
Let's start with the first person.  The probability that person 1 has a unique birthday is 1, of course.  All right.  Now let's move on to the second one.  The probability that the second person has a unique birthday given that person 1 already took one of the days is 364 divided by 365.  *Then for a person 3, given that the first two people already have unique birthdays, that leaves 363.  So now that probability is 363 divided by 365*.  If we continue this way and find the chances of all, say, 50 people having unique birthdays, we would multiply 1 times 364 divided by 365, times 363 divided by 365, dot dot dot, all the way to the 50th element.  

Here's the equation.  Now, we can easily write a function that does this.  This time we'll call it exact prob.  It takes n as a argument, and it computes this probability using this simple code.  Now we can compute each probability for each n using sapply again, like this.  And if we plot it, we can see that the Monte Carlo simulations were almost exactly right.  They were almost exact approximations of the actual value.  Now, notice had it not been possible to compute the exact probabilities, something that sometimes happens, we would have still been able to accurately estimate the probabilities using Monte Carlo.  



[][Textbook links]
The textbook discussion of the basics of sapply() can be found in this textbook section External link.
https://rafalab.github.io/dsbook/programming-basics.html#vectorization
The textbook discussion of sapply() for the birthday problem can be found within the birthday problem section External link.
https://rafalab.github.io/dsbook/probability.html#birthday-problem



[][Key points]

    Some functions automatically apply element-wise to vectors, such as sqrt() and *.
    However, other functions do not operate element-wise by default. This includes functions we define ourselves.

    The function sapply(x, f) allows any other function f to be applied element-wise to the vector x.

    The probability of an event happening is 1 minus the probability of that event not happening:
        Pr(event) = 1 - Pr(no event)
        
    We can compute the probability of shared birthdays mathematically:
        Pr(shared birthdays) = 1 - Pr(no shared birthdays) = 1 - (1 * 364/365 * 363/365 * ... * (365-n+1)/365)

 

Code: Function for birthday problem Monte Carlo simulations

Note that the function body of compute_prob() is the code that we wrote in the previous video. If we write this code as a function, we can use sapply() to apply this function to several values of n.

# function to calculate probability of shared bdays across n people
compute_prob <- function(n, B = 10000) {
	same_day <- replicate(B, {
    	bdays <- sample(1:365, n, replace = TRUE)
        any(duplicated(bdays))
    })
    mean(same_day)
}

n <- seq(1, 60)

Code: Element-wise operation over vectors and sapply

x <- 1:10
sqrt(x)    # sqrt operates on each element of the vector

y <- 1:10
x*y    # * operates element-wise on both vectors

compute_prob(n)    # does not iterate over the vector n without sapply

x <- 1:10
sapply(x, sqrt)    # this is equivalent to sqrt(x)

prob <- sapply(n, compute_prob)    # element-wise application of compute_prob to n
plot(n, prob)

Code: Computing birthday problem probabilities with sapply

# function for computing exact probability of shared birthdays for any n
exact_prob <- function(n){
    prob_unique <- seq(365, 365-n+1)/365   # vector of fractions for mult. rule
    1 - prod(prob_unique)    # calculate prob of no shared birthdays and subtract from 1
}

# applying function element-wise to vector of n values
eprob <- sapply(n, exact_prob)

# plotting Monte Carlo results and exact probabilities on same graph
plot(n, prob)    # plot Monte Carlo results
lines(n, eprob, col = "red")    # add line for exact prob




```{r}
library(tidyverse)


compute_prob <- function(n, B = 100000){
  same_day <- replicate(B, {                # We are applying a Monte Carlo simulation, obviously we need replicate function
    bdays <- sample(1:365, n, replace = T)
    any(duplicated(bdays))
  })
  mean(same_day)
}



compute_prob(23)       # Can you believe it, I got one shoot and get the correct answer 50%
                       # The power of replacement, how interesting ???
# ===============================================================================================================================
# Get prepared, womeday we may need to write a R function for later use



# ===============================================================================================================================
n <- seq(1, 30)

sapply(n, compute_prob)   # =========================================================================================================



plot(sapply(n, compute_prob))
```

```{r}
exact_prob <- function(n){                # =============================================================================================
  prob_unique <- seq(365, 365-n+1)/365    # What does this line of code doing ???
  1 - prod(prob_unique)       # prod stands for product calculation
}


exact_prob(23)



eprob <- sapply(60, exact_prob)           # Using sappy() a sequence with defined function, but inside the function, seq() already did element wise 
                                          # ========================================================================================================
eprob
```

```{r}
ff <- seq(365, 300)


ff
```

```{r}
n <- seq(1: 60)      # ==================================================================================================================



prob <- sapply(n, compute_prob)       ##############


exact_prob <- function(n){                # =============================================================================================
  prob_unique <- seq(365, 365-n+1)/365    # What does this line of code doing ???
  1 - prod(prob_unique)       # prod stands for product calculation
}



eprob <- sapply(n, exact_prob)         ##############


plot(n, prob) 
lines(n, eprob, col = "red")
```

![](C:/Users/qp/Pictures/the monte carlo simulation and the calculation outcome were almost identical.png)









![](C:/Users/qp/Pictures/say we are digging on the same birthday peobability problem, when would the probability of at least 2 people share birthday larger than 50%.png)

![](C:/Users/qp/Pictures/and how about larger than larger than 75%.png)

![This image ment to be here](C:/Users/qp/Pictures/20220609_233459.jpg)

![](C:/Users/qp/Pictures/can you see it, how can we change the variable n to change random pick group size and check if ther is same birthday shared within.png)

![](C:/Users/qp/Pictures/r provides the calculation element wise, and for loop in r pperforms bad.png)

![On any function](C:/Users/qp/Pictures/r sapply function permits us to perform element-wise operations.png)

![](C:/Users/qp/Pictures/and we can make a plot with plot function on at least 2 people share birthday from 1 through 30 people sized group.png)

![](C:/Users/qp/Pictures/lets start with the first person has a unique birthday.png)

![break thought into tensors and approaching the solution.png](C:/Users/qp/Pictures/break thought into tensors and approaching the solution.png)

![Now you must can understand this, this is the bridge](C:/Users/qp/Pictures/and here is for the person 3 have unique birthday given person 1 and person 2 already has unique birthday .png)

![](C:/Users/qp/Pictures/not saying we are finding the group size n which no one share birthday in that group, here is the probability.png)

![](C:/Users/qp/Pictures/after observed the pattern in above sample, we can writ that down into a function, and let r help us doing the calculations.png)

![](C:/Users/qp/Pictures/using sapply function to applying sequence element wise function calculation.png)









# How Many Monte Carlo Experiments are Enough?


In the examples we have seen, we have used 10,000 Monte Carlo experiments.  It turns out that this provided very accurate estimates for the examples
we looked at.  In more complex calculations, 10,000 may not nearly be enough.  Also for some calculations, 10,000 experiments might not be computationally feasible, and it might be more than we need.  In practice, we won't know what the answer is, so we don't know if our Monte Carlo estimate is accurate.  

We know that the larger the number of experiments, we've been using the letter B to represent that, the better the approximation.  But how big do we need it to be?  This is actually a challenging question, and answering it often requires advanced theoretical statistics training.  One practical approach we will describe here is to check for the stability of the estimate.  Here's an example with the birthday problem.  We're going to use n equals 22.  There's 22 people.  So we're going to run a simulation where we compute or estimate the probability of two people having a certain birthday using different sizes of the Monte Carlo simulations.  So the value of b going to go from 10, to 20, to 40, to 100, et cetera.  We compute the simulation, and now we look at the values that we get for each simulation.  

Remember, each simulation has a different b, a different number of experiments.  When we see this graph, we can see that it's wiggling up and down.  That's because the estimate is not stable yet.  It's not such a great estimate.  But as b gets bigger and bigger, eventually it starts to stabilize.
And that's when we start getting a feeling for the fact that now perhaps we have a large enough number of experiments.  


[][Textbook link]

Here is a link to the matching textbook section External link.
https://rafalab.github.io/dsbook/probability.html#infinity-in-practice


[][Key points]

    The larger the number of Monte Carlo replicates , the more accurate the estimate.
    Determining the appropriate size for can require advanced statistics.
    One practical approach is to try many sizes for and look for sizes that provide stable estimates.

Code: Estimating a practical value of B

This code runs Monte Carlo simulations to estimate the probability of shared birthdays using several B values and plots the results. When B is large enough that the estimated probability stays stable, then we have selected a useful value of B.

B <- 10^seq(1, 5, len = 100)    # defines vector of many B values
compute_prob <- function(B, n = 22){    # function to run Monte Carlo simulation with each B
	same_day <- replicate(B, {
    	bdays <- sample(1:365, n, replace = TRUE)
        any(duplicated(bdays))
    })
    mean(same_day)
}

prob <- sapply(B, compute_prob)    # apply compute_prob to many values of B
plot(log10(B), prob, type = "l")    # plot a line graph of estimates 




```{r}
B <- 10^seq(1, 5, len=100)

compute_prob22 <- function(B, n=22){
  same_day <- replicate(B, {
    bdays <- sample(365, n, replace = T)
    any(duplicated(bdays))
  })
  mean(same_day)
}


prob22 <- sapply(B, compute_prob22)
prob22

hist(prob22)
```

```{r}
plot(log10(B), prob22)
```





![This is actually a challenging question, and answering it often requires advanced theoretical statistics training.](C:/Users/qp/Pictures/how big does b need to be, depending on the problem we are solving.png)

![](C:/Users/qp/Pictures/apply different sized monte carlo simulation on shared birthday with 22 people.png)

![](C:/Users/qp/Pictures/and this is how we plot the outcome of same birthday monte carlo simulation.png)

![](C:/Users/qp/Pictures/and using this graph to represent the trending of probability whne change the size of simulation.png)









# DataCamp Assessment: Combinations and Permutations



Assessment due Jun 15, 2022 10:43 AWST

This assessment covers combinations and permutations.

By clicking OK, you agree to DataCamp's privacy policy: https://www.datacamp.com/privacy-policy. Note that you might need to disable your pop-up blocker, or allow "www.datacamp.com" in your pop-up blocker allowed list. When you have completed the exercises, return to edX to continue your learning.
Assessment: Combinations and permutations (External resource) (5.0 points possible)
By clicking OK, you agree to DataCamp's privacy policy: https://www.datacamp.com/privacy-policy.


## Exercise 1. Independence

Imagine you draw two balls from a box containing colored balls. You either replace the first ball before you draw the second or you leave the first ball out of the box when you draw the second ball.

Under which situation are the two draws independent of one another?

Remember that two events A and B are independent if Pr(A and B) = Pr(A) * P(B).
Instructions
50 XP
Possible Answers

    You don't replace the first ball before drawing the next.
    You do replace the first ball before drawing the next.
    Neither situation describes independent events.
    Both situations describe independent events.


## Exercise 2. Sampling with replacement

Say you’ve drawn 5 balls from the a box that has 3 cyan balls, 5 magenta balls, and 7 yellow balls, with replacement, and all have been yellow.

What is the probability that the next one is yellow?
Instructions
100 XP

    Assign the variable p_yellow as the probability of choosing a yellow ball on the first draw.
    Using the variable p_yellow, calculate the probability of choosing a yellow ball on the sixth draw.


# Below are my solutions
```{r}
cyan <- 3
magenta <- 5
yellow <- 7

# Assign the variable 'p_yellow' as the probability that a yellow ball is drawn from the box.
p_yellow <- yellow/(cyan + magenta + yellow)
p_yellow

# Using the variable 'p_yellow', calculate the probability of drawing a yellow ball on the sixth draw. Print this value to the console.
p_yellow6 <- (1 - p_yellow)^5 * p_yellow        # Those are my imagination, figured what does the question ask for first
p_yellow6
```

# Incorrect submission
You are not providing a calculation that gives the correct answer. The probability of choosing a yellow ball does not depend on subsequent draws when balls are replaced after each draw. 


## Exercise 3. Rolling a die

If you roll a 6-sided die once, what is the probability of not seeing a 6? If you roll a 6-sided die six times, what is the probability of not seeing a 6 on any of those rolls?
Instructions
100 XP

    Assign the variable p_no6 as the probability of not seeing a 6 on a single roll.
    Then, calculate the probability of not seeing a 6 on six rolls using p_no6.

```{r}
# Assign the variable 'p_no6' as the probability of not seeing a 6 on a single roll.
p_no6 <- 1/6

# Calculate the probability of not seeing a 6 on six rolls using `p_no6`. Print your result to the console: do not assign it to a variable.
1- p_no6
```

# Incorrect submission
Make sure that you use p_no6 as your variable name for the probability of rolling any number other than six on a 6-sided die. 

# Incorrect submission
You are not providing a calculation that gives the correct answer. The probability of rolling any number other than a six does not change from roll to roll. 

```{r}
# Assign the variable 'p_no6' as the probability of not seeing a 6 on a single roll.
p_no6 <- 5/6

# Calculate the probability of not seeing a 6 on six rolls using `p_no6`. Print your result to the console: do not assign it to a variable.
1 - p_no6
```

```{r}
# Assign the variable 'p_no6' as the probability of not seeing a 6 on a single roll.
p_no6 <- 5/6

# Calculate the probability of not seeing a 6 on six rolls using `p_no6`. Print your result to the console: do not assign it to a variable.
p_no6
```

```{r}
# Assign the variable 'p_no6' as the probability of not seeing a 6 on a single roll.
p_no6 <- 5/6

# Calculate the probability of not seeing a 6 on six rolls using `p_no6`. Print your result to the console: do not assign it to a variable.
p_no6^6
```

# Its a problem with carefully read the silly question instead of training your statistics knowledge and etc


## Exercise 4. Probability the Celtics win a game

Two teams, say the Celtics and the Cavs, are playing a seven game series. The Cavs are a better team and have a 60% chance of winning each game.

What is the probability that the Celtics win at least one game? Remember that the Celtics must win one of the first four games, or the series will be over!
Instructions
100 XP

    Calculate the probability that the Cavs will win the first four games of the series.
    Calculate the probability that the Celtics win at least one game in the first four games of the series.

```{r}
# Assign the variable `p_cavs_win4` as the probability that the Cavs will win the first four games of the series.
p_cavs_win4 <- (0.6)^4

# Using the variable `p_cavs_win4`, calculate the probability that the Celtics win at least one game in the first four games of the series.
1 - p_cavs_win4
```


## Exercise 5. Monte Carlo simulation for Celtics winning a game

Create a Monte Carlo simulation to confirm your answer to the previous problem by estimating how frequently the Celtics win at least 1 of 4 games. Use B <- 10000 simulations.

The provided sample code simulates a single series of four random games, simulated_games.
Instructions
100 XP

    Use the replicate function for B <- 10000 simulations of a four game series. The results of replicate should be stored to a variable named celtic_wins.
    Within each simulation, replicate the sample code to simulate a four-game series named simulated_games. Then, use the any function to indicate whether the four-game series contains at least one win for the Celtics. Perform these operations in two separate steps.
    Use the mean function on celtic_wins to find the proportion of simulations that contain at least one win for the Celtics out of four games.

# This is wrong code
```{r}
# This line of example code simulates four independent random games where the Celtics either lose or win. Copy this example code to use within the `replicate` function.
simulated_games <- sample(c("lose","win"), 4, replace = TRUE, prob = c(0.6, 0.4))

# The variable 'B' specifies the number of times we want the simulation to run. Let's run the Monte Carlo simulation 10,000 times.
B <- 10000

# Use the `set.seed` function to make sure your answer matches the expected result after random sampling.
set.seed(1)

# Create an object called `celtic_wins` that replicates two steps for B iterations: (1) generating a random four-game series `simulated_games` using the example code, then (2) determining whether the simulated series contains at least one win for the Celtics.

celtic_wins <- replicate(B, {
    simulated_games <- sample(c("lose","win"), 4, replace = TRUE, prob = c(0.6, 0.4))
    any(duplicated(simulated_games))
})



# Calculate the frequency out of B iterations that the Celtics won at least one game. Print your answer to the console.
mean(celtic_wins)
```

# This should be the correct one
```{r}
# This line of example code simulates four independent random games where the Celtics either lose or win. Copy this example code to use within the `replicate` function.
simulated_games <- sample(c("lose","win"), 4, replace = TRUE, prob = c(0.6, 0.4))

# The variable 'B' specifies the number of times we want the simulation to run. Let's run the Monte Carlo simulation 10,000 times.
B <- 10000

# Use the `set.seed` function to make sure your answer matches the expected result after random sampling.
set.seed(1)

# Create an object called `celtic_wins` that replicates two steps for B iterations: (1) generating a random four-game series `simulated_games` using the example code, then (2) determining whether the simulated series contains at least one win for the Celtics.

celtic_wins <- replicate(B, {
    simulated_games <- sample(c("lose","win"), 4, replace = TRUE, prob = c(0.6, 0.4))
    any(simulated_games %in% "win")
})



# Calculate the frequency out of B iterations that the Celtics won at least one game. Print your answer to the console.
mean(celtic_wins)
```


## End of Assessment: Independence and Multiplication Rule

This is the end of the programming assignment for this section. Please DO NOT click through to additional assessments from this page. Please DO answer the question on this page. If you do click through, your scores may NOT be recorded.

Click on "Awesome" to get the "points" for this question and then return to the course on edX.

You can close this window and return to Data Science: Probability.
Answer the question
50XP
Possible Answers

    Awesome
    press
    1
    Nope
    press
    2








## Course  /  Section 1: Discrete Probability  /  1.3 Addition Rule and Monty Hall



# The Addition Rule


Earlier, we showed you how to compute the probability of a natural 21 in blackjack.  This is **getting a face card and an ace in your first draw**.  Here, we're going to show you the [][*addition rule*], which gives you another way to compute this probability.  The addition rule tells us that the probability of A or B, right, we're going to have to make this calculation now, because [][***you can get to 21 in two ways.  You can get either a face card and then an ace.  Or you can get an ace and then a face card***].  So we're asking what's the probability of that or?  A or B?  The addition rule tells us the probability of A or B is the probability of A plus the probability a B minus the probability of A and B.  

To understand why this makes sense, [][*think of a Venn diagram*].  If we're computing the probability of this whole thing happening, A or B, we can add the blue circle plus the pink circle, and then subtract the middle because we have added it twice by adding the blue plus the pink.  So it makes sense-- the addition rule makes a lot of sense.  So now let's apply it to the natural 21.  In the case of natural 21, the intersection is empty.  Since both hands can't happen, you can't have both an ace and then a face card, and then at the same time have a face card and then an ace.  Those two things can't happen at the same time.  So this will be a very easy application of the addition rule.  The probability of an ace followed by a face card we know is 1 over 13 times 16 divided by 51.  And the probability of a face card followed by an ace is 16 over 52 times 4 over 51.  These are actually the same, which makes sense due to symmetry.  

These two values are actually the same.  In any case, we get the same result that we got before for the natural 21, which is about 0.05.  


[][Textbook link]

Here is a link to the textbook section on the addition rule External link.
https://rafalab.github.io/dsbook/probability.html#addition-rule


[][Clarification]

By "facecard", the professor means a card with a value of 10 (K, Q, J, 10).
Key points

[][**    The addition rule states that the probability of event A or event B happening is the probability of event A plus the probability of event B minus the probability of both events A and B happening together.**]
[][**        Pr(A or B) = Pr(A) + Pr(B) - Pr(A and B)**]

[][*    Note that (A or B) is equivalent to (A | B).*]  ========================================================================================

Example: The addition rule for a natural 21 in blackjack

[][***We apply the addition rule where A = drawing an ace then a facecard and B = drawing a facecard then an ace. Note that in this case, both events A and B cannot happen at the same time, so Pr(A and B) = 0***]

        Pr(aces then faced cards) = 4/52 * 16/51                 ------------> Pr(B|A)
        pr(faced cards then aces) = 16/52 * 4/51                 ------------> Pr(A|B)
        Pr(aces then faced cards | faced cards then aces) = 4/51 * 16/51 * 16/52 * 4/51 = 0.0483.

**
 


![this gives you another to compute the natural 21 in blackjack card game](C:/Users/qp/Pictures/the addition rule.png)

![](C:/Users/qp/Pictures/this is what addition rule tell us.png)

![if we compute this whole thing happening, a or b](C:/Users/qp/Pictures/we can use a ven diagram to understand this.png)

![](C:/Users/qp/Pictures/the probability of an ace followed by a faced card is this.png)

![](C:/Users/qp/Pictures/and this the the probability of a faced card followed by an ace card.png)

```{r}
1/13 * 16/51

16/52 * 4/51
```








# The Monty Hall Problem


We're going to look at one last example from discrete probability.  It's the Monty Hall problem.  In the 1970s, there was a game show called Let's Make A Deal.  Monty Hall was the host.  This is where the name of the problem comes from.  ***At some point in the game, contestants were asked to pick one of three doors.  Behind one door there was a prize.  The other two had a goat behind them.  And this basically meant that you lost.  Monty Hall would open one of the two remaining doors and show the contestant that there was no prize behind that door.  So you're left with two doors-- the one you picked, and one door that you do not know what's behind it.  So then, Monty Hall would ask, do you want to switch doors?  ***

What would you do?  We can use probability to show that if you stick to the original door, your chances of winning a car, or whatever big prize, is 1 in 3.  But if you switch, your chances double to 2 in 3.  This seems counterintuitive.  Many people incorrectly think both chances are 1 in 2 since there's only two doors left, and there's a prize behind one of them.  You can find detailed explanations-- we're going to provide links of the mathematics of how you can calculate that this is in fact wrong, that you have a higher chance if you switch.  [][*But here, we're going to use a Monte Carlo simulation to show you that this is the case.  And this will help you understand how it all works*].  

Note that the code we're going to show you is longer than it needs to be.  But we're using it as an illustration to help you understand the logic behind what actually happens here.  So let's start by creating a simulation that imitates the strategy of sticking to the same door.  Let's go through the code.  We're going to do 10,000 experiments.  We're going to do this over and over 10,000 times.  First thing we do, is we assign the prize to a door.  All right, so the prize is going to be in one of the three doors that we have created.  Then, that's going to be in prize door.  Prize door contains the number of the door with the prize behind it.  Then, we're going to imitate your pick by taking a random sample of these three doors.  Now, in the next step, we're going to decide which door you're shown.  You're going to be shown--not your door and not the door with the prize.  You're going to be shown the other one.  You stick to your door, that's what you stick to, nothing changed.  All this that we did right above doesn't matter because you're sticking to your door.  So now we do this over and over again, and at the end (*Its actually not at the end, its at the end of each simulation, so we will get a boolean value of each simulation 10000 times*), we ask, is the door you chose the one you stuck to?  Is that the prize door?  How often does this happen?  

We know it's going to be a third, because none of this procedure changed anything.  You started picking one out of three, nothing changed, and now you are asked, is the one I picked the door?  If we run the simulation, we actually see it happening.  We ran it 10,000 times and the probability of winning was 0.3357, about 1 in 3.  Now let's repeat the exercise, but consider the switch strategy.  We start the same way.  We have three doors, we assign three prizes at random.  Then we know which one has the good prize, the car, but we don't tell the contestant.  So the contestant has to pick one.  It's basically a random pick.  That's what my pick is.  And now we're going to show the contestant one door.  It can't be the one they chose, and it can't be the one with the car.  So, that only leaves one door-- the door with nothing behind it that was not chosen.  So now what you're going to do is you're going to switch.  You're going to switch to the door that they didn't show you, because the one that they did show you had nothing behind it.  So basically, what's happening is you are switching from the original that had a 1 in 3 chance of being the one to whatever is the other option, which has to have a 2 in 3 chance.  So if we run the simulation, we actually confirm that.  We get that the proportion of times a win is 0.6717 which is about 2/3.  The Monte Carlo estimate confirms the two out of three calculation.  


[][Textbook section]

Here is the textbook section on the Monty Hall Problem External link.
https://rafalab.github.io/dsbook/probability.html#monty-hall-problem


[][Key points]

    Monte Carlo simulations can be used to simulate random outcomes, which makes them useful when exploring ambiguous or less intuitive problems like the Monty Hall problem.
    In the Monty Hall problem, contestants choose one of three doors that may contain a prize. Then, one of the doors that was not chosen by the contestant and does not contain a prize is revealed. The contestant can then choose whether to stick with the original choice or switch to the remaining unopened door.
    Although it may seem intuitively like the contestant has a 1 in 2 chance of winning regardless of whether they stick or switch, Monte Carlo simulations demonstrate that the actual probability of winning is 1 in 3 with the stick strategy and 2 in 3 with the switch strategy.
    For more on the Monty Hall problem, you can watch a detailed explanation here External link or read an explanation here External link.
    https://www.khanacademy.org/math/statistics-probability/probability-library/basic-theoretical-probability/v/monty-hall-problem
    https://en.wikipedia.org/wiki/Monty_Hall_problem

Code: Monte Carlo simulation of stick strategy

B <- 10000
stick <- replicate(B, {
	doors <- as.character(1:3)
	prize <- sample(c("car","goat","goat"))    # puts prizes in random order
	prize_door <- doors[prize == "car"]    # note which door has prize
	my_pick  <- sample(doors, 1)    # note which door is chosen
	show <- sample(doors[!doors %in% c(my_pick, prize_door)],1)    # open door with no prize that isn't chosen
	stick <- my_pick    # stick with original door
	stick == prize_door    # test whether the original door has the prize
})
mean(stick)    # probability of choosing prize door when sticking

Code: Monte Carlo simulation of switch strategy

switch <- replicate(B, {
	doors <- as.character(1:3)
	prize <- sample(c("car","goat","goat"))    # puts prizes in random order
	prize_door <- doors[prize == "car"]    # note which door has prize
	my_pick  <- sample(doors, 1)    # note which door is chosen first
	show <- sample(doors[!doors %in% c(my_pick, prize_door)], 1)    # open door with no prize that isn't chosen
	switch <- doors[!doors%in%c(my_pick, show)]    # switch to the door that wasn't chosen first or opened
	switch == prize_door    # test whether the switched door has the prize
})
mean(switch)    # probability of choosing prize door when switching




# Trying to understand the logic behind the below R code, next time you should be able to draft those kind to code by reading the questions =========
```{r}
B <- 100000

stick <- replicate(B, {
  doors <- as.character(1:3)
  prize <- sample(c("Car", "Goat", "Goat"))
  prize_door <- doors[prize == "Car"]
  the_pick <- sample(doors, 1)
  show <- sample(doors[!doors %in% c(the_pick, prize_door)], 1)
  stick <- the_pick
  stick == prize_door
})


mean(stick)




switch <- replicate(B, {
  doors <- as.character(1:3)
  prize <- sample(c("Car", "Goat", "Goat"))
  prize_door <- doors[prize == "Car"]
  the_pick <- sample(doors, 1)
  show <- sample(doors[!doors %in% c(the_pick, prize_door)], 1)
  switch <- sample(doors[!doors %in% c(the_pick, show)], 1)
  switch == prize_door
})


mean(switch)
```

```{r}
doors <- as.character(1:3)
doors

prize <- sample(c("Car", "Goat", "Goat"))
prize
```





![](C:/Users/qp/Pictures/monty hall problem.png)

![](C:/Users/qp/Pictures/behind one door is the prize, and had goat behind other two.png)

![and show contestant that there is no prize behind that door](C:/Users/qp/Pictures/if the contestant failed at first try, monty hall will oepn one of two remaining door that has a goat behind it.png)

![](C:/Users/qp/Pictures/because the original choose are based on first condition, the 3 potential prize probabilities.png)

![](C:/Users/qp/Pictures/but if you switch, the chance will double to 2 in 3.png)

![](C:/Users/qp/Pictures/lets find the probability of monty hall problem with monte carlo simulation and comparing with 2 strategies.png)

![](C:/Users/qp/Pictures/lets go through the code with stick to the same door strategy with monte carlo simulation.png)

![](C:/Users/qp/Pictures/now lets repeat this exercise but consider the switch strategy.png)

![](C:/Users/qp/Pictures/same as stick strategy, we have 3 doors, we assign 3 prizes at random, then we know which one has the good prize.png)









# DataCamp Assessment: The Addition Rule and Monty Hall


Assessment due Jun 15, 2022 10:43 AWST

This assessment covers more principles of probability again looking at sports game series.

By clicking OK, you agree to DataCamp's privacy policy: https://www.datacamp.com/privacy-policy. Note that you might need to disable your pop-up blocker, or allow "www.datacamp.com" in your pop-up blocker allowed list. When you have completed the exercises, return to edX to continue your learning.
Assessment: The Addition Rule and Monty Hall (External resource) (4.5 points possible)
By clicking OK, you agree to DataCamp's privacy policy: https://www.datacamp.com/privacy-policy.


## Exercise 1. The Cavs and the Warriors

Two teams, say the Cavs and the Warriors, are playing a seven game championship series. The first to win four games wins the series. The teams are equally good, so they each have a 50-50 chance of winning each game.

If the Cavs lose the first game, what is the probability that they win the series?
Instructions
100 XP

    Assign the number of remaining games to the variable n.
    Assign a variable outcomes as a vector of possible outcomes in a single game, where 0 indicates a loss and 1 indicates a win for the Cavs.
    Assign a variable l to a list of all possible outcomes in all remaining games. Use the rep function to create a list of n games, where each game consists of list(outcomes).
    Use the expand.grid function to create a data frame containing all the combinations of possible outcomes of the remaining games.
    Use the rowSums function to identify which combinations of game outcomes result in the Cavs winning the number of games necessary to win the series.
    Use the mean function to calculate the proportion of outcomes that result in the Cavs winning the series and print your answer to the console.
    
# My poor initial approach, 
# ==================================================================================================================================================
```
# Assign a variable 'n' as the number of remaining games.
n <- 7 - 1

# Assign a variable `outcomes` as a vector of possible game outcomes, where 0 indicates a loss and 1 indicates a win for the Cavs.
outcomes <- sample(c(0, 1), 1, prob=c(0.5, 0.5), replace=T)
outcomes

# Assign a variable `l` to a list of all possible outcomes in all remaining games. Use the `rep` function on `list(outcomes)` to create list of length `n`.
l <- rep(list(outcomes), times=n)
l

# Create a data frame named 'possibilities' that contains all combinations of possible outcomes for the remaining games.
possibilities <- expand.grid(l)

# Create a vector named 'results' that indicates whether each row in the data frame 'possibilities' contains enough wins for the Cavs to win the series.
results <- 

# Calculate the proportion of 'results' in which the Cavs win the series. Print the outcome to the console.
mean(results)
```

![Vectors are generally created using the c function.png](C:/Users/qp/Pictures/Vectors are generally created using the c function.png)

# Hint

    Any combination of 4 or more wins across the 6 remaining games allows the Cavs to win the series.
    The possibilities data frame should contain one column for each remaining game.
    The possibilities data frame should contain one row for each combination of wins (1) or losses (0) for the next 6 games.
    results should be a logical vector.

```{r}
# Assign a variable 'n' as the number of remaining games.
n <- 6

# Assign a variable `outcomes` as a vector of possible game outcomes, where 0 indicates a loss and 1 indicates a win for the Cavs.
outcomes <- c(0, 1)

# Assign a variable `l` to a list of all possible outcomes in all remaining games. Use the `rep` function on `list(outcomes)` to create list of length `n`.
l <- rep(list(outcomes), n)
l
# Create a data frame named 'possibilities' that contains all combinations of possible outcomes for the remaining games.
possibilities <- expand.grid(l)
possibilities
# Create a vector named 'results' that indicates whether each row in the data frame 'possibilities' contains enough wins for the Cavs to win the series.
results <- rowSums(possibilities) >= 4
results
# Calculate the proportion of 'results' in which the Cavs win the series. Print the outcome to the console.
mean(results)
```



## Exercise 2. The Cavs and the Warriors - Monte Carlo

Confirm the results of the previous question with a Monte Carlo simulation to estimate the probability of the Cavs winning the series after losing the first game.
Instructions
100 XP

    Use the replicate function to replicate the sample code for B <- 10000 simulations.
    Use the sample function to simulate a series of 6 games with random, independent outcomes of either a loss for the Cavs (0) or a win for the Cavs (1) in that order. Use the default probabilities to sample.
    Use the sum function to determine whether a simulated series contained at least 4 wins for the Cavs.
    Use the mean function to find the proportion of simulations in which the Cavs win at least 4 of the remaining games. Print your answer to the console.

# Thank god, got corrected with one shoot
```{r}
# The variable `B` specifies the number of times we want the simulation to run. Let's run the Monte Carlo simulation 10,000 times.
B <- 10000

# Use the `set.seed` function to make sure your answer matches the expected result after random sampling.
set.seed(1)

# Create an object called `results` that replicates for `B` iterations a simulated series and determines whether that series contains at least four wins for the Cavs.
results <- replicate(B, {
    sum(sample(c(0, 1), 6, replace=T)) >= 4
})



# Calculate the frequency out of `B` iterations that the Cavs won at least four games in the remainder of the series. Print your answer to the console.
mean(results)
```


## Exercise 3. A and B play a series - part 1

Two teams, A and B, are playing a seven series game series. Team A is better than team B and has a p>0.5 chance of winning each game.
Instructions
100 XP

    Use the function `sapply()` to compute the probability, call it `Pr` of winning for `p <- seq(0.5, 0.95, 0.025)`.
    Then plot the result plot(p, Pr).

# Thank god, got one shoot again
```{r}
# Let's assign the variable 'p' as the vector of probabilities that team A will win.
p <- seq(0.5, 0.95, 0.025)

# Given a value 'p', the probability of winning the series for the underdog team B can be computed with the following function based on a Monte Carlo simulation:
prob_win <- function(p){
  B <- 10000
  result <- replicate(B, {
    b_win <- sample(c(1,0), 7, replace = TRUE, prob = c(1-p, p))
    sum(b_win)>=4
    })
  mean(result)
}

# Apply the 'prob_win' function across the vector of probabilities that team A will win to determine the probability that team B will win. Call this object 'Pr'.
Pr <- sapply(p, prob_win)

# Plot the probability 'p' on the x-axis and 'Pr' on the y-axis.
plot(p, Pr) 
```



## Exercise 4. A and B play a series - part 2

Repeat the previous exercise, but now keep the probability that team

wins fixed at p <- 0.75 and compute the probability for different series lengths. For example, wins in best of 1 game, 3 games, 5 games, and so on through a series that lasts 25 games.
Instructions
100 XP

    Use the seq function to generate a list of odd numbers ranging from 1 to 25.
    Use the function sapply to compute the probability, call it Pr, of winning during series of different lengths.
    Then plot the result plot(N, Pr).

# Lets see how to construct a R solution based on the statistic thinking framework we trained
```{r}
# Given a value 'p', the probability of winning the series for the underdog team B can be computed with the following function based on a Monte Carlo simulation:
prob_win <- function(N, p=0.75){
      B <- 10000
      result <- replicate(B, {
        b_win <- sample(c(1,0), N, replace = TRUE, prob = c(1-p, p))
        sum(b_win)>=(N+1)/2
        })
      mean(result)
    }

# Assign the variable 'N' as the vector of series lengths. Use only odd numbers ranging from 1 to 25 games.
N <- seq(1,25, 2)
N
# Apply the 'prob_win' function across the vector of series lengths to determine the probability that team B will win. Call this object `Pr`.
Pr <- sapply(N, prob_win)
Pr
# Plot the number of games in the series 'N' on the x-axis and 'Pr' on the y-axis.
plot(N, Pr)
```


## End of Assessment: The Addition Rule

This is the end of the programming assignment for this section. Please DO NOT click through to additional assessments from this page. Please DO answer the question on this page. If you do click through, your scores may NOT be recorded.

Click on "Awesome" to get the "points" for this question and then return to the course on edX.

You can close this window and return to Data Science: Probability.
Answer the question
50XP
Possible Answers

    Awesome
    press
    1
    Nope
    press
    2








## Course  /  Section 1: Discrete Probability  /  1.4 Assessment: Discrete Probability


# Introduction

The following assessments allow you to practice the probability and coding skills you've learned in Section 1: Discrete Probability. You will likely find it useful to try out code to answer the problems using R on your own machine.

You will benefit from using the following libraries:

library(gtools)
library(tidyverse)


```{r}
library(gtools)
library(tidyverse)


```



## Question 1: Olympic running


Assessment due Jun 15, 2022 10:43 AWST

In the 200m dash finals in the Olympics, 8 runners compete for 3 medals (order matters). In the 2012 Olympics, 3 of the 8 runners were from Jamaica and the other 5 were from different countries. The three medals were all won by Jamaica (Usain Bolt, Yohan Blake, and Warren Weir).

Use the information above to help you answer the following four questions.


## Question 1a
1/1 point (graded)
How many different ways can the 3 medals be distributed across 8 runners?
correct

336
Loading

Explanation

The following code can be used to determine the number of permutations:

          library(gtools)
medals <- permutations(8,3)
nrow(medals)
        

You have used 2 of 10 attempts 


# The thing is how can we understand this statistic case based on what we learned in the previous card game?  Does this makes sense to us????
# =============================================================================================================================
```{r}
library(gtools)
library(tidyverse)


prizes <- c("Gold", "Silver", "Bronze") 
runners <- c("Runner_01", "Runner_02", "Runner_03", "Runner_04", "Runner_05", "Runner_06", "Runner_07", "Runner_08")


# Use your brain to think about this condition and the card game?  Why we shouldn't apple same process here???
# ===============================================================================================================================
prob <- expand.grid(prize = prizes, runner = runners)

cases <- paste(prob$prize, prob$runner)


outcomes <- permutations(8, 3, v=runners)
#outcomes
length(outcomes[, 1])
```


##  Question 1b
1/1 point (graded)
How many different ways can the three medals be distributed among the 3 runners from Jamaica?
correct

6
Loading

Explanation

The following code can be used to determine the number of permutations:

          jamaica < permutations(3,3)
nrow(jamaica)
        

You have used 1 of 10 attempts 

# Below is my own approach, withought reading the solution
```{r}
library(gtools)

suits <- c("Diamonds", "Clubs", "Hearts", "Spades")
numbers <- c("Ace", "Deuce", "Three", "Four", "Five", "Six", "Seven", "Eight", "Nine", "Ten", "Jack", "Queen", "King")


deck <- expand.grid(suit = suits, number = numbers)
deck <- paste(deck$suit, deck$number)
deck


hands <- permutations(52, 2, v = deck)     # ===========================================================================

length(hands[, 1])
deck
```


##  Question 1c
1/1 point (graded)
What is the probability that all 3 medals are won by Jamaica?
correct

0.0179
Loading

Explanation

The following code can be used to determine the probability:

          nrow(jamaica)/nrow(medals)
        

You have used 1 of 10 attempts Some

# Below is my own approach, withought reading the solution
```{r}
library(gtools)
library(tidyverse)


prizes <- c("Gold", "Silver", "Bronze") 
runners <- c("Jamaica_01", "Jamaica_02", "Jamaica_03", "Runner_04", "Runner_05", "Runner_06", "Runner_07", "Runner_08")
jamaicas <- c("Jamaica_01", "Jamaica_02", "Jamaica_03")


# Use your brain to think about this condition and the card game?  Why we shouldn't apple same process here???
prob <- expand.grid(prize = prizes, runner = runners)

cases <- paste(prob$prize, prob$runner)


all_outcomes <- permutations(8, 3, v=runners)
jamaica_coucomes <- permutations(3, 3, v=jamaicas)
length(all_outcomes[, 1])
length(jamaica_coucomes[, 1])

mean(all_outcomes[, 1] %in% jamaica_coucomes & all_outcomes[, 2] %in% jamaica_coucomes & all_outcomes[, 3] %in% jamaica_coucomes)
```


##  Question 1d
1/1 point (graded)

Run a Monte Carlo simulation on this vector representing the countries of the 8 runners in this race:

      
runners <- c("Jamaica", "Jamaica", "Jamaica", "USA", "Ecuador", "Netherlands", "France", "South Africa")

    

For each iteration of the Monte Carlo simulation, within a replicate() loop, select 3 runners representing the 3 medalists and check whether they are all from Jamaica. Repeat this simulation 10,000 times. Set the seed to 1 before running the loop.
Calculate the probability that all the runners are from Jamaica.
correct

0.0165
Loading

Explanation

Note that your answer will differ depending on whether you are using R 3.5 or earlier (0.0165) or R 3.6 or later (0.0174) because of the way set.seed works in different versions of R.

The following code can be used to determine the probability:

          set.seed(1)
runners <- c("Jamaica", "Jamaica", "Jamaica", "USA", "Ecuador", "Netherlands", "France", "South Africa")
B <- 10000
all_jamaica <- replicate(B, {
  results <- sample(runners, 3)
  all(results == "Jamaica")
})
mean(all_jamaica)
        

# Below is my own approach, withought reading the solution
```{r}
set.seed(1, sample.kind="Rounding")


runners <- c("Jamaica", "Jamaica", "Jamaica", "USA", "Ecuador", "Netherlands", "France", "South Africa")

B <- 10000

monte_carlo <- replicate(B, {
  outcomes <- sample(runners, 3)
  outcomes[1] == "Jamaica" & outcomes[2] == "Jamaica" & outcomes[3] == "Jamaica"
})


mean(monte_carlo)
```








# Question 2: Restaurant management



Assessment due Jun 15, 2022 10:43 AWST

Use the information below to answer the following five questions.

A restaurant manager wants to advertise that his lunch special offers enough choices to eat different meals every day of the year. He doesn't think his current special actually allows that number of choices, but wants to change his special if needed to allow at least 365 choices.

A meal at the restaurant includes 1 entree, 2 sides, and 1 drink. He currently offers a choice of 1 entree from a list of 6 options, a choice of 2 different sides from a list of 6 options, and a choice of 1 drink from a list of 2 options.


## Question 2a
1/1 point (graded)
How many meal combinations are possible with the current menu?
correct

180
Loading

Explanation

The following code can be used to determine the number of combinations:

          6 * nrow(combinations(6,2)) * 2
        

You have used 2 of 10 attempts Some


```{r}
6 * 6*5 * 2


all_entrees <- c("entree_1", "entree_2", "entree_3", "entree_4", "entree_5", "entree_6")
sides <- c("side_1", "side_2", "side_3", "side_4", "side_5", "side_6")
all_comb <- combinations(6, 2, v=sides)
all_comb

all_sides <- paste(all_comb[, 1], all_comb[, 2])
all_sides

all_drinks <- c("drink_1", "drink_2")


special_offers <- expand.grid(all_entrees, all_sides, all_drinks)
nrow(special_offers)

```


##  Question 2b
1/1 point (graded)

The manager has one additional drink he could add to the special.
How many combinations are possible if he expands his original special to 3 drink options?
correct

270
Loading

Explanation

The following code can be used to determine the number of combinations:

          6 * nrow(combinations(6,2)) * 3
        

You have used 1 of 10 attempts Some

```{r}
all_entrees <- c("entree_1", "entree_2", "entree_3", "entree_4", "entree_5", "entree_6")
sides <- c("side_1", "side_2", "side_3", "side_4", "side_5", "side_6")
all_comb <- combinations(6, 2, v=sides)
all_comb

all_sides <- paste(all_comb[, 1], all_comb[, 2])
all_sides

all_drinks <- c("drink_1", "drink_2", "drink_3")


special_offers <- expand.grid(all_entrees, all_sides, all_drinks)
nrow(special_offers)
```


##  Question 2c
1/1 point (graded)

The manager decides to add the third drink but needs to expand the number of options. The manager would prefer not to change his menu further and wants to know if he can meet his goal by letting customers choose more sides.
How many meal combinations are there if customers can choose from 6 entrees, 3 drinks, and select 3 sides from the current 6 options?
correct

360
Loading

Explanation

The following code can be used to determine the number of combinations:

          6 * nrow(combinations(6,3)) * 3
        

You have used 1 of 10 attempts Some

```{r}
all_entrees <- c("entree_1", "entree_2", "entree_3", "entree_4", "entree_5", "entree_6")
sides <- c("side_1", "side_2", "side_3", "side_4", "side_5", "side_6")
all_comb <- combinations(6, 3, v=sides)
all_comb

all_sides <- paste(all_comb[, 1], all_comb[, 2])
all_sides

all_drinks <- c("drink_1", "drink_2", "drink_3")


special_offers <- expand.grid(all_entrees, all_sides, all_drinks)
nrow(special_offers)
```


##  Question 2d
1/1 point (graded)

The manager is concerned that customers may not want 3 sides with their meal. He is willing to increase the number of entree choices instead, but if he adds too many expensive options it could eat into profits. He wants to know how many entree choices he would have to offer in order to meet his goal.

- Write a function that takes a number of entree choices and returns the number of meal combinations possible given that number of entree options, 3 drink choices, and a selection of 2 sides from 6 options.

- Use sapply() to apply the function to entree option counts ranging from 1 to 12.
What is the minimum number of entree options required in order to generate more than 365 combinations?
correct

9
Loading

Explanation

The following code can be used to determine the minimum number of entree options:

          library(tidyverse)

entree_choices <- function(x){
  x * nrow(combinations(6,2)) * 3
}

combos <- sapply(1:12, entree_choices)

data.frame(entrees = 1:12, combos = combos) %>%
  filter(combos > 365) %>%
  min(.$entrees)
        

You have used 2 of 10 attempts Some

```{r}
n <- seq(1:12)
n


# function to calculate probability of shared bdays across n people
my_func <- function(n) {
	all_entrees <- c(1:n)
  sides <- c("side_1", "side_2", "side_3", "side_4", "side_5", "side_6")
  all_comb <- combinations(6, 2, v=sides)
  all_sides <- paste(all_comb[, 1], all_comb[, 2])
  all_drinks <- c("drink_1", "drink_2", "drink_3")
  
  special_offers <- expand.grid(all_entrees, all_sides, all_drinks)
  nrow(special_offers)
  #mean(same_day)
}



sapply(n, my_func)
```


## Question 2e
1/1 point (graded)

The manager isn't sure he can afford to put that many entree choices on the lunch menu and thinks it would be cheaper for him to expand the number of sides. He wants to know how many sides he would have to offer to meet his goal of at least 365 combinations.

- Write a function that takes a number of side choices and returns the number of meal combinations possible given 6 entree choices, 3 drink choices, and a selection of 2 sides from the specified number of side choices.

- Use sapply() to apply the function to side counts ranging from 2 to 12.
What is the minimum number of side options required in order to generate more than 365 combinations?
correct

7
Loading

Explanation

The following code can be used to determine the minimum number of side options:

          side_choices <- function(x){
  6 * nrow(combinations(x, 2)) * 3
}

combos <- sapply(2:12, side_choices)

data.frame(sides = 2:12, combos = combos) %>%
  filter(combos > 365) %>%
  min(.$sides)
        

You have used 1 of 10 attempts Some

```{r}
n <- seq(2,12)
n


# function to calculate probability of shared bdays across n people
my_func <- function(n) {
	all_entrees <- c("side_1", "side_2", "side_3", "side_4", "side_5", "side_6")
  sides <- seq(n)
  sides
  all_comb <- combinations(n, 2, v=sides)
  all_sides <- paste(all_comb[, 1], all_comb[, 2])
  all_drinks <- c("drink_1", "drink_2", "drink_3")
  
  special_offers <- expand.grid(all_entrees, all_sides, all_drinks)
  nrow(special_offers)
  #mean(same_day)
}



sapply(n, my_func)
```









# Questions 3 and 4: Esophageal cancer and alcohol/tobacco use, part 1



Assessment due Jun 15, 2022 10:43 AWST

Case-control studies help determine whether certain exposures are associated with outcomes such as developing cancer. The built-in dataset esoph contains data from a case-control study in France comparing people with esophageal cancer (cases, counted in ncases) to people without esophageal cancer (controls, counted in ncontrols) that are carefully matched on a variety of demographic and medical characteristics. The study compares alcohol intake in grams per day (alcgp) and tobacco intake in grams per day (tobgp) across cases and controls grouped by age range (agegp).

The dataset is available in base R and can be called with the variable name esoph:

head(esoph)

You will be using this dataset to answer the following four multi-part questions (Questions 3-6).

You may wish to use the tidyverse package:

library(tidyverse)

The following three parts have you explore some basic characteristics of the dataset.

Each row contains one group of the experiment. Each group has a different combination of age, alcohol consumption, and tobacco consumption. The number of cancer cases and number of controls (individuals without cancer) are reported for each group.


##  Question 3a
1/1 point (graded)
How many groups are in the study?
correct

88
Loading

Explanation

You can find the number of groups using nrow(esoph).
You have used 1 of 10 attempts Some

```{r}
esoph



library(tidyverse)


nrow(esoph)



all_cases <- esoph %>%
  filter(ncases != 0) 


all_cases <- sum(all_cases$ncases)
all_cases

all_controls <- sum(esoph$ncontrols)
all_controls
```


##  Question 3b
1/1 point (graded)
How many cases are there?

Save this value as all_cases for later problems.
correct

200
Loading

Explanation

You can find the number of cases using this code:

          all_cases <- sum(esoph$ncases)
all_cases
        

You have used 2 of 10 attempts Some


## ##  Question 3c
1/1 point (graded)
How many controls are there?

Save this value as all_controls for later problems.
correct

Loading
You have used 1 of 10 attempts Some problems have options such as save, reset, hints, or show answer. These options follow the Submit button.
Correct (1/1 point) 

```{r}
esoph %>% 
  mutate(n_ncases = sum(ncases),
         n_ncontrols = sum(ncontrols), 
         prob = n_ncases/(n_ncases + n_ncontrols)) %>%
  group_by(alcgp) %>%
  select(alcgp, n_ncases, n_ncontrols, prob) %>%
  unique()
```

```
Teams %>% 
  filter(yearID %in% 1961:2001) %>%
  mutate(Singles = (H - HR - X2B - X3B)/G, BB = BB/G, HR = HR/G) %>%
  summarize(cor(BB, HR), cor(Singles, HR), cor(BB, Singles))
```


##  Question 4a
1/1 point (graded)
What is the probability that a subject in the highest alcohol consumption group is a cancer case?

Report your answer to 3 significant figures.
correct

0.672 or 0.402
Loading

Explanation

You can find the probability using this code:

          esoph %>%
  filter(alcgp == "120+") %>%
  summarize(ncases = sum(ncases), ncontrols = sum(ncontrols)) %>%
  mutate(p_case = ncases / (ncases + ncontrols)) %>%
  pull(p_case)
        

Note that depending on your version of R and the dataset you will get a different answer because of bug fixes to the dataset!
You have used 2 of 10 attempts Some

```{r}
high_alco <- esoph %>%
  filter(alcgp == "120+")


high_alco

sum(high_alco$ncases)/(sum(high_alco$ncases) + sum(high_alco$ncontrols))
```


##   Question 4b
1/1 point (graded)
What is the probability that a subject in the lowest alcohol consumption group is a cancer case?

Report your answer to 3 significant figures.
correct

0.0699 or 0.0653
Loading

Explanation

You can find the probability using this code:

          esoph %>%
  filter(alcgp == "0-39g/day") %>%
  summarize(ncases = sum(ncases), ncontrols = sum(ncontrols)) %>%
  mutate(p_case = ncases / (ncases + ncontrols)) %>%
  pull(p_case)
        

Note that depending on your version of R and the dataset you will get a different answer because of bug fixes to the dataset!
You have used 3 of 10 attempts Some

```{r}
esoph


low_alco <- esoph %>%
  filter(alcgp == "0-39g/day")


low_alco

sum(low_alco$ncases)/(sum(low_alco$ncases) + sum(low_alco$ncontrols))
```


##  Question 4c
1/1 point (graded)
Given that a person is a case, what is the probability that they smoke 10g or more a day?
correct

0.61
Loading

Explanation

You can find the probability using this code:

          tob_cases <- esoph %>%
  filter(tobgp != "0-9g/day") %>%
  pull(ncases) %>%
  sum()

tob_cases/all_cases

        

You have used 1 of 10 attempts Some

```{r}
esoph


cas <- esoph %>%
  filter(ncases != 0)


Less10 <- cas %>%
  filter(tobgp == "0-9g/day")

Less10



1- sum(Less10$ncases)/sum(cas$ncases)
```


##  Question 4d
1/1 point (graded)
Given that a person is a control, what is the probability that they smoke 10g or more a day?

Report your answer to 3 significant figures.
correct

0.423 or 0.462
Loading

Explanation

You can find the probability using this code:

          tob_controls <- esoph %>%
  filter(tobgp != "0-9g/day") %>%
  pull(ncontrols) %>%
  sum()

tob_controls/all_controls
        

Note that depending on your version of R and the dataset you will get a different answer because of bug fixes to the dataset!
You have used 2 of 10 attempts Some


```{r}
esoph


to_df <- esoph %>%
  filter(ncontrols != 0) %>%
  group_by(tobgp) %>%
  mutate(NO = sum(ncontrols)) %>%
  select(tobgp, NO) %>%
  unique()

to_df


not10 <- to_df %>%
  filter(!tobgp %in% "0-9g/day") %>%
  pull(NO) %>%
  sum
not10

not10/sum(to_df$NO)

```








## Questions 5 and 6: Esophageal cancer and alcohol/tobacco use, part 2


Assessment due Jun 15, 2022 10:43 AWST

The following four parts look at probabilities related to alcohol and tobacco consumption among the cases.


## Question 5a
1.0/1.0 point (graded)
For cases, what is the probability of being in the highest alcohol group?
correct

0.225
Loading

Explanation

The probability can be calculated using the following code:

          high_alc_cases <- esoph %>%
  filter(alcgp == "120+") %>%
  pull(ncases) %>%
  sum()

p_case_high_alc <- high_alc_cases/all_cases
p_case_high_alc
        

You have used 1 of 10 attempts Some


```{r}
esoph


cases_alcogroup <- esoph %>%
  filter(ncases != 0) %>%
  group_by(alcgp) %>%
  mutate(n_cases = sum(ncases)) %>%
  select(alcgp, n_cases) %>%
  unique()

cases_alcogroup


cases_highalco <- cases_alcogroup %>%
  filter(alcgp == "120+")

cases_highalco$n_cases/sum(cases_alcogroup$n_cases)

45/(45+29+75+51)
```


##  Question 5b
1.0/1.0 point (graded)
For cases, what is the probability of being in the highest tobacco group?
correct

0.155
Loading

Explanation

The probability can be calculated using the following code:

          high_tob_cases <- esoph %>%
  filter(tobgp == "30+") %>%
  pull(ncases) %>%
  sum()

p_case_high_tob <- high_tob_cases/all_cases
p_case_high_tob
        

You have used 1 of 10 attempts Some


```{r}
cases_togroup <- esoph %>%
  filter(ncases != 0) %>%
  group_by(tobgp) %>%
  mutate(n_cases = sum(ncases)) %>%
  select(tobgp, n_cases) %>%
  unique()

cases_togroup


cases_highto <- cases_togroup %>%
  filter(tobgp == "30+")

cases_highto$n_cases/sum(cases_togroup$n_cases)

31/(31+78+33+58)
```


##  Question 5c
1.0/1.0 point (graded)
For cases, what is the probability of being in the highest alcohol group and the highest tobacco group?
correct

0.05
Loading

Explanation

The probability can be calculated using the following code:

          high_alc_tob_cases <- esoph %>%
  filter(alcgp == "120+" & tobgp == "30+") %>%
  pull(ncases) %>%
  sum()

p_case_high_alc_tob <- high_alc_tob_cases/all_cases
p_case_high_alc_tob
        

You have used 2 of 10 attempts Some


```{r}
esoph


cases_2group <- esoph %>%
  filter(ncases != 0) %>%
  group_by(alcgp, tobgp) %>%
  mutate(n_cases = sum(ncases)) %>%
  select(alcgp, tobgp, n_cases) %>%
  unique()

cases_2group


both_high <- cases_2group %>%
  filter(alcgp == "120+" & tobgp == "30+")

both_high$n_cases/sum(cases_2group$n_cases)
10/(5+5+10+7+6+19+19+9+34+9+7+16+15+17+10+12)
```


##  Question 5d
1.0/1.0 point (graded)
For cases, what is the probability of being in the highest alcohol group or the highest tobacco group?
correct

0.33
Loading

Explanation

The probability can be calculated using the following code:

          p_case_either_highest <- p_case_high_alc + p_case_high_tob - p_case_high_alc_tob
p_case_either_highest
        

You have used 1 of 10 attempts Some


```{r}
cases_2group <- esoph %>%
  filter(ncases != 0) %>%
  group_by(alcgp, tobgp) %>%
  mutate(n_cases = sum(ncases)) %>%
  select(alcgp, tobgp, n_cases) %>%
  unique()

cases_2group


or_high <- cases_2group %>%
  filter(alcgp == "120+" | tobgp == "30+")

sum(or_high$n_cases)/sum(cases_2group$n_cases)
(12+16+7+9+7+10+5)/(5+5+10+7+6+19+19+9+34+9+7+16+15+17+10+12)
```


##  Question 6a
1.0/1.0 point (graded)
For controls, what is the probability of being in the highest alcohol group?

Report your answer to 3 significant figures.
correct

0.0284 or 0.0687
Loading

Explanation

The probability can be calculated using the following code:

          high_alc_controls <- esoph %>%
  filter(alcgp == "120+") %>%
  pull(ncontrols) %>%
  sum()

p_control_high_alc <- high_alc_controls/all_controls
p_control_high_alc
        

Note that depending on your version of R and the dataset you will get a different answer because of bug fixes to the dataset!
You have used 1 of 10 attempts Some


```{r}
esoph


cont_alcogroup <- esoph %>%
  filter(ncontrols != 0) %>%
  group_by(alcgp) %>%
  mutate(n_controls = sum(ncontrols)) %>%
  select(alcgp, n_controls) %>%
  unique()

cont_alcogroup


alco_high <- cont_alcogroup %>%
  filter(alcgp == "120+")

sum(alco_high$n_controls)/sum(cont_alcogroup$n_controls)
```


##  Question 6b
1.0/1.0 point (graded)
How many times more likely are cases than controls to be in the highest alcohol group?

Report your answer to 3 significant figures.
correct

7.93 or 3.27
Loading

Explanation

This calculated using the following code:

p_case_high_alc/p_control_high_alc

Note that depending on your version of R and the dataset you will get a different answer because of bug fixes to the dataset!
You have used 2 of 10 attempts Some problems have options such as save, reset, hints, or show answer. These options follow the Submit button. 


# Keep thinking, instead of jumpping or guessing or assumption
```{r}
cas_alcogroup <- esoph %>%
  filter(ncases != 0) %>%
  group_by(alcgp) %>%
  mutate(n_ncases = sum(ncases)) %>%
  select(alcgp, n_ncases) %>%
  unique()

cas_alcogroup


cas_alcohigh <- cas_alcogroup %>%
  filter(alcgp == "120+")



con_alcogroup <- esoph %>%
  filter(ncontrols != 0) %>%
  group_by(alcgp) %>%
  mutate(n_ncontrols = sum(ncontrols)) %>%
  select(alcgp, n_ncontrols) %>%
  unique()

con_alcogroup


con_alcohigh <- con_alcogroup %>%
  filter(alcgp == "120+")

(cas_alcohigh$n_ncases/sum(cas_alcogroup$n_ncases))/(con_alcohigh$n_ncontrols/sum(con_alcogroup$n_ncontrols))
```


##  Question 6c
1/1 point (graded)
For controls, what is the probability of being in the highest tobacco group?

Report your answer to 3 significant figures.
correct

0.0658 or 0.0841
Loading

Explanation

The probability can be calculated using the following code:

          high_tob_controls <- esoph %>%
  filter(tobgp == "30+") %>%
  pull(ncontrols) %>%
  sum()

p_control_high_tob <- high_tob_controls/all_controls
p_control_high_tob
        

Note that depending on your version of R and the dataset you will get a different answer because of bug fixes to the dataset!
You have used 1 of 10 attempts Some


```{r}
esoph


conts <- esoph %>%
  filter(ncontrols != 0)

cont_hightob <- conts %>%
  filter(tobgp == "30+")


cont_hightob


sum(cont_hightob$ncontrols)/sum(conts$ncontrols)
```


##  Question 6d
1/1 point (graded)
For controls, what is the probability of being in the highest alcohol group and the highest tobacco group?

Report your answer to 3 significant figures.
correct

0.00387 or 0.0133
Loading

Explanation

The probability can be calculated using the following code:

          high_alc_tob_controls <- esoph %>%
  filter(alcgp == "120+" & tobgp == "30+") %>%
  pull(ncontrols) %>%
  sum()

p_control_high_alc_tob <- high_alc_tob_controls/all_controls
p_control_high_alc_tob
        

Note that depending on your version of R and the dataset you will get a different answer because of bug fixes to the dataset!
You have used 1 of 10 attempts Some


```{r}
esoph


conts <- esoph %>%
  filter(ncontrols != 0)

cont_2high <- conts %>%
  filter(tobgp == "30+" & alcgp == "120+")


cont_2high


sum(cont_2high$ncontrols)/sum(conts$ncontrols)
```


##  Question 6e
1/1 point (graded)
For controls, what is the probability of being in the highest alcohol group or the highest tobacco group?

Report your answer to 3 significant figures.
correct

0.0903 or 0.139
Loading

Explanation

The probability can be calculated using the following code:

          p_control_either_highest <- p_control_high_alc + p_control_high_tob - p_control_high_alc_tob
p_control_either_highest
        

Note that depending on your version of R and the dataset you will get a different answer because of bug fixes to the dataset!
You have used 1 of 10 attempts Some


```{r}
esoph


conts <- esoph %>%
  filter(ncontrols != 0)

cont_2high <- conts %>%
  filter(tobgp == "30+" | alcgp == "120+")


cont_2high


sum(cont_2high$ncontrols)/sum(conts$ncontrols)
```


##  Question 6f
1/1 point (graded)
How many times more likely are cases than controls to be in the highest alcohol group or the highest tobacco group?

Report your answer to 3 significant figures.
correct

3.65 or 2.37
Loading

Explanation

This calculated using the following code:

p_case_either_highest/p_control_either_highest

Note that depending on your version of R and the dataset you will get a different answer because of bug fixes to the dataset!
You have used 3 of 10 attempts Some


```{r}
esoph


conts <- esoph %>%
  filter(ncontrols != 0)

cont_2high <- conts %>%
  filter(tobgp == "30+" | alcgp == "120+")


cont_2high



cas <- esoph %>%
  filter(ncases != 0)

cas_2high <- cas %>%
  filter(tobgp == "30+" | alcgp == "120+")


cas_2high


(sum(cas_2high$ncases)/sum(cas$ncases))/(sum(cont_2high$ncontrols)/sum(conts$ncontrols))
# (cas_alcohigh$n_ncases/sum(cas_alcogroup$n_ncases))/(con_alcohigh$n_ncontrols/sum(con_alcogroup$n_ncontrols))
```


## The 1.4 Assessment: Discrete Probability were designed poorly, you are not trying hard enough, just add up similar questions

discussion posted less than a minute ago by john_hhu2020

If you can update the part, it would be a good course, but the instructor seems not really care about the statistic training he can provides to the audience. I suppose he can talked about more info about Montely Hall problem, instead of simulation









## Course  /  Section 2: Continuous Probability  /  Section 2 Overview


# Section 2 Overview



Section 2 introduces you to [][*Continuous Probability*].

After completing Section 2, you will:

[][*        understand the differences between calculating probabilities for discrete and continuous data.*]
[][*        be able to use cumulative distribution functions to assign probabilities to intervals when dealing with continuous data.*]
        be able to use R to generate normally distributed outcomes for use in Monte Carlo simulations.
[][***        know some of the useful theoretical continuous distributions in addition to the normal distribution, such as the student-t, chi-squared, exponential, gamma, beta, and beta-binomial distributions.***] #######################################################

There is 1 assignment that uses the DataCamp platform for you to practice your coding skills as well as a set of questions on the edX platform at the end of section 3.

This section corresponds to the continuous probability section of the course textbook.

We encourage you to use R to interactively test out your answers and further your learning.









## Course  /  Section 2: Continuous Probability  /  2.1 Continuous Probability


# Continuous Probability


[][***Earlier we explained why when summarizing a list of numeric values such as heights, it's not useful to construct a distribution that assigns a proportion to each possible outcome***] (`need to read the text book anyway`).  Note, for example, that if we measure every single person in a very large population with extremely high precision, **because no two people are exactly the same height**, we would need to assign a proportion to each observed value and attain no useful summary at all.  

*Similarly when defining probability distributions, it is not useful to assign a very small probability to every single height*.  Just like when using distributions to summarize numeric data, it is much [][*more practical to define a function that operates on intervals (`timeframe`) rather than single values*].  The standard way of doing this is using the cumulative distribution function.  

We previously described the [][*empirical cumulative distribution function*] (*what is this ??? Go back to read the textbook*)--eCDF--as a basic summary of a list of numeric values.  As an example, we define the eCDF for heights for male adult students.  Here, we define the vector x to use as an example that contains the male heights.  We use this little piece of code.  Now we can define the empirical distribution function like this very simply.  

We just count the number of cases where x is smaller or equal to a and divide by n.  We take the mean, that's the proportion of cases.  So for every value of a, this gives a proportion of values in the list x that are smaller or equal to a.  Note that we have not yet introduced probability.  **We've been talking about list of numbers and summarizing these lists**.  So let's introduce probability.  For example, let's ask if I pick one of the male students at random, what is the chance that he is taller than 70.5 inches?  

Because every student has the same chance of being picked, the answer to this question is the proportion of students that are taller than 70.5.  **Using eCDF, we obtain the answer**.  We simply type 1 minus f of 70, and we get about 37%.  Once the CDF is defined, we can use this to compute the probability of any subset.  For example, the probability of a student being between the height a and the height b is simply f of b minus f of a.  Because we can compute the probability for any possible event this way, the cumulative probability function defines a probability distribution for picking a height at random from our vector of heights x.  


[][Textbook links]

Ths video corresponds to the textbook section on continuous probability.
https://rafalab.github.io/dsbook/probability.html#continuous-probability

The previous discussion of CDF is from the Data Visualization course. Here is the textbook section on the CDF.
https://rafalab.github.io/dsbook/distributions.html#cdf-intro


[][Key points]

    The cumulative distribution function (CDF) is a distribution function for continuous data x that reports the proportion of the data below a for all values of a:

F(a)=Pr(x≤a)

    The CDF is the probability distribution function for continuous variables. For example, to determine the probability that a male student is taller than 70.5 inches given a vector of male heights x, we can use the CDF:

Pr(x>70.5)=1−Pr(x≤70.5)=1−F(70.5)

    The probability that an observation is in between two values a,b is F(b)−F(a).

Code: Cumulative distribution function

Define x as male heights from the dslabs heights dataset:

library(tidyverse)
library(dslabs)
data(heights)
x <- heights %>% filter(sex=="Male") %>% pull(height)

Given a vector x, we can define a function for computing the CDF of x using:

F <- function(a) mean(x <= a)
1 - F(70)    # probability of male taller than 70 inches



```{r}
library(tidyverse)
library(dslabs)


data(heights)

x <- heights %>%
  filter(sex=='Male') %>%
  .$height


length(x)
x[1:7]
```

```{r}
F <- function(a) mean(x<a)
# Think about it, how can we get what we wanted value through this function?


F(70)

```



![](C:/Users/qp/Pictures/the standard way of doing operate the intervals statistics is cdf.png)

![](C:/Users/qp/Pictures/we previously described the empirical commulative distribution function as a bacis summary of a list of numeric value.png)

![](C:/Users/qp/Pictures/we defind a ecdf from the male height.png)

![](C:/Users/qp/Pictures/what is the chance our random selected male height is talled that 70.5 inches.png)

![](C:/Users/qp/Pictures/once the cdf is defined, we can use it to compute the probability of any subset.png)










# Theoretical Distribution


# ==============================================================================================================
In the data visualization module, [][*we introduced the normal distribution as a useful approximation to many naturally occurring distributions*], including that of height.  The cumulative distribution for the normal distribution is defined by a mathematical formula, which in R can be obtained with the function pnorm().  We say that a random quantity is normally distributed with average, avg, and standard deviation, s, if its probability distribution is defined by f of a equals pnorm a, average, s.  This is the code.  **This is useful, because if we are willing to use the normal approximation for say, height, we don't need the entire dataset to answer questions such as, what is the probability that a randomly selected student is taller than 70.5 inches**.  

*We just need the average height and the standard deviation*.  Then we can use this piece of code, 1 minus pnorm 70.5 mean of x, sd of x.  And that gives us the answer of 0.37.  [][*The normal distribution is derived mathematically*].  Apart from computing the average and the standard deviation, we don't use data to define it.  Also [][*the normal distribution is defined for continuous variables*].  It is not described for discrete variables.  However, ***for practicing data scientists, pretty much everything we do involves data, which is technically speaking discrete***.  For example, we could consider our adult data categorical with each specific height a unique category.  The probability distribution would then be defined by the proportion of students reporting each of those unique heights.  

Here is what a plot of that would look like.  This would be the distribution function for those *categories*.  So each reported height gets a probability defined by the proportion of students reporting it.  Now while most students rounded up their height to the nearest inch, others reported values with much more precision.  For example, student reported his height to be 69.6850393700787.  What is that about?  What's that very, very precise number?  Well, it turns out, that's 177 centimeters.  So the student converted it to inches, and copied and pasted the result into the place where they had to report their heights.  **The probability assigned to this height is about 0.001.  It's 1 in 708.  However, the probability for 70 inches is 0.12**.  This is much higher than what was reported with this other value.  But does it really make sense to think that the probability of being exactly 70 inches is so much higher than the probability of being 69.68?  Clearly, it is much more useful for data analytic purposes to treat this outcome as a continuous numeric variable. ( Think, Think, Think )  

But keeping in mind that very few people, perhaps none, are exactly 70 inches.  But rather, that people rounded to the nearest inch.  *With continuous distributions, the probability of a singular value is not even defined*.  For example, it does not make sense to ask what is the probability that a normally distributed value is 70.  Instead, we define probabilities for intervals.  So we could ask instead, what is a probability that someone is between 69.99 and 70.01.  [][*In cases like height in which the data is rounded, the normal approximation is particularly useful if we deal with intervals that include exactly one round number*].  So for example, the normal distribution is useful for approximating the proportion of students reporting between 69.5 and 70.5.  

Here are three other examples.  Look at the numbers that are being reported.  This is using the data, the actual data, not the approximation.  Now look at what we get when we use the approximation.  We get almost the same values.  For these particular intervals, the normal approximation is quite useful.  [][*However, the approximation is not that useful for other intervals.  For example, those that don't include an integer*].  Here are two examples.  If we use these two intervals, again, this is the data, look at the approximations now with the normal distribution.  They're not that good.  In general, we call this situation discretization.  Although the true height distribution is continuous, the reported heights tend to be more common at discrete values, in this case, due to rounding.  As long as we are aware of how to deal with this reality, the normal approximation can still be a very useful tool.  


```{r}
library(tidyverse)
library(dslabs)


data(heights)
x <- heights %>% 
  filter(sex=="Male") %>% 
  pull(height)

x[1:30]
```

```{r}
1 - pnorm(70.5, mean(x), sd(x))
```

```{r}
plot(prop.table(table(x)), xlab='a=Height in inches', ylabel='Pr(X=a)')
```

```{r}
mean(x <= 68.5) - mean(x <= 67.5)
mean(x <= 69.5) - mean(x <= 68.5)
mean(x <= 70.5) - mean(x <= 69.5)

pnorm(68.5, mean(x), sd(x)) - pnorm(67.5, mean(x), sd(x))
pnorm(69.5, mean(x), sd(x)) - pnorm(68.5, mean(x), sd(x))
pnorm(70.5, mean(x), sd(x)) - pnorm(69.5, mean(x), sd(x))


# Why ???
mean(x <= 70.9) - mean(x <= 70.1)
pnorm(70.9, mean(x), sd(x)) - pnorm(70.1, mean(x), sd(x))

# ===============================================================================================================
# the approximation with normal distribution is totally different than the 
```


[][Textbook link]

This video corresponds to the textbook section on the theoretical distribution and the normal approximation.
https://rafalab.github.io/dsbook/probability.html#theoretical-continuous-distributions


[][Key points]

[][*    pnorm(a, avg, s) gives the value of the cumulative distribution function F(a) for the normal distribution defined by average avg and standard deviation s.*]

[][*    We say that a random quantity is normally distributed with average avg and standard deviation s if the approximation pnorm(a, avg, s) holds for all values of a.]
    If we are willing to use the normal approximation for height, we can estimate the distribution simply from the mean and standard deviation of our values.
[][*    If we treat the height data as discrete rather than categorical, we see that the data are not very useful because integer values are more common than expected due to rounding. This is called discretization.*]
*============================================================================================================*
[][*    With rounded data, the normal approximation is particularly useful when computing probabilities of intervals of length 1 that include exactly one integer.*]   *Sorry I didn't get it*

Code: Using pnorm() to calculate probabilities

Given male heights x:

library(tidyverse)
library(dslabs)
data(heights)
x <- heights %>% filter(sex=="Male") %>% pull(height)

We can estimate the probability that a male is taller than 70.5 inches using:

1 - pnorm(70.5, mean(x), sd(x))

Code: Discretization and the normal approximation

# plot distribution of exact heights in data
plot(prop.table(table(x)), xlab = "a = Height in inches", ylab = "Pr(x = a)")

# probabilities in actual data over length 1 ranges containing an integer
mean(x <= 68.5) - mean(x <= 67.5)
mean(x <= 69.5) - mean(x <= 68.5)
mean(x <= 70.5) - mean(x <= 69.5)

# probabilities in normal approximation match well
pnorm(68.5, mean(x), sd(x)) - pnorm(67.5, mean(x), sd(x))
pnorm(69.5, mean(x), sd(x)) - pnorm(68.5, mean(x), sd(x))
pnorm(70.5, mean(x), sd(x)) - pnorm(69.5, mean(x), sd(x))

# probabilities in actual data over other ranges don't match normal approx as well
mean(x <= 70.9) - mean(x <= 70.1)
pnorm(70.9, mean(x), sd(x)) - pnorm(70.1, mean(x), sd(x))




![normal distribution is a useful approximation to many naturally occurring distributions.png](C:/Users/qp/Pictures/normal distribution is a useful approximation to many naturally occurring distributions.png)

![the cumulative distribution for the normal distribution is defined by a mathematical formula.png](C:/Users/qp/Pictures/the cumulative distribution for the normal distribution is defined by a mathematical formula.png)

![pnorm function in r.png](C:/Users/qp/Pictures/pnorm function in r.png)

![pnorm function require the random quantity with normally distributed avg and standard deviation s.png](C:/Users/qp/Pictures/pnorm function require the random quantity with normally distributed avg and standard deviation s.png)

![so what is the probability of randomly selected student is taller than 70.5 inches.png](C:/Users/qp/Pictures/so what is the probability of randomly selected student is taller than 70.5 inches.png)

# The normal distribution is defined for continous variables, it is not described for discrete variables.  Whereas for data science, pretty much every data involved is technically discrete

![consider our adult data categorical with each specific height a unique category.png](C:/Users/qp/Pictures/consider our adult data categorical with each specific height a unique category.png)

![](C:/Users/qp/Pictures/and this would be the distribution function for those categories.png)

![](C:/Users/qp/Pictures/how about one student report his height with much more precision like this value.png)

![](C:/Users/qp/Pictures/if we change the unit, that high presion value would be 177 center meters.png)

![](C:/Users/qp/Pictures/and the probability assigned to this height is about this value.png)

![](C:/Users/qp/Pictures/or it can be say as the 1 in 708.png)

![](C:/Users/qp/Pictures/with continuous distributions, we define probabilities for intervals, the probability of single value is not defined.png)

![](C:/Users/qp/Pictures/here are 3 examples, which we deal with intervals that include the exactly one round number.png)

![and here is the numbers being reported, this is using the actual data not the approximation.png](C:/Users/qp/Pictures/and here is the numbers being reported, this is using the actual data not the approximation.png)

![and here is what we got when use the approximation.png](C:/Users/qp/Pictures/and here is what we got when use the approximation.png)

![check this example when we use 2 intervals and the approximations when not include an interger.png](C:/Users/qp/Pictures/check this example when we use 2 intervals and the approximations when not include an interger.png)

![In general, we call this situation discretization.png](C:/Users/qp/Pictures/In general, we call this situation discretization.png)









## Probability Density


For categorical data, we can define the probability of a category.  For example, a roll of a die, let's call it x, can be 1, 2, 3, 4, 5, or 6.  The probability of 4 is defined as probability of x equals 4 is 1/6.  The CDF can easily be defined by simply adding up probability.  So F of 4 is the probability of x being less than 4, which is the probability of x being 4, or 3, or 2, or 1.  So we just add up those probabilities.  

In contrast, [][*for continuous distributions, the probability of a single value is not defined*] (*How, I dont understand*).  However, there is a theoretical definition that has a similar interpretation.  This has to do with the probability density.  The probability density at x is defined as the function, we're going to call it little f of x, such that the probability distribution big F of a, which is the probability of x being less than or equal to a, is the integral of all values up to a of little f of x dx.  

For those that know calculus, remember, that the integral is related to a sum.  It's the sum of bars with widths that are approximating 0.  If you don't know calculus, you can think of little f of x as a curve for which the area under the curve up to the value a gives you the probability of x being less than or equal to a.  For example, to use the normal approximation to estimate the probability of someone being taller than 76 inches, we can use the probability density.  So this is a mathematical formula.  Mathematically, the gray area in this figure represents the probability of x being bigger than 76.  The curve you see is a probability density function for the normal distribution.  

In R you get the probability density function for the normal distribution using the function dnorm().  D stands for density.  Although it may not be immediately obvious why knowing about probability densities is useful, [][*understanding this concept will be essential to those wanting to fit models to data for which predefined functions are not available*].  *++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++=*


# Estimate the probability of someone being taller than 76 inches
```{r}
avg <- mean(x)
s <- sd(x)

1 - pnorm(76, avg, s)
```



[][Textbook link]

This video corresponds to the textbook section on probability density.
https://rafalab.github.io/dsbook/probability.html#the-probability-density


[][Key points]

[][*    The probability of a single value is not defined for a continuous distribution.*]
    The quantity with the most similar interpretation to the probability of a single value is the probability density function (f(x).
    The probability density f(x) is defined such that the integral of f(x) over a range gives the CDF of that range.

F(a) = Pr(X<=a) = f_(-infty~a) f(x) dx

    In R, the probability density function for the normal distribution is given by dnorm(). We will see uses of dnorm() in the future.
    Note that dnorm() gives the density function and pnorm() gives the distribution function, which is the integral of the density function.



![](C:/Users/qp/Pictures/row a diegives you the probability of a category.png)

![](C:/Users/qp/Pictures/and this the cdf, can easily be defined by adding up probabilities.png)

![](C:/Users/qp/Pictures/for continous distribution the single value is not defined in cdf but there is a theoretical definition.png)

![](C:/Users/qp/Pictures/the probabiliti density at x is defined as this function.png)

![](C:/Users/qp/Pictures/if you don't know calculus you can think fx as a curve for whitch the area under the curve adds up to a.png)

![](C:/Users/qp/Pictures/for example to use the normal approximation to estimate the probability of someone being taller than 76 inches.png)

![the curve we see here is the probability density function for the normal distribution](C:/Users/qp/Pictures/represent the probability of x being bigger than 76.png)

![](C:/Users/qp/Pictures/you use dnorm function to get the probability dnesity function for the normal distribution.png)








## Plotting the Probability Density


Plotting the probability density for the normal distribution

We can use dnorm() to plot the [][*density curve*] for the normal distribution. dnorm(z) gives the probability density f(x) of a certain z-score, so we can draw a curve by calculating the density over a range of possible values of z.

First, we generate a series of z-scores covering the typical range of the normal distribution. Since we know 99.7% of observations will be within -3 <= z <= 3, we can use a value of z slightly larger than 3 and this will cover most likely values of the normal distribution. Then, we calculate f(z), which is dnorm() of the series of z-scores. Last, we plot z against f(z).

library(tidyverse)
x <- seq(-4, 4, length = 100)
data.frame(x, f = dnorm(x)) %>%
    ggplot(aes(x, f)) +
    geom_line()

```{r}
library(tidyverse)

x <- seq(-4, 4, length = 10)
data.frame(x, f = dnorm(x)) %>%
    ggplot(aes(x, f)) +
    geom_line()
```


Here is the resulting plot:
![](C:/Users/qp/Pictures/normal-density-1.png)

Plot of the normal distribution generated using the dnorm function.

Note that dnorm() gives densities for the standard normal distribution by default. Probabilities for alternative normal distributions with mean mu and standard deviation sigma can be evaluated with:

[][*dnorm(z, mu, sigma)*]









## Monte Carlo Simulations


In this video, we're going to show you how to run Monte Carlo simulations using normally distributed variables.  R provides a function to generate normally distributed outcomes.  Specifically, the rnorm() function takes three arguments-- size; average, which defaults to 0; standard deviation, which defaults to 1--and produces these random numbers.  Here's an example of how we can generate data that looks like our reported heights.  

So if our reported heights are in the vector x, we compute their length, their average, and their standard deviation, and then use the rnorm() function to generate the randomly distributed outcomes.  Not surprisingly, the distribution of **these outcomes looks normal because they were generated to look normal**.  [][*This is one of the most useful functions in R, as it will permit us to generate data that mimics naturally occurring events*], and it'll let us answer questions related to what could happen by chance by running Monte Carlo simulations.  

For example, if we pick 800 males at random, what is the distribution of the tallest person?  Specifically, we could ask, how rare is that the tallest person is a seven footer?  We can use the following Monte Carlo simulation to answer this question.  We're going to run 10,000 simulations, and for each one, we're going to generate 800 normally distributed values, pick the tallest one, and return that.  The tallest variable will have these values.  So now we can ask, what proportion of these simulations return a seven footer as the tallest person?  And we can see that it's a very small number.  


[][Textbook link]

This video corresponds to the textbook section on Monte Carlo simulations for continuous variables.
https://rafalab.github.io/dsbook/probability.html#monte-carlo-simulations-for-continuous-variables


[][Key points]

    rnorm(n, avg, s) generates n random numbers from the normal distribution with average avg and standard deviation s.
    By generating random numbers from the normal distribution, we can simulate height data with similar properties to our dataset. Here we generate simulated height data using the normal distribution.

Code: Generating normally distributed random numbers

# define x as male heights from dslabs data
library(tidyverse)
library(dslabs)
data(heights)
x <- heights %>% filter(sex=="Male") %>% pull(height)

# generate simulated height data using normal distribution - both datasets should have n observations
n <- length(x)
avg <- mean(x)
s <- sd(x)
simulated_heights <- rnorm(n, avg, s)

# plot distribution of simulated_heights
data.frame(simulated_heights = simulated_heights) %>%
    ggplot(aes(simulated_heights)) +
    geom_histogram(color="black", binwidth = 2)

Code: Monte Carlo simulation of tallest person over 7 feet

B <- 10000
tallest <- replicate(B, {
    simulated_data <- rnorm(800, avg, s)    # generate 800 normally distributed random heights
    max(simulated_data)    # determine the tallest height
})
mean(tallest >= 7*12)    # proportion of times that tallest person exceeded 7 feet (84 inches)



```{r}
x <- heights %>%
  filter(sex=='Male') %>%
  .$height

n <- length(x)
acg <- mean(x)
s <- sd(x)

simulated_height <- rnorm(n, avg, s)   # ========================================================================
dim(simulated_height)
# The rnorm() function in R generates a random number using a normal(bell curve) distribution. Thus, the rnorm() function simulates random variates having a specified normal distribution.21 Dec 2021


ds_theme_set()
data.frame(simulated_height=simulated_height) %>%
  ggplot(aes(simulated_height)) +
  geom_histogram(color='white', bins = 30)   # This is a histogram, so what does rnorm() function returns us ???


y <- heights %>%
  filter(sex=='Male') %>%
  .$height

data.frame(Mheight=y) %>%
  ggplot(aes(Mheight)) +
  geom_histogram(color='orange', bins=30)
```

```{r}
B <- 10000

tallest <- replicate(B, {
  simulated_data <- rnorm(800, avg, s)
  max(simulated_data)
})


mean(tallest >= 7*12)
```




![](C:/Users/qp/Pictures/the rnorm function helps us generate normally distributed outcomes.png)

![](C:/Users/qp/Pictures/here is an example of how we can generate the data that looks like our reported height.png)

![](C:/Users/qp/Pictures/now lets check the distribution of these outcome.png)

![](C:/Users/qp/Pictures/the plot seems a normal distributioned from those outcomes.png)

![](C:/Users/qp/Pictures/we are using normal distribution assumption and generate 800 normal distributed values and pick the tallest one.png)

![](C:/Users/qp/Pictures/then we can ask what proportion of these simulations returns 7 feet as the tallest person.png)









# Other Continuous Distributions


The normal distribution is not the only useful theoretical distribution.  Other continuous distributions that we may encounter are [][*student-t, the chi-squared, the exponential, the gamma, and the beta distribution.  *]

R provides functions to compute the density, the quantiles, the cumulative distribution function, and to generate Monte Carlo simulations for all these distributions.  ***R uses a convention that lets us remember the names of these functions.  Namely, using the letters d for density, q for quantile, p for probability density function, and r for random.  By putting these letters in front of a shorthand for the distribution, gives us the names of these useful functions.  ***

We have already seen the functions dnorm, pnorm, and rnorm.  So these are examples of what we just described.  These are for the normal distribution.  Norm is the nickname or shorthand for the normal distribution.  The function qnorm gives us the quantiles.  For example, we can use the dnorm function to generate this plot.  This is the density function for the normal distribution.  We use the function dnorm.  For the student's t distribution, which has shorthand t, we can use the functions dt, qt, pt, and rt to generate the density, quantiles, probability density function, or a Monte Carlo simulation.  


[][Textbook link]

This video corresponds to the textbook section on other continuous distributions.
https://rafalab.github.io/dsbook/probability.html#continuous-distributions


[][Key points]

    You may encounter other continuous distributions (Student t, chi-squared, exponential, gamma, beta, etc.).
[][*    R provides functions for density (d), quantile (q), probability distribution (p) and random number generation (r) for many of these distributions.*]
[][*    Each distribution has a matching abbreviation (for example, norm() or t()) that is paired with the related function abbreviations (d, p, q, r) to create appropriate functions.*]
    For example, use rt() to generate random numbers for a Monte Carlo simulation using the Student t distribution.

Code: Plotting the normal distribution with dnorm

Use d to plot the density function of a continuous distribution. Here is the density function for the normal distribution (abbreviation norm()):

x <- seq(-4, 4, length.out = 100)
data.frame(x, f = dnorm(x)) %>%
    ggplot(aes(x,f)) +
    geom_line()



![](C:/Users/qp/Pictures/other continuous distribution student-t.png)

![](C:/Users/qp/Pictures/other continuous distribution chi-squared.png)

![](C:/Users/qp/Pictures/other continuous distribution exponential.png)

![](C:/Users/qp/Pictures/other continuous distribution gama.png)

![](C:/Users/qp/Pictures/other continuous distribution beta.png)

![](C:/Users/qp/Pictures/r provides functions to compute the density .png)

![](C:/Users/qp/Pictures/r provides functions to compute the quantiles.png)

![](C:/Users/qp/Pictures/r provides function to compute the cumulative distribution functions.png)

![](C:/Users/qp/Pictures/and provides the functuions to generate the monte carlo simulations.png)

![](C:/Users/qp/Pictures/r use a convention that let us remember the names of these functions, d for density.png)

![](C:/Users/qp/Pictures/r use a convention that let us remember the names of these functions, q for quantile.png)

![](C:/Users/qp/Pictures/r use a convention that let us remember the names of these functions, p for probability density function.png)

![](C:/Users/qp/Pictures/r use a convention that let us remember the names of these functions, r for random.png)

![](C:/Users/qp/Pictures/we have already seen the function dnorm.png)

![](C:/Users/qp/Pictures/we have already seen the function pnorm.png)

![](C:/Users/qp/Pictures/we have already seen the function rnorm.png)

![](C:/Users/qp/Pictures/the norm in r is the nickname for normal distribution.png)

![](C:/Users/qp/Pictures/the function qnorm gives us the quantiles.png)

![](C:/Users/qp/Pictures/we can use dnorm function to generate this plot.png)

![](C:/Users/qp/Pictures/use dnorm function to generate this plot, do you remember it.png)

![](C:/Users/qp/Pictures/for the student-t distribution we can use the functions dt and others.png)

![](C:/Users/qp/Pictures/in student-t distribution we can use dt, qt, pt, rt functions.png)

![](C:/Users/qp/Pictures/d for density, q for quantitles, p for density, or r for random simulation.png)









## DataCamp Assessment: Continuous Probability



Assessment due Jun 26, 2022 15:31 AWST

This assessment covers the basics of continuous probability.

By clicking OK, you agree to DataCamp's privacy policy: https://www.datacamp.com/privacy-policy. Note that you might need to disable your pop-up blocker, or allow "www.datacamp.com" in your pop-up blocker allowed list. When you have completed the exercises, return to edX to continue your learning.
Assessment: Continuous Probability (External resource) (7.5 points possible)
By clicking OK, you agree to DataCamp's privacy policy: https://www.datacamp.com/privacy-policy.


## Exercise 1. Distribution of female heights - 1

Assume the distribution of female heights is approximated by a normal distribution with a mean of 64 inches and a standard deviation of 3 inches. If we pick a female at random, what is the probability that she is 5 feet or shorter?
Instructions
100 XP

    Use pnorm to define the probability that a height will take a value less than 5 feet given the stated distribution.

```{r}
# Assign a variable 'female_avg' as the average female height.
female_avg <- 64

# Assign a variable 'female_sd' as the standard deviation for female heights.
female_sd <- 3

# Using variables 'female_avg' and 'female_sd', calculate the probability that a randomly selected female is shorter than 5 feet. Print this value to the console.

pnorm(5*12, female_avg, female_sd)
```


## Exercise 2. Distribution of female heights - 2

Assume the distribution of female heights is approximated by a normal distribution with a mean of 64 inches and a standard deviation of 3 inches. If we pick a female at random, what is the probability that she is 6 feet or taller?
Instructions
100 XP

    Use pnorm to define the probability that a height will take a value of 6 feet or taller.

```{r}
# Assign a variable 'female_avg' as the average female height.
female_avg <- 64

# Assign a variable 'female_sd' as the standard deviation for female heights.
female_sd <- 3

# Using variables 'female_avg' and 'female_sd', calculate the probability that a randomly selected female is 6 feet or taller. Print this value to the console.
1 - pnorm(6*12, female_avg, female_sd)
```


## Exercise 3. Distribution of female heights - 3

Assume the distribution of female heights is approximated by a normal distribution with a mean of 64 inches and a standard deviation of 3 inches. If we pick a female at random, what is the probability that she is between 61 and 67 inches?
Instructions
100 XP

    Use pnorm to define the probability that a randomly chosen woman will be shorter than 67 inches.
    Subtract the probability that a randomly chosen will be shorter than 61 inches.

```{r}
# Assign a variable 'female_avg' as the average female height.
female_avg <- 64

# Assign a variable 'female_sd' as the standard deviation for female heights.
female_sd <- 3

# Using variables 'female_avg' and 'female_sd', calculate the probability that a randomly selected female is between the desired height range. Print this value to the console.
pnorm(67, female_avg, female_sd) - pnorm(61, female_avg, female_sd)
```


## Exercise 4. Distribution of female heights - 4

Repeat the previous exercise, but convert everything to centimeters. That is, multiply every height, including the standard deviation, by 2.54. What is the answer now?
Instructions
100 XP

    Convert the average height and standard deviation to centimeters by multiplying each value by 2.54.
    Repeat the previous calculation using pnorm to define the probability that a randomly chosen woman will have a height between 61 and 67 inches, converted to centimeters by multiplying each value by 2.54.

```{r}
# Assign a variable 'female_avg' as the average female height. Convert this value to centimeters.
female_avg <- 64*2.54

# Assign a variable 'female_sd' as the standard deviation for female heights. Convert this value to centimeters.
female_sd <- 3*2.54

# Using variables 'female_avg' and 'female_sd', calculate the probability that a randomly selected female is between the desired height range. Print this value to the console.
pnorm(67*2.54, female_avg, female_sd) - pnorm(61*2.54, female_avg, female_sd)
```


## Exercise 5. Probability of 1 SD from average

Compute the probability that the height of a randomly chosen female is within 1 SD from the average height.
Instructions
100 XP

    Calculate the values for heights one standard deviation taller and shorter than the average.
    Calculate the probability that a randomly chosen woman will be within 1 SD from the average height.

```{r}
# Assign a variable 'female_avg' as the average female height.
female_avg <- 64

# Assign a variable 'female_sd' as the standard deviation for female heights.
female_sd <- 3

# To a variable named 'taller', assign the value of a height that is one SD taller than average.
taller <- female_avg + female_sd

# To a variable named 'shorter', assign the value of a height that is one SD shorter than average.
shorter <- female_avg - female_sd

# Calculate the probability that a randomly selected female is between the desired height range. Print this value to the console.
pnorm(taller, female_avg, female_sd) - pnorm(shorter, female_avg, female_sd)
```


## Exercise 6. Distribution of male heights

Imagine the distribution of male adults is approximately normal with an average of 69 inches and a standard deviation of 3 inches. How tall is a male in the 99th percentile?
Instructions
100 XP

    Determine the height of a man in the 99th percentile, given an average height of 69 inches and a standard deviation of 3 inches.

```{r}
# Assign a variable 'male_avg' as the average male height.
male_avg <- 69

# Assign a variable 'male_sd' as the standard deviation for male heights.
male_sd <- 3

# Determine the height of a man in the 99th percentile of the distribution.
qnorm(0.99, mean=male_avg, sd=male_sd)
```


## Exercise 7. Distribution of IQ scores

The distribution of IQ scores is approximately normally distributed. The average is 100 and the standard deviation is 15. Suppose you want to know the distribution of the person with the highest IQ in your school district, where 10,000 people are born each year.

Generate 10,000 IQ scores 1,000 times using a Monte Carlo simulation. Make a histogram of the highest IQ scores.
Instructions
100 XP

    Use the function rnorm to generate a random distribution of 10,000 values with a given average and standard deviation.
    Use the function max to return the largest value from a supplied vector.
    Repeat the previous steps a total of 1,000 times. Store the vector of the top 1,000 IQ scores as highestIQ.
    Plot the histogram of values using the function hist.

Incorrect submission
Make sure to use the function rnorm to generate a distribution of 10,000 IQ scores in every iteration. 

# Read the statement careful
```{r}
# The variable `B` specifies the number of times we want the simulation to run.
B <- 1000

# Use the `set.seed` function to make sure your answer matches the expected result after random number generation.
set.seed(1)

# Create an object called `highestIQ` that contains the highest IQ score from each random distribution of 10,000 people.
highestIQ <- replicate(B, {
    simulated_dat <- rnorm(10000, mean=100, sd=15)
    max(simulated_dat)
})

# Make a histogram of the highest IQ scores.
hist(highestIQ)
```


## End of Assessment: Continuous Probability

This is the end of the programming assignment for this section. Please DO NOT click through to additional assessments from this page. Please DO answer the question on this page. If you do click through, your scores may NOT be recorded.

Click on "Awesome" to get the "points" for this question and then return to the course on edX.

You can close this window and return to Data Science: Probability.
Answer the question
50XP
Possible Answers

    Awesome
    press
    1
    Nope
    press
    2







## Course  /  Section 2: Continuous Probability  /  2.2 Assessment: Continuous Probability


# Questions 1 and 2: ACT scores, part 1



Assessment due Jun 26, 2022 15:31 AWST

The ACT is a standardized college admissions test used in the United States. The four multi-part questions in this assessment all involve simulating some ACT test scores and answering probability questions about them.

For the three year period 2016-2018, ACT standardized test scores were approximately normally distributed with a mean of 20.9 and standard deviation of 5.7. (Real ACT scores are integers between 1 and 36, but we will ignore this detail and use continuous values instead.)

First we'll simulate an ACT test score dataset and answer some questions about it.

Set the seed to 16, then use rnorm() to generate a normal distribution of 10000 tests with a mean of 20.9 and standard deviation of 5.7. Save these values as act_scores. You'll be using this dataset throughout these four multi-part questions.

(IMPORTANT NOTE! If you use R 3.6 or later, you will need to use the command format set.seed(x, sample.kind = "Rounding") instead of set.seed(x). Your R version will be printed at the top of the Console window when you start RStudio.)



##  Question 1a
1/1 point (graded)
What is the mean of act_scores?
correct

20.8
Loading

Explanation

The mean can be found using the following code:

          set.seed(16, sample.kind = "Rounding")
act_scores <- rnorm(10000, 20.9, 5.7)
mean(act_scores)
        

You have used 1 of 10 attempts Some


##  Question 1b
1/1 point (graded)
What is the standard deviation of act_scores?
correct

5.68
Loading

Explanation

The standard deviation can be found using the following code:

          sd(act_scores)
        

You have used 2 of 10 attempts Some


##  Question 1c
1/1 point (graded)

A perfect score is 36 or greater (the maximum reported score is 36).
In act_scores, how many perfect scores are there out of 10,000 simulated tests?
correct

41
Loading

Explanation

The number of perfect scores can be found using the following code:

          sum(act_scores >= 36)
        

```{r}
set.seed(16, sample.kind = "Rounding")


act_scores <- rnorm(10000, mean=20.9, sd=5.7)

mean(act_scores)
sd(act_scores)


act_scores[1:5]
sum(act_scores>=36)


mean(act_scores>=30)


mean(act_scores<=10)
```

##  Question 1d
1/1 point (graded)
In act_scores, what is the probability of an ACT score greater than 30?
correct

0.0527
Loading

Explanation

The probability can be found using the following code:

          mean(act_scores > 30)
        

You have used 1 of 10 attempts Some


##  Question 1e
1/1 point (graded)
In act_scores, what is the probability of an ACT score less than or equal to 10?
correct

0.0282
Loading

Explanation

The probability can be found using the following code:

          mean(act_scores <= 10)
        

You have used 1 of 10 attempts Some


```{r}
x <- seq(36)

x

f_x <- dnorm(x, 20.9, 5.7)


plot(x, f_x)
```


## Question 2
1/1 point (graded)

Set x equal to the sequence of integers 1 to 36. Use dnorm to determine the value of the probability density function over x given a mean of 20.9 and standard deviation of 5.7; save the result as f_x. Plot x against f_x.
Which of the following plots is correct?
f_x on y-axis, x on x-axis. Curve starts at (0,0) and increases with slight s-shape to about (36,1).
f_x on y-axis, x on x-axis. Bell-shaped curve that starts at roughly (0,0) with peak at roughly (22,0.07) and ends at about (36,0.001).
f_x on y-axis, x on x-axis. Curve starts at roughly (1,0.24) and drops steeply to (4,0) and remains flat at y=0 through x=36.
f_x on y-axis, x on x-axis. Bell-shaped curve that starts at roughly (-3.5,0) with peak at roughly (0,0.4) and ends at about (2.5,0.001).
correct

Explanation

The second plot, generated using the following code, is correct:

![](C:/Users/qp/Pictures/IDS_Mod3_Sect2_Q2_imageA.png)

![](C:/Users/qp/Pictures/IDS_Mod3_Sect2_Q2_imageB.png)

![](C:/Users/qp/Pictures/IDS_Mod3_Sect2_Q2_imageC.png)

![](C:/Users/qp/Pictures/IDS_Mod3_Sect2_Q2_imageD.png)

          x <- 1:36
f_x <- dnorm(x, 20.9, 5.7)
data.frame(x, f_x) %>%
  ggplot(aes(x, f_x)) +
  geom_line()
        

The first plot is the distribution function rather than the density function. The third plot fails to include the mean and standard deviation in the dnorm call. The fourth plot is of Z-score values.
You have used 1 of 2 attempts Some








# Questions 3 and 4: ACT scores, part 2



Assessment due Jun 26, 2022 15:31 AWST

In this 3-part question, you will convert raw ACT scores to Z-scores and answer some questions about them.

Convert act_scores to Z-scores. Recall from Data Visualization (the second course in this series) that to standardize values (convert values into Z-scores, that is, values distributed with a mean of 0 and standard deviation of 1), you must subtract the mean and then divide by the standard deviation. Use the mean and standard deviation of act_scores, not the original values used to generate random test scores.


##  Question 3a
1.0/1.0 point (graded)
What is the probability of a Z-score greater than 2 (2 standard deviations above the mean)?
correct

0.0233
Loading

Explanation

The probability can be calculated using the following code:

          z_scores <- (act_scores - mean(act_scores))/sd(act_scores)
mean(z_scores > 2)
        

You have used 1 of 10 attempts Some


##  Question 3b
1/1 point (graded)
What ACT score value corresponds to 2 standard deviations above the mean (Z = 2)?
correct

32.2
Loading

Explanation

The score value can be calculated using the following code:

          2*sd(act_scores) + mean(act_scores)
        

You have used 1 of 10 attempts Some


##  Question 3c
1/1 point (graded)

A Z-score of 2 corresponds roughly to the 97.5th percentile.

Use qnorm() to determine the 97.5th percentile of normally distributed data with the mean and standard deviation observed in act_scores.
What is the 97.5th percentile of act_scores?
correct

32.0
Loading

Explanation

The 97.5th percentile can be calculated using the following code:

          qnorm(.975, mean(act_scores), sd(act_scores))
        

You have used 1 of 10 attempts Some



# ============================================================================================================
In this 4-part question, you will write a function to create a CDF for ACT scores.

Write a function that takes a value and produces the probability of an ACT score less than or equal to that value (the CDF). Apply this function to the range 1 to 36.


##  Question 4a
1/1 point (graded)
What is the minimum integer score such that the probability of that score or lower is at least .95?

Your answer should be an integer 1-36.
correct

31
Loading

Explanation

The minimum score can be calculated using the following code:

          cdf <- sapply(1:36, function (x){
  mean(act_scores <= x)
})
min(which(cdf >= .95))
        

You have used 1 of 10 attempts 


##  Question 4b
1/1 point (graded)

Use qnorm() to determine the expected 95th percentile, the value for which the probability of receiving that score or lower is 0.95, given a mean score of 20.9 and standard deviation of 5.7.
What is the expected 95th percentile of ACT scores?
correct

30.3
Loading

Explanation

The expected 95th percentile can be calculated using the following code:

          qnorm(.95, 20.9, 5.7)
        

You have used 1 of 10 attempts Some


##  Question 4c
1/1 point (graded)

As discussed in the Data Visualization course, we can use quantile() to determine sample quantiles from the data.

Make a vector containing the quantiles for p <- seq(0.01, 0.99, 0.01), the 1st through 99th percentiles of the act_scores data. Save these as sample_quantiles.
In what percentile is a score of 26?

Your answer should be an integer (i.e. 60), not a percent or fraction. Note that a score between the 98th and 99th percentile should be considered the 98th percentile, for example, and that quantile numbers are used as names for the vector sample_quantiles.
correct

82
Loading

Explanation

The percentile for a score of 26 can be calculated using the following code:

          p <- seq(0.01, 0.99, 0.01)
sample_quantiles <- quantile(act_scores, p)
names(sample_quantiles[max(which(sample_quantiles < 26))])
        

You have used 1 of 10 attempts Some


##  Question 4d
1/1 point (graded)

Make a corresponding set of theoretical quantiles using qnorm() over the interval p <- seq(0.01, 0.99, 0.01) with mean 20.9 and standard deviation 5.7. Save these as theoretical_quantiles. Make a QQ-plot graphing sample_quantiles on the y-axis versus theoretical_quantiles on the x-axis.
Which of the following graphs is correct?
QQ plot with theoretical_quantiles on x-axis and sample_quantiles on y-axis. Points form a straight line from about (-2.25, 7.5) to (2.25, 34).
![](C:/Users/qp/Pictures/IDS_Mod3_Sect2_Q4D_imageA.png)
QQ plot with sample_quantiles on x-axis and theoretical_quantiles on y-axis. Points form a straight line from about (7.5, -2.25) to (34, 2.25).
![](C:/Users/qp/Pictures/IDS_Mod3_Sect2_Q4D_imageB.png)
QQ plot with sample_quantiles on x-axis and theoretical_quantiles on y-axis. Points form a straight line from about (7.5, 7.5) to (34, 34).
![](C:/Users/qp/Pictures/IDS_Mod3_Sect2_Q4D_imageC.png)
QQ plot with theoretical_quantiles on x-axis and sample_quantiles on y-axis. Points form a straight line from about (7.5, 7.5) to (34, 34).
![](C:/Users/qp/Pictures/IDS_Mod3_Sect2_Q4D_imageD.png)
correct

Explanation

The fourth graph is correct and can be generated using the following code:

          p <- seq(0.01, 0.99, 0.01)
sample_quantiles <- quantile(act_scores, p)
theoretical_quantiles <- qnorm(p, 20.9, 5.7)
qplot(theoretical_quantiles, sample_quantiles) + geom_abline()
        

The first graph uses standard normal theoretical quantiles, the result of omitting the mean and standard deviation in qnorm. The second graph uses standard normal theoretical quantiles and inverts the x and y axes. The third graph inverts the x and y axes.
You have used 1 of 2 attempts Some



```{r}
z_act_scores <- scale(act_scores)


mean(z_act_scores > mean(z_act_scores) + 2*sd(z_act_scores))


pnorm(mean(z_act_scores)+2*sd(z_act_scores), mean(z_act_scores), sd(z_act_scores))



mean(act_scores)+2*sd(act_scores)
```

```{r}
qnorm(0.975, mean(act_scores), sd(act_scores))
```

```{r}
v <- seq(36)


my_cdf <- function(v){
 mean(act_scores<=v) 
}


sapply(v, my_cdf)
```

```{r}
qnorm(0.95, mean=20.9, sd=5.7)
```

```{r}
quantile(act_scores)


p <- seq(0.01, 0.99, 0.01)
quantile(act_scores, probs=p)
```

```{r}
p <- seq(0.01, 0.99, 0.01)


p <- seq(0.01, 0.99, 0.01)
sample_quantiles <- quantile(act_scores, p)
names(sample_quantiles[max(which(sample_quantiles < 26))])


theoretical_quantiles <- qnorm(p, mean=20.9, sd=5.7)
theoretical_quantiles




qqplot(x=theoretical_quantiles, y=sample_quantiles)
```








# Questions about Assessment: Continuous Probability?




Ask your questions about the continuous probability assessment here. Remember to search the discussion board before posting to see if someone else has asked the same thing before asking a new question! You're also encouraged to answer each other's questions to help further your own learning.

Some reminders:

    Please be specific in the title and body of your post regarding which question you're asking about to facilitate answering your question.
    Posting snippets of code is okay, but posting full code solutions is not.
    If you do post snippets of code, please format it as code for readability. If you're not sure how to do this, there are instructions in a pinned post in the general discussion forum.

Discussion: Continuous probability assessment
Topic: Section 2 / Assessment: Continuous Probability







## Course  /  Section 3: Random Variables, Sampling Models, and the Central Limit Theorem  /  Section 3 Overview


# Section 3 Overview



Section 3 introduces you to Random Variables, Sampling Models, and the Central Limit Theorem.

Section 3 is divided into two parts:

        Random Variables and Sampling Models
        The Central Limit Theorem.

After completing Section 3, you will:

        understand what random variables are, how to generate them, and the correct mathematical notation to use with them.
        be able to use sampling models to estimate characteristics of a larger population.
        be able to explain the difference between a distribution and a probability distribution.
        understand the Central Limit Theorem and the law of large numbers.

There are 2 assignments that use the DataCamp platform for you to practice your coding skills as well as a set of questions on the edX platform at the end of Section 3.

This section corresponds to the following section of the course textbook.
https://rafalab.github.io/dsbook/random-variables.html

We encourage you to use R to interactively test out your answers and further your learning.







## Course  /  Section 3: Random Variables, Sampling Models, and the Central Limit Theorem  /  3.1 Random Variables and Sampling Models


# Random Variables


Random variables are numeric outcomes resulting from a random process.  We can easily generate random variables using some of the simple examples we have shown, such as the red and blue bead urn.  For example, define x to be 1 if a bead is blue, and red otherwise.  Here's the R code you can write to generate that random variable.  X is a random variable.  Every time we select a new bead, the outcome changes randomly.  Sometimes it's 1, sometimes it's 0.  Here's some examples of how that random variable changes.  We're going to do it three times.  Here is 0, here is 1, and here's 1 again.  In data science, we often deal with data that is affected by chance in some way.  The data comes from a random sample, the data is affected by measurement error, or the data measures some outcome that is random in nature.  [][***Being able to quantify the uncertainty introduced by randomness is one of the most important jobs of a data scientist***].  Statistical inference offers a framework for doing this, as well as several practical tools.  The first step is to learn how to mathematically describe random variables.  We start with games of chance as an illustrative example.  


[][Textbook link]

This video corresponds to the textbook section on random variables External link.
https://rafalab.github.io/dsbook/random-variables.html#random-variables-1



[][Key points]

    Random variables are numeric outcomes resulting from random processes.
    Statistical inference offers a framework for quantifying uncertainty due to randomness.

Code: Modeling a random variable

# define random variable x to be 1 if blue, 0 otherwise
beads <- rep(c("red", "blue"), times = c(2, 3))
x <- ifelse(sample(beads, 1) == "blue", 1, 0)

# demonstrate that the random variable is different every time
ifelse(sample(beads, 1) == "blue", 1, 0)
ifelse(sample(beads, 1) == "blue", 1, 0)
ifelse(sample(beads, 1) == "blue", 1, 0)




```{r}
beads <- rep(c("Red", "Blue"), times=c(2, 3))

x <- ifelse(sample(beads, 1)=="Blue", 1, 0)
x
```



![](C:/Users/qp/Pictures/Random variables are numeric outcomes resulting from random process.png)

![](C:/Users/qp/Pictures/generate random variable with distribute distribution balls in an urn.png)

![](C:/Users/qp/Pictures/everytime we select a new bead the outcome changes.png)

![](C:/Users/qp/Pictures/statistical inference provides a framework for quantifying the uncertainty introduced by randomness.png)










# Sampling Models


Many data-generation procedures, those that produce data that we study, can be modeled quite well as **draws from an urn**.  For example, we can model the process of polling likely voters as drawing 0's--Republicans-- and 1's--Democrats-- from an urn containing the 0 and 1 code for all likely voters.  We'll see that in more detail later.  In epidemiological studies, [][*we often assume that the subjects in our study are a random sample from the population of interest*].  The data related to a specific outcome can be modeled as a random sample from an urn containing the values for those outcomes for the entire population of interest.  

Similarly, in experimental research, we often assume that the individual organisms we are studying-- for example, worms, flies, or mice--are a random sample from a larger population.  [][Randomized experiments can also be modeled by draws from urn, given the way individuals are assigned to groups].  When getting assigned, you draw your group at random.  Sampling models are therefore ubiquitous in data science.  Casino games offer a *plethora* (a large amount of) of examples of real-world situations in which sampling models are used to answer specific questions.  We will therefore start with such examples.  OK, let's start with this.  

Suppose a very small casino hires you to consult on whether they should set up a roulette wheel.  They want to know if they can make money off it, or if it's too risky and they might lose.  To keep the example simple, we will assume that 1,000 people will play, and that the only game you can play is to bet on red or black.  The casino wants to predict how much money they will make or lose.  They want a range of values that are possible, and in particular, [][*they want to know, what is the chance of losing money?*]  If this probability is too high, they will pass on installing roulette wheels, since they can't take the risk, given that they need to pay their employees and keep the lights on.  

***We're going to define a random variable, capital S, that will represent the casino's total winnings***.  Let's start by constructing the urn, the urn we use for our sampling model.  A roulette wheel has 18 red pockets, 18 black pockets, and 2 green ones.  So playing a color in one game of roulette is equivalent to drawing from this urn (*so in what situations it is not???* continue data and no category maybe).  Let's write some code.  There's 18 black, 18 red, and 2 green.  The 1,000 outcomes from 1,000 people playing are independent draws from this urn.  If red comes up, the gambler wins, and the casino loses $1, so we draw a negative 1.  Otherwise, the casino wins $1, and we draw a 1.  We can code 1,000 independent draws using the following code.  Here are the first 10 outcomes of these 1,000 draws.  [][***Because we know the proportions of 1's and negative 1's, inside the urn***], we can generate the draws with one line of code, without defining color.  Here's that line of code.  [][We call this approach a sampling model, since we are modeling the random behavior of a roulette with the sampling of draws from an urn].  

[][*The total winnings, capital S, is simply the sum of these 1,000 independent draws*].  So here's a code that generates an example of S.  If you run that code over and over again, you see that S changes every time.  This is, of course, [][because S is a random variable].  A very important and useful concept is the probability distribution of the random variable.  ***The probability distribution of a random variable tells us the probability of the observed value falling in any given interval***.  So for example, *if we want to know the probability that we lose money, we're asking, what is the probability that S is in the interval S less than 0*?  Note that if we can define a cumulative distribution function--let's call it f of a, which is equal to the probability of S being less than or equal to a--then we'll be able to answer any question related to the probability of events defined by a random variable S, including the event S less than 0.  *We call f the random variable's distribution function*.  We can estimate the distribution function for the random variable S by using a Monte Carlo simulation to generate many, many realizations of the random variable.  

With the code we're about to write, [][*we run the experiment of having 1,000 people play roulette over and over*].  Specifically, we're going to do this experiment 10,000 times.  Here's the code.  So now we're going to ask, in our *simulation*, how often did we get sums smaller or equal to a?  We can get the answer by using this simple code.  This will be a very good approximation of f of a, our distribution function.  In fact, we can visualize the distribution by creating a histogram showing the probability f(b) minus f(a) for several intervals ab.  Here it is.  Now we can easily answer the casino's question, how likely is it that we lose money?  We simply ask, how often was S, out of the 10,000 simulations, smaller than 0?  And the answer is, it was only 4.6% of the time.  So it's quite low.  From the [][*histogram*] (*What is this hist??? Think how that 1000 gamblers involved*), we also see that that distribution appears to be approximately normal.  If you make a Q-Q plot, you'll confirm that the normal approximation is close to perfect.  If, in fact, the distribution is normal, then all we need to define is the distribution's average and standard deviation.  Because we have the original values from which the distribution is created, we can easily compute these.  The average is 52.5, and the standard deviation is 31.75.  

*If we add a normal density with this average and standard deviation to the histogram we created earlier, we see that it matches very well.  This average and this standard deviation have special names.  They are referred to as the expected value and the standard error of the random variable S*.  We will say more about this in the next section.  It actually turns out that statistical theory provides a way to derive the distribution of a random variable defined as independent draws from an urn.  Specifically, in our example, we can show that S plus n divided by 2 follows what is known as a [][*binomial distribution*].  We therefore do not need to run Monte Carlo simulations, nor use the normal approximation, to know the probability distribution of S.  We did this for illustrative purposes.  We ran the simulation for illustrative purposes.  For the details of the binomial distribution, you can consult any probability book, or even Wikipedia.  However, we will discuss an incredibly useful approximation provided by mathematical theory that applies generally to sums of averages of draws from any urn, the central limit theorem.  


[][Textbook link and additional information]

This video corresponds to the textbook section on sampling models.
https://rafalab.github.io/dsbook/random-variables.html#sampling-models

You can read more about the binomial distribution here.
https://en.wikipedia.org/wiki/Binomial_distribution


[][Key points]

[][    A sampling model models the random behavior of a process as the sampling of draws from an urn.]
    The probability distribution of a random variable is the probability of the observed value falling in any given interval.
[][    We can define a CDF F(a) = Pr(S<= a) to answer questions related to the probability of S being in any interval.]
    *The average of many draws of a random variable is called its expected value.*
    *The standard deviation of many draws of a random variable is called its standard error.*

Monte Carlo simulation: Chance of casino losing money on roulette

We build a sampling model for the random variable S that represents the casino's total winnings. 

```{r}
# sampling model 1: define urn, then sample
color <- rep(c("Black", "Red", "Green"), c(18, 18, 2)) # define the urn for the sampling model

n <- 1000
X <- sample(ifelse(color == "Red", -1, 1), n, replace = TRUE)
X[1:10]
```

```{r}
# sampling model 2: define urn inside sample function by noting probabilities
x <- sample(c(-1, 1), n, replace = TRUE, prob = c(9/19, 10/19))    # 1000 independent draws
S <- sum(x)    # total winnings = sum of draws
S
```

We use the sampling model to run a Monte Carlo simulation and use the results to estimate the probability of the casino losing money.

```{r}
n <- 1000    # number of roulette players
B <- 10000    # number of Monte Carlo experiments
S <- replicate(B, {
    X <- sample(c(-1,1), n, replace = TRUE, prob = c(9/19, 10/19))    # simulate 1000 spins
    sum(X)    # determine total profit
})

mean(S < 0)    # probability of the casino losing money
```

We can plot a histogram of the observed values of S as well as the normal density curve based on the mean and standard deviation of S.

```{r}
library(tidyverse)

s <- seq(min(S), max(S), length = 100)    # sequence of 100 values across range of S

normal_density <- data.frame(s = s, f = dnorm(s, mean(S), sd(S))) # generate normal density for S

data.frame (S = S) %>%    # make data frame of S for histogram
    ggplot(aes(S, ..density..)) +
    geom_histogram(color = "black", binwidth = 10) +
    ylab("Probability") +
    geom_line(data = normal_density, mapping = aes(s, f), color = "blue")
```



```{r}
colors <- rep(c("Black", "Red", "Green"), c(18, 18, 2))

colors



color_cotcomes <- sample(colors, 1000, replace=T)

mean(color_cotcomes=="Red")


casino_2 <- sample(ifelse(colors=="Red", -1, 1), 1000, replace=T)

sum(casino_2)
```

```{r}
casino_x <- sample(c(-1, 1), 1000, replace=TRUE, prob=c(9/19, 10/19))


sum(casino_x)
```

```{r}
n <- 1000
B <- 100000

S <- replicate(B, {
  X <- sample(ifelse(colors=="Red", -1, 1), n, replace=T)
  sum(X)
})


mean(S>100)
summary(S)
```

```{r}
hist(S, 30)
```

```{r}
qqnorm(S, pch = 1, frame = FALSE)

qqline(S, col = "steelblue", lwd = 2)
```

```{r}
s <- seq(min(S), max(S), length=100)

normal_den <- data.frame(s=s, f=dnorm(s, mean(S), sd(S)))

data.frame(S=S) %>%
  ggplot(aes(S, ..density..)) +
  geom_histogram(color="black", bins=30) +
  ylab("Probability") +
  geom_line(data=normal_den, mapping=aes(s, f), color='blue')
```



![](C:/Users/qp/Pictures/many data generation procedures can be modeled as draws from an urn.png)

![](C:/Users/qp/Pictures/we often assume that the subjects in our study are random sample from the population on interest.png)

![](C:/Users/qp/Pictures/sampling models are ubiquitous in data science.png)

![](C:/Users/qp/Pictures/our assumption is 1000 peopel will play rouletter wheel.png)

![](C:/Users/qp/Pictures/also assume the only game you can play with roulette is bet on red or black.png)

![](C:/Users/qp/Pictures/we use capital s to represent the the casino's total winnings.png)

![](C:/Users/qp/Pictures/lets start by constructing the urn, a roulette has 18 red, 18 black, 2 green pocket.png)

![](C:/Users/qp/Pictures/now put all colors into our urn.png)

![](C:/Users/qp/Pictures/we construct the urn with this code.png)

![](C:/Users/qp/Pictures/we can then code 1000 independent draws and using 1 as casino earn 1 dallor ad -1 as losing 1 dallor.png)

![](C:/Users/qp/Pictures/we call this approach a sampling model.png)

![](C:/Users/qp/Pictures/if we run that code over and over again, we see that outcome changes everytime.png)

![](C:/Users/qp/Pictures/because the outcome is a random variable.png)

![a very important and useful concept is the probability distribution of the random variable.png](C:/Users/qp/Pictures/a very important and useful concept is the probability distribution of the random variable.png)

![lets define a comulative distribution function first and then we'll be able to answer any question related to the probability of events defined by random variable.png](C:/Users/qp/Pictures/lets define a comulative distribution function first and then we'll be able to answer any question related to the probability of events defined by random variable.png)

![](C:/Users/qp/Pictures/we call the F as random variable distribution function.png)

![](C:/Users/qp/Pictures/lets use a monte carlo simulation to generate many realizations of the random variable.png)

![](C:/Users/qp/Pictures/we can now answer casino owner's question how likely is it that we lose money.png)

![](C:/Users/qp/Pictures/from the histogram we also see that the distribution appears to be approximatelly normal.png)

![](C:/Users/qp/Pictures/we can check distribution's average and standard deviation.png)

![](C:/Users/qp/Pictures/and using this code to comparing with normal distributed corve with same mean and sd.png)

![](C:/Users/qp/Pictures/we see it matches very well here with above code.png)

![](C:/Users/qp/Pictures/this average and standard deviation has special name, ther are refered to as the expected value.png)

![](C:/Users/qp/Pictures/and the standard error of variable S.png)

![](C:/Users/qp/Pictures/specifically in our example we can show that S plus n follows binomial distribution.png)

![](C:/Users/qp/Pictures/we'll then introduce an incredibly useful approximation.png)









# Distributions versus Probability Distributions


Before we continue, let's make a subtle, yet [][important distinction and connection between the distribution of a list of numbers, which we covered in our data visualization module and a probability distribution], which we're talking about here.  *Previously we described how any list of numbers, let's call it x1 through xn, has a distribution*.  The definition is quite straightforward.  We define capital F of a as a function that answers the question, what proportion of the list is less than or equal to a.  Because they are useful summaries, when the distribution is approximately normal, we define the average and the standard deviation.  These are defined with a straightforward operation of the list.  In r we simply compute the average and standard deviation this way, for example.  

# ===========================================================================================================
**A random variable x has a distribution function.  To define this, we do not need a list of numbers.  It's a theoretical concept**.  In this case, [][**to define the distribution, we define capital F of a as a function that answers the question, what is the probability that x is less than or equal to a.  There is no list of numbers**].  However, if x is defined by drawing from an urn with numbers in it, then there is a list, the list of numbers inside the urn.  In this case, the distribution of that list is the probability distribution of x and the average and standard deviation of that list are the expected value and standard errors of the random variable.  
# ===========================================================================================================

*Another way to think about it that does not involve an urn is to run a Monte Carlo simulation and generate a very large list of outcomes of x.  These outcomes are a list of numbers.  The distribution of this list will be a very good approximation of the probability distribution of x.  The longer the list, the better the approximation.  The average and standard deviation of this list will approximate the expected value and standard error of the random variable.  *


[][Textbook link]

This video corresponds to the textbook section on distributions versus probability distributions.
https://rafalab.github.io/dsbook/random-variables.html#distributions-versus-probability-distributions
# ================================================================================================================
# You want read this, and understand this


[][Key points]

[][*    A random variable X has a probability distribution function F(a) that defines Pr(X<=a) over all values of a.*]
[][*    Any list of numbers has a distribution. The probability distribution function of a random variable is defined mathematically and does not depend on a list of numbers.*]
    The results of a Monte Carlo simulation with a large enough number of observations will approximate the probability distribution of X.
    ***If a random variable is defined as draws from an urn:***

[][        The probability distribution function of the random variable is defined as the distribution of the list of values in the urn.]
[][*        The expected value of the random variable is the average of values in the urn.]
        The standard error of one draw of the random variable is the standard deviation of the values of the urn.




![](C:/Users/qp/Pictures/distribution of a list of numbers.png)

![](C:/Users/qp/Pictures/probability distribution.png)

![](C:/Users/qp/Pictures/previously we described how any list of numbers has a distribution.png)

![what proportion of the list is less than or equal to a](C:/Users/qp/Pictures/we define capital F of a as a function that answers the question what proportion of the list is less that or equal to a.png)

![](C:/Users/qp/Pictures/we can calculate the average and standard deviation in r this way.png)

![](C:/Users/qp/Pictures/a random variable has a distribution function.png)

![](C:/Users/qp/Pictures/what is the probability that X is less than or equal to a.png)









# Notation for Random Variables


Note that in statistical textbooks, capital letters are used to denote random variables and we follow this convention here.  Lower case letters are used for observed values.  You'll see some notation that includes both.  For example, you'll see events defined as capital X less than or equal than small cap x.  Here, x is a random variable, making it a random event.  And little x is an arbitrary value and not random.  [][**So for example, capital X might represent the number on a die roll--that's a random value--and little x will represent an actual value we see.  So in this case, the probability of capital X being equal to little x is 1 in 6 regardless of the value of little x**].  Note that this notation is a bit strange, because we ask questions about probability, since big x is not an observed quantity.  Instead, it's a random quantity that we will see in the future.  We can talk about what we expect it to be, what values are probable, but not what it is.  But once we have the data, we do see a realization of big x, so data scientists talk about what could have been, but after we see what actually happened.  


[][Textbook link]

This video corresponds to the textbook section on notation for random variables.
https://rafalab.github.io/dsbook/random-variables.html#notation-for-random-variables


[][Key points]

[][    Capital letters denote random variables (X) and lowercase letters denote observed values (x).]
[][    In the notation Pr(X=x), we are asking how frequently the random variable X is equal to the value x. For example, if x=6, this statement becomes Pr(X=6).]



![](C:/Users/qp/Pictures/usually you'll see we use notation with capital letter as the random variables and low case letters as the observed values.png)

![](C:/Users/qp/Pictures/real case using capital letter and low case letter to represent.png)

![in die row game, the probability of capital X being equal to letter x is 1 in 6 regardless of the value of little x.png](C:/Users/qp/Pictures/in die row game, the probability of capital X being equal to letter x is 1 in 6 regardless of the value of little x.png)









# Central Limit Theorem


# ==================================================================================================================
[][**The Central Limit Theorem--or the CLT for short tells us that when the number of independent draws--also called sample size--is large, the probability distribution of the sum of these draws is approximately normal**](So we need a list of numbers here? or mark those pockets with numbers in roulette wheel?).  Because sampling models are used for so many data generation processes, the CLT is considered one of the most important mathematical insights in history.  Previously, we discussed that if we know that the *distribution of a list of numbers is approximated by the normal distribution, all we need to describe the list are the average and the standard deviation*.  [][We also know that the same applies to probability distributions].  If a random variable has a probability distribution that is approximated with the normal distribution, then all we need to describe that probability distribution are the average and the standard deviation.  Referred to as the expected value and the standard error.  

We have described sampling models for draws.  We will now go over the mathematical theory that lets us approximate the probability distribution [][*for the sum of draws*](what is it???).  Once we do this, we will be able to help the Casino predict how much money they will make.  The same approach we use for sum of the draws will be useful for describing the distribution of averages and proportions, which we will need to understand, for example, how polls work.  The first important concept to learn is the [][*expected value*].  *In statistics books, it is common to use the letter capital E, like this, E of X equals mu, to denote that the expected value of the random variable X is mu*.  Mu is a Greek letter for M, which is the first letter in the word mean, which is a synonym with average.  A random variable will vary around an expected value in a way that if you take the average of many, many draws, the average of the draws will approximate the expected value.  Getting closer and closer the more draws you take.  A useful formula is that [][*the expected value of the random variables defined by one draw is the average of the numbers in the urn*].  

[][***For example, in our urn used to model betting on red on roulette, we have twenty 1's and 18 negative 1's.  So the expected value is E of X equals 20 plus negative 18 divided by 38***].  Which is about $0.05.  It is a bit counterintuitive to say that X varies around 0.05 when the only values it takes is 1 and minus 1.  An intuitive way to think about the expected value is that if we play the game over and over, the Casino wins, on average, $0.05 per game.  Our Monte Carlo Simulation confirms this.  Here we run a million games and we see that the mean of X, which is a bunch of 0s and 1s, is about $0.05.  [][*In general, if the urn has just two possible outcomes--say, a and b, with proportions p and 1 minus p respectively, the average is a times p plus b times 1 minus p*].  To see this, notice that if there are nb's in the urn, then we have npa's and n times 1 minus is pb's.  And because the average is the sum, we have n times a times p plus n times b times 1 minus p divided by the total, which is n.  And we get the formula that we just saw.  Now, the reason we define the expected value is because this mathematical definition turns out to be useful for approximating the probability distribution of sums, which in turn, is useful to describe the distribution of averages and proportions.  

[][*The first useful fact is that the expected value of the sum of draws is the number of draws times the average of the numbers in the urn*].  So if 1,000 people play roulette, the Casino expects to win, on average, 1,000 times $0.05, which is $50.  But this is an expected value.  How different can one observation be from the expected value ?  The Casino really wants to know this.  What is the range of possibilities?  If negative numbers are too likely, we may not install the roulette wheels.  Statistical theory, once again, answers this question.  The standard error, or SE for short, gives us an idea of the size of the variation around the expected value.  In statistics books, it is common to use SE of X to know the standard error of the random variable X If our draws are independent--That's an important assumption-- *then the standard error of the sum is given by the equation, the square root of the number of draws, times the standard deviation of the numbers in the urn*.  Using the definition of standard deviation, we can derive with a bit of math, that if an urn contains two values--a and b, with proportions p and 1 minus p, respectively--the standard deviation is the absolute value of b minus a times the square root of p times 1 minus p.  

So in our roulette example, the standard deviation of the values inside the urn is 1 minus minus 1 times the square root of 10/19 times 9/19.  Or 0.9986, so practically 1.  The standard error tells us the typical difference between a random variable and its expectation.  So because 1 draw is obviously the sum of 1 draw, we can use a formula to calculate that the random variable defined by 1 draw has an expected value of $0.05 and a standard error of about 1.  This makes sense since we either get a 1 or a minus 1 with 1 slightly favored over the minus 1.  Using the formula, the sum of 1,000 people playing has standard error of about $32.  So when 1,000 people bet on red, the Casino is expected to win $50 with a standard error of $32.  So it seems like a safe bet.  [][*But we still really can't answer the question--How likely is the Casino to lose money*]?  Here The Central Limit Theorem will help.  *The central limit theorem tells us that the distribution of the sum of S is approximated by a normal distribution*.  Using the formula, we know that the expected value and standard errors are $52 and $32, respectively.  Note that the theoretical values match those obtained with the Monte Carlo simulation we ran earlier.  Using the Central Limit Theorem, we can skip the Monte Carlo simulation and instead, compute the probability of the Casino losing money using the approximation.  We write the simple code using the pnorm function and we get the answer.  It's about 5%.  Which, is in very good agreement-- with the Monte Carlo simulation we ran.  


[][Textbook links]

This video corresponds to the textbook sections on expected value and standard error and the Central Limit Theorem.
https://rafalab.github.io/dsbook/random-variables.html#the-expected-value-and-standard-error
https://rafalab.github.io/dsbook/random-variables.html#central-limit-theorem


[][Key points]

[][*    The Central Limit Theorem (CLT) says that the distribution of the sum of a random variable is approximated by a normal distribution.*]
[][*    The expected value of a random variable, E(X) = mu, is the average of the values in the urn. This represents the expectation of one draw. *]
[][*    The standard error of one draw of a random variable is the standard deviation of the values in the urn.*]
[][*    The expected value of the sum of draws is the number of draws times the expected value of the random variable. *]
[][*    The standard error of the sum of independent draws of a random variable is the square root of the number of draws times the standard deviation of the urn. *]

Equations

These equations apply to the case where there are only two outcomes, a and b with proportions p and 1-p respectively. The general principles above also apply to random variables with more than two outcomes.

Expected value of a random variable: 
    ap + b(1-p)

Expected value of the sum of n draws of a random variable: 
    n * (ap + b(1-p))

[][*Standard deviation of an urn with two values: *]
    |b-a|sqrt(p(1-p))

Standard error of the sum of n draws of a random variable:
    sqrt(n)*|b-a|*sqrt(p(1-p))



```{r}
B <- 10^6

X <- sample(c(1, -1), B, replace = TRUE, prob = c(10/19, 9/19))

  
mean(X)
```

```{r}
mean(S<0)


n <- 1000

mu <- n*(20-18)/38
se <- sqrt(n)*abs(1-(-1))*sqrt(90)/19    # =================================================================================

pnorm(0, mu, se)
```




![](C:/Users/qp/Pictures/when the number of independent draws or the sample size is large, the sum of these independent draws is approximately normal.png)

![](C:/Users/qp/Pictures/if a random variable has a probability distribution that is approximated with the normal distribution thenall we need to know to describe the probability distribution is.png)

![](C:/Users/qp/Pictures/the first important concept to learn is expected value.png)

# ===============================================================================================================
![Whay is this???](C:/Users/qp/Pictures/in casino roulette games, the expected value of the random variables defined by one draw is the average number in the urn.png)

![](C:/Users/qp/Pictures/here we run a monte carlo simulation to check the espected value is 0.05.png)

![the expected value is this](C:/Users/qp/Pictures/if the uren has just 2 possible outcomes a and b, with proportion p and 1-p, the average is this.png)

![](C:/Users/qp/Pictures/the first useful fact is that the expected value of the sum of draws is the number of of draw times the average of the number in the urn.png)

![](C:/Users/qp/Pictures/the number of draws times the average of the number in the urn.png)

![](C:/Users/qp/Pictures/so if 1000 people roulette, the casino expected to win on average 50 usd.png)

![the standard error gives us an idea of the size of the variation around the expected value.png](C:/Users/qp/Pictures/the standard error gives us an idea of the size of the variation around the expected value.png)

![then we should define the random variable X](C:/Users/qp/Pictures/in statistics books its common to use SE of X to denote the standard error of the random variable X.png)

![the standard error of the sum is given by this equation, the square root of the number of draws times the standard deviation of the numbers in the urn.png](C:/Users/qp/Pictures/the standard error of the sum is given by this equation, the square root of the number of draws times the standard deviation of the numbers in the urn.png)

![](C:/Users/qp/Pictures/the standard deviation is the absolute value of b minus a times the square root of p times 1 minus p.png)

![the standard errors tell us the typical difference between a random variable and its expectation](C:/Users/qp/Pictures/the standard deviation of the value inside the urn is this.png)

![](C:/Users/qp/Pictures/so when 1000 people playing, the casino is expected to win 50 usd with standard error of 32.png)

![](C:/Users/qp/Pictures/using the central limit theorem, we can skip the monte carlo simulation.png)

![we can use the use central limit theorem to compute the casino losing money using the approximation.png](C:/Users/qp/Pictures/we can use the use central limit theorem to compute the casino losing money using the approximation.png)










# DataCamp Assessment: Random Variables and Sampling Models


Assessment due Jul 7, 2022 20:19 AWST

This assessment covers the random variables and sampling models.

By clicking OK, you agree to DataCamp's privacy policy: https://www.datacamp.com/privacy-policy. Note that you might need to disable your pop-up blocker, or allow "www.datacamp.com" in your pop-up blocker allowed list. When you have completed the exercises, return to edX to continue your learning.
Assessment: Random Variables and Sampling Models (External resource) (7.5 points possible)
By clicking OK, you agree to DataCamp's privacy policy: https://www.datacamp.com/privacy-policy.


## Exercise 1. American Roulette probabilities

An American roulette wheel has 18 red, 18 black, and 2 green pockets. Each red and black pocket is associated with a number from 1 to 36. The two remaining green slots feature "0" and "00". Players place bets on which pocket they think a ball will land in after the wheel is spun. Players can bet on a specific number (0, 00, 1-36) or color (red, black, or green).

What are the chances that the ball lands in a green pocket?
Instructions
100 XP

    Define a variable p_green as the probability of the ball landing in a green pocket.
    Print the value of p_green.

```{r}
# The variables `green`, `black`, and `red` contain the number of pockets for each color
green <- 2
black <- 18
red <- 18

# Assign a variable `p_green` as the probability of the ball landing in a green pocket
p_green <- green/(green + black + red)

# Print the variable `p_green` to the console
p_green
```


## Exercise 2. American Roulette payout

In American roulette, the payout for winning on green is $17. This means that if you bet $1 and it lands on green, you get $17 as a prize.

Create a model to predict your winnings from betting on green one time.
Instructions
100 XP

    Use the sample function return a random value from a specified range of values.
    Use the prob = argument in the sample function to specify a vector of probabilities for returning each of the values contained in the vector of values being sampled.
    Take a single sample (n = 1).

```{r}
# Use the `set.seed` function to make sure your answer matches the expected result after random sampling.
set.seed(1, sample.kind="Rounding")


# The variables 'green', 'black', and 'red' contain the number of pockets for each color
green <- 2
black <- 18
red <- 18

# Assign a variable `p_green` as the probability of the ball landing in a green pocket
p_green <- green / (green+black+red)

# Assign a variable `p_not_green` as the probability of the ball not landing in a green pocket
p_not_green <- 1 - p_green

# Create a model to predict the random variable `X`, your winnings from betting on green. Sample one time.
X <- sample(c(green, black, red), 1)

# Print the value of `X` to the console
X

```

```{r}
# Use the `set.seed` function to make sure your answer matches the expected result after random sampling.
set.seed(1, sample.kind="Rounding")

# The variables 'green', 'black', and 'red' contain the number of pockets for each color
green <- 2
black <- 18
red <- 18

# Assign a variable `p_green` as the probability of the ball landing in a green pocket
p_green <- green / (green+black+red)

# Assign a variable `p_not_green` as the probability of the ball not landing in a green pocket
p_not_green <- 1 - p_green

# Create a model to predict the random variable `X`, your winnings from betting on green. Sample one time.
X <- sample(c(1, -1), prob=c(p_green, p_not_green), 1)

# Print the value of `X` to the console
X

```


## Exercise 3. American Roulette expected value

In American roulette, the payout for winning on green is $17. This means that if you bet $1 and it lands on green, you get $17 as a prize.In the previous exercise, you created a model to predict your winnings from betting on green.

Now, compute the expected value of X, the random variable you generated previously.
Instructions
100 XP

    Using the chances of winning $17 (p_green) and the chances of losing $1 (p_not_green), calculate the expected outcome of a bet that the ball will land in a green pocket.

```{r}
# The variables 'green', 'black', and 'red' contain the number of pockets for each color
green <- 2
black <- 18
red <- 18

# Assign a variable `p_green` as the probability of the ball landing in a green pocket
p_green <- green / (green+black+red)

# Assign a variable `p_not_green` as the probability of the ball not landing in a green pocket
p_not_green <- 1-p_green

# Calculate the expected outcome if you win $17 if the ball lands on green and you lose $1 if the ball doesn't land on green
17*p_green+(-1)*p_not_green
```


## Exercise 4. American Roulette standard error

The standard error of a random variable X tells us the difference between a random variable and its expected value. You calculated a random variable X in exercise 2 and the expected value of that random variable in exercise 3.

Now, compute the standard error of that random variable, which represents a single outcome after one spin of the roulette wheel.
Instructions
100 XP

    Compute the standard error of the random variable you generated in exercise 2, or the outcome of any one spin of the roulette wheel.
    Recall that the payout for winning on green is $17 for a $1 bet.

```{r}
# The variables 'green', 'black', and 'red' contain the number of pockets for each color
green <- 2
black <- 18
red <- 18

# Assign a variable `p_green` as the probability of the ball landing in a green pocket
p_green <- green / (green+black+red)

# Assign a variable `p_not_green` as the probability of the ball not landing in a green pocket
p_not_green <- 1-p_green

# Compute the standard error of the random variable
abs(17+1)*sqrt(p_green*p_not_green)
```


## Exercise 5. American Roulette sum of winnings

You modeled the outcome of a single spin of the roulette wheel,X, in exercise 2.

Now create a random variable S that sums your winnings after betting on green 1,000 times.
Instructions
100 XP

    Use set.seed to make sure the result of your random operation matches the expected answer for this problem.
    Specify the number of times you want to sample from the possible outcomes.
    Use the sample function to return a random value from a vector of possible values.
    Be sure to assign a probability to each outcome and to indicate that you are sampling with replacement.
[][*    Do not use replicate as this changes the output of random sampling and your answer will not match the grader.*]

```{r}
# The variables 'green', 'black', and 'red' contain the number of pockets for each color
green <- 2
black <- 18
red <- 18

# Assign a variable `p_green` as the probability of the ball landing in a green pocket
p_green <- green / (green+black+red)

# Assign a variable `p_not_green` as the probability of the ball not landing in a green pocket
p_not_green <- 1-p_green

# Use the `set.seed` function to make sure your answer matches the expected result after random sampling
set.seed(1, sample.kind="Rounding")

# Define the number of bets using the variable 'n'
n <- 1000

# Create a vector called 'X' that contains the outcomes of 1000 samples
X <- sample(c(17, -1), prob=c(p_green, p_not_green), n, replace=T)

# Assign the sum of all 1000 outcomes to the variable 'S'
S <- sum(X)

# Print the value of 'S' to the console
S
```


## Exercise 6. American Roulette winnings expected value

In the previous exercise, you generated a vector of random outcomes,S, after betting on green 1,000 times.

What is the expected value of S?
Instructions
100 XP

    Using the chances of winning $17 (p_green) and the chances of losing $1 (p_not_green), calculate the expected outcome of a bet that the ball will land in a green pocket over 1,000 bets.

```{r}
# The variables 'green', 'black', and 'red' contain the number of pockets for each color
green <- 2
black <- 18
red <- 18

# Assign a variable `p_green` as the probability of the ball landing in a green pocket
p_green <- green / (green+black+red)

# Assign a variable `p_not_green` as the probability of the ball not landing in a green pocket
p_not_green <- 1-p_green

# Define the number of bets using the variable 'n'
n <- 1000

# Calculate the expected outcome of 1,000 spins if you win $17 when the ball lands on green and you lose $1 when the ball doesn't land on green
sqrt(n)*abs(17+1)*sqrt(p_green*p_not_green)

# ====================================================================================================================
# Read before you code, it asking you to calculate the expected value of 100 spins
1000*(17*p_green+(-1)*p_not_green)
```


## Exercise 7. American Roulette winnings expected value

You generated the expected value of

, the outcomes of 1,000 bets that the ball lands in the green pocket, in the previous exercise.

What is the standard error of

?
Instructions
100 XP

    Compute the standard error of the random variable you generated in exercise 5, or the outcomes of 1,000 spins of the roulette wheel.

```{r}
# The variables 'green', 'black', and 'red' contain the number of pockets for each color
green <- 2
black <- 18
red <- 18

# Assign a variable `p_green` as the probability of the ball landing in a green pocket
p_green <- green / (green+black+red)

# Assign a variable `p_not_green` as the probability of the ball not landing in a green pocket
p_not_green <- 1-p_green

# Define the number of bets using the variable 'n'
n <- 1000

# Compute the standard error of the sum of 1,000 outcomes
sqrt(n)*abs(17+1)*sqrt(p_green*p_not_green)
```


## End of Assessment: Random Variables and Sampling Models

This is the end of the programming assignment for this section. Please DO NOT click through to additional assessments from this page. Please DO answer the question on this page. If you do click through, your scores may NOT be recorded.

Click on "Awesome" to get the "points" for this question and then return to the course on edX.

You can close this window and return to Data Science: Probability.
Answer the question
50XP
Possible Answers

    Awesome
    press
    1
    Nope
    press
    2









## Course  /  Section 3: Random Variables, Sampling Models, and the Central Limit Theorem  /  3.2 The Central Limit Theorem Continued


# Averages and Proportions


There are some useful mathematical results, some of which we used previously, that are often used when working with data.  We're going to list them here.  The first, is that the expected value of a sum of random variables is the sum of the expected values of the individual random variables.  We can write it like this, using mathematical notation.  If the x are drawn from the same urn, then they all have the same expected value.  We call it mu here.  And therefore, the expected value of the sum is n times mu, which is another way of writing the result of the sum of draws.  

A second property is that the expected value of random variables times a non-random constant is the expected value times that non-random constant.  This is easier to explain with mathematical symbols, which we show here.  To see why this is intuitive, think of a change of units.  If we change the units of the random variable, say from dollars to cents, the expectation should change in the same way.  A consequence of these two facts that we described is that the expected value of the average of draws from the same urn is the expected value of the urn, call it mu again.  Here is that result written out in mathematical formula.  So the expected value of the average is mu, the average of the values in the urn.  

A third property is that the square of the standard error of the sum of independent random variables is the sum of the square of the standard error of each random variable.  This one is easier to understand in its math form.  The standard error of the sum of x is the square root of the sum of the standard error squared, as you can see here.  Note that the square of the standard error is referred to as a variance in statistical textbooks.  

A fourth property is that the standard error of random variables times a non-random constant is the standard error times a non-random constant.  As with the expectation, we have the following formula.  To see why this is intuitive, again, think of change of units.  A consequence of these previous two properties is that the standard error of the average of independent draws from the same urn is the standard deviation of the urn-- let's call it sigma, this is the Greek letter for s.  --divided by the square root of n.  Here it is in the mathematical form.  And here we are using the properties we just described to derive that result.  

The last property that we're going to talk about is that if x is a normally distributed random variable, then if a and b are non-random constants, then a times X plus b is also a normally distributed random variable.  Note that we are doing is changing the units of the random variable by multiplying by a and then shifting the center by adding b.  


[][Error in video]

Note that at 1:54 in the video that the last term should be SE[X_n]^2 , not E[X_n]^2.



[][Textbook link]

This video corresponds to the textbook section on the statistical properties of averages.
https://rafalab.github.io/dsbook/random-variables.html#statistical-properties-of-averages



Key points

    Random variable times a constant

      The expected value of a random variable multiplied by a constant is that constant times its original expected value:
          E[aX] = a*mu
      The standard error of a random variable multiplied by a constant is that constant times its original standard error:
          SE[aX] = a*Sigma
    Average of multiple draws of a random variable

      The expected value of the average of multiple draws from an urn is the expected value of the urn (mu).
      The standard deviation of the average of multiple draws from an urn is the standard deviation of the urn divided by the square root of the number of draws ( Sigma*sqrt(n) ).

    The sum of multiple draws of a random variable

          The expected value of the sum of
draws of a random variable is

times its original expected value:

The standard error of the sum of
draws of random variable is 

times its original standard error:

    The sum of multiple different random variables

The expected value of the sum of different random variables is the sum of the individual expected values for each random variable:

The standard error of the sum of different random variables is the square root of the sum of squares of the individual standard errors:

    Transformation of random variables

If
is a normally distributed random variable and and are non-random constants, then is also a normally distributed random variable.



![](C:/Users/qp/Pictures/the expected value of the sum of random variables is the sum of the expected value of the individual random variables.png)

![the expected value of a sum of random variables is the sum of the expected values of the individual random variables](C:/Users/qp/Pictures/the expected value of a sum of random variables is the sum of the expected values of the individual random variables.png)

![if the x are drawn from same urn, then they all have same expected value, mu, therefore the expected value of the sum is nmu](C:/Users/qp/Pictures/if the x are drawn from same urn, then they all have same expected value, mu, therefore the expected value of the sum is nmu.png)

![think this as unit change from cent to dollar](C:/Users/qp/Pictures/the expected value of a random variable times a non-random constant is the expected value times that non-random constant.png)

![](C:/Users/qp/Pictures/expected value of a random variable times a non-random constant is the expected value times that non-random constant.png)

![](C:/Users/qp/Pictures/the consequence of those two facts that we described is that the expected value of the average of draws from the same urn is the expected value of the urn.png)

![](C:/Users/qp/Pictures/the square of the standard error of the sum of independent random variables is the sum of the square of the standard error of each random variable.png)

![](C:/Users/qp/Pictures/the standard error of the sum of x is the square root of the sum of the standard error squared.png)

![](C:/Users/qp/Pictures/and note that the square of the standard error is referred to as variance.png)

![](C:/Users/qp/Pictures/the standard error of a random variable times a non-random constant is the standard error times the non-random constant.png)

![think of the change of units form cent to dallor](C:/Users/qp/Pictures/the standard error of random variable times a non-random constant is teh standard error times a non-random constant.png)

![](C:/Users/qp/Pictures/a consequence of previous 2 properties is that the standard error of the average of independent draws form the same urn is the standard deviation of the urn devided by the square root of n.png)

# ========================================================================================================
# ========================================================================================================
![](C:/Users/qp/Pictures/the standard error of the average of independent draws form the same urn is the standard deviation of the urn devided by the square root of n.png)

![Think about it, why???](C:/Users/qp/Pictures/if X is a normally distributed random variable, then if a and b are non-random constants, aX+b is also normally distributed random variable.png)











## Law of Large Numbers


An important implication of the properties
we described in the previous video is that the standard error
of the average of draws becomes smaller and smaller as the number of draws n
grows larger.
When n is very large, then the standard error is practically 0,
and the average of the draws converges to the average of the urn.
This is known in statistical textbooks as the law of large numbers,
or the law of averages.
Note that the law of averages is sometimes misinterpreted.
For example, if you toss a coin five times and you see heads each time,
you might hear someone argue that the next toss is probably
a tail because of the law of averages.
On average, we should 50% heads and 50% tails,
so we need more tails to make up.
A similar argument would be to say that red is due on roulette
after seeing black come up five times in a row.
These events are independent.
So the chance of a coin landing heads is 50%, regardless of the previous five.
Similarly for the roulette outcome.
The law of averages applies only when the number of draws
is very, very large, not in small samples.
After a million tosses, you will definitely see about 50% heads,
regardless of what the first five were.
Another somewhat funny misuse of the law of averages
is in sports, where you sometimes hear TV announcers predict
a player is about to succeed because they have failed a few times in a row,
and they need successes to make up and match their average.


[][Textbook link]

This video corresponds to the textbook section on the law of large numbers.
https://rafalab.github.io/dsbook/random-variables.html#law-of-large-numbers



[][Key points]

[][*    The law of large numbers states that as n increases, the standard error of the average of a random variable decreases. In other words, when n is large, the average of the draws converges to the average of the urn.*]
    The law of large numbers is also known as the law of averages.
    The law of averages only applies when n is very large and events are independent. It is often misused to make predictions about an event being "due" because it has happened less frequently than expected in a small sample size.



![](C:/Users/qp/Pictures/An important implication of the properties we described earlier.png)

![](C:/Users/qp/Pictures/the law of large numbers.png)

![](C:/Users/qp/Pictures/remember when the chance are indepanednt of each event.png)











# How Large is Large in CLT?


The central limit theorem works when the number of draws is large.  But large is a relative term.  How big is large?  15, 100, a million?  In many circumstances, as few as 30 draws is enough to make the CLT useful.
In specific instances, as few as 10 is enough.
However, these should not be considered general rules.
Note for example, that when the probability of success is very small,
we need larger sample sizes.
Consider for example, the lottery.
In the lottery, the chance of winning are less than 1 in a million.
Thousands of people play, so the number of draws is very large.
So the central limit should work.
Yet, the number of winners, the sum of the draws,
range between 0, and in very extreme cases, four.
This sum is certainly not well approximated
by the normal distribution.
So the central limit theorem doesn't apply, even
with a very large sample size.
This is generally true when the probability of success is very low.
In these cases, the Poisson distribution is more appropriate.
We do not cover the theory here, but you can
learn about the Poisson distribution in any probability textbook and even
Wikipedia.


[][Textbook links and further information]

This video corresponds to the textbook section on sample size for CLT.
https://rafalab.github.io/dsbook/random-variables.html#how-large-is-large-in-the-central-limit-theorem
You can read more about the Poisson distribution here.
https://en.wikipedia.org/wiki/Poisson_distribution



[][Key points]

    The sample size required for the Central Limit Theorem and Law of Large Numbers to apply differs based on the probability of success.
            If the probability of success is high, then relatively few observations are needed.
            As the probability of success decreases, more observations are needed.
    If the probability of success is extremely low, such as winning a lottery, then the Central Limit Theorem may not apply even with extremely large sample sizes. The normal distribution is not a good approximation in these cases, and other distributions such as the Poisson distribution (not discussed in these courses) may be more appropriate.





![](C:/Users/qp/Pictures/how big is large, 15, 100, million.png)

![](C:/Users/qp/Pictures/in the extreme cases like lottery, the poisson distribution is more appropriate.png)










# DataCamp Assessment: The Central Limit Theorem


Assessment due Jul 7, 2022 20:19 AWST

This assessment covers the central limit theorem.

By clicking OK, you agree to DataCamp's privacy policy: https://www.datacamp.com/privacy-policy. Note that you might need to disable your pop-up blocker, or allow "www.datacamp.com" in your pop-up blocker allowed list. When you have completed the exercises, return to edX to continue your learning.
Assessment: The Central Limit Theorem (External resource) (10.5 points possible)
By clicking OK, you agree to DataCamp's privacy policy: https://www.datacamp.com/privacy-policy.


## Exercise 1. American Roulette probability of winning money

The exercises in the previous chapter explored winnings in American roulette. In this chapter of exercises, we will continue with the roulette example and add in the Central Limit Theorem.

In the previous chapter of exercises, you created a random variable

that is the sum of your winnings after betting on green a number of times in American Roulette.

What is the probability that you end up winning money if you bet on green 100 times?
Instructions
100 XP

    Execute the sample code to determine the expected value avg and standard error se as you have done in previous exercises.
    Use the pnorm function to determine the probability of winning money.

```{r}
# Assign a variable `p_green` as the probability of the ball landing in a green pocket
p_green <- 2 / 38

# Assign a variable `p_not_green` as the probability of the ball not landing in a green pocket
p_not_green <- 1-p_green

# Define the number of bets using the variable 'n'
n <- 100

# Calculate 'avg', the expected outcome of 100 spins if you win $17 when the ball lands on green and you lose $1 when the ball doesn't land on green
avg <- n * (17*p_green + -1*p_not_green)

# Compute 'se', the standard error of the sum of 100 outcomes
se <- sqrt(n) * (17 - -1)*sqrt(p_green*p_not_green)

# Using the expected value 'avg' and standard error 'se', compute the probability that you win money betting on green 100 times.
1-pnorm(0, avg, se)
```


## Exercise 2. American Roulette Monte Carlo simulation

Create a Monte Carlo simulation that generates 10,000 outcomes of

, the sum of 100 bets.

Compute the average and standard deviation of the resulting list and compare them to the expected value (-5.263158) and standard error (40.19344) for

that you calculated previously.
Instructions
100 XP

    Use the replicate function to replicate the sample code for B <- 10000 simulations.
    Within replicate, use the sample function to simulate n <- 100 outcomes of either a win (17) or a loss (-1) for the bet. Use the order c(17, -1) and corresponding probabilities. Then, use the sum function to add up the winnings over all iterations of the model. Make sure to include sum or DataCamp may crash with a "Session Expired" error.
    Use the mean function to compute the average winnings.
    Use the sd function to compute the standard deviation of the winnings.

```{r}
# Assign a variable `p_green` as the probability of the ball landing in a green pocket
p_green <- 2 / 38

# Assign a variable `p_not_green` as the probability of the ball not landing in a green pocket
p_not_green <- 1-p_green

# Define the number of bets using the variable 'n'
n <- 100

# The variable `B` specifies the number of times we want the simulation to run. Let's run the Monte Carlo simulation 10,000 times.
B <- 10000

# Use the `set.seed` function to make sure your answer matches the expected result after random sampling.
set.seed(1, sample.kind="Rounding")

# Create an object called `S` that replicates the sample code for `B` iterations and sums the outcomes.
S <- replicate(B, {
    sum(sample(c(17, -1), prob=c(p_green, p_not_green), n, replace=T))
})



# Compute the average value for 'S'
mean(S)

# Calculate the standard deviation of 'S'
sd(S)

```


## Exercise 3. American Roulette Monte Carlo vs CLT

In this chapter, you calculated the probability of winning money in American roulette using the CLT.

Now, calculate the probability of winning money from the Monte Carlo simulation. The Monte Carlo simulation from the previous exercise has already been pre-run for you, resulting in the variable S that contains a list of 10,000 simulated outcomes.
Instructions
100 XP

    Use the mean function to calculate the probability of winning money from the Monte Carlo simulation, S.

```{r}
# Calculate the proportion of outcomes in the vector `S` that exceed $0
mean(S>0)
```


## Exercise 4. American Roulette Monte Carlo vs CLT comparison

The Monte Carlo result and the CLT approximation for the probability of losing money after 100 bets are close, but not that close. What could account for this?
Instructions
50 XP
Possible Answers

    10,000 simulations is not enough. If we do more, the estimates will match.
    The CLT does not work as well when the probability of success is small.
    The difference is within rounding error.
    The CLT only works for the averages.

```
Incorrect submission
Try again. The difference between the CLT and Monte Carlo estimates is a consequence of the odds associated with winning. 


Incorrect submission
Try again. The CLT works better under certain conditions. 


not work well when probability of success is small
```



## Exercise 5. American Roulette average winnings per bet

Now create a random variable Y that contains your average winnings per bet after betting on green 10,000 times.
Instructions
100 XP

    Run a single Monte Carlo simulation of 10,000 bets using the following steps. (You do not need to replicate the sample code.)
    Specify n as the number of times you want to sample from the possible outcomes.
    Use the sample function to return n values from a vector of possible values: winning $17 or losing $1. Be sure to assign a probability to each outcome and indicate that you are sampling with replacement.
    Calculate the average result per bet placed using the mean function.

```{r}
# Use the `set.seed` function to make sure your answer matches the expected result after random sampling.
set.seed(1, sample.kind="Rounding")

# Define the number of bets using the variable 'n'
n <- 10000

# Assign a variable `p_green` as the probability of the ball landing in a green pocket
p_green <- 2 / 38

# Assign a variable `p_not_green` as the probability of the ball not landing in a green pocket
p_not_green <- 1 - p_green

# Create a vector called `X` that contains the outcomes of `n` bets
X <- sample(c(17, -1), n, prob=c(p_green, p_not_green), replace=T)

# Define a variable `Y` that contains the mean outcome per bet. Print this mean to the console.
Y <- mean(X)
Y
```


## Exercise 6. American Roulette per bet expected value

What is the expected value of

, the average outcome per bet after betting on green 10,000 times?
Instructions
100 XP

    Using the chances of winning $17 (p_green) and the chances of losing $1 (p_not_green), calculate the expected outcome of a bet that the ball will land in a green pocket.
    Use the expected value formula rather than a Monte Carlo simulation.
    Print this value to the console (do not assign it to a variable).

```{r}
# Assign a variable `p_green` as the probability of the ball landing in a green pocket
p_green <- 2 / 38

# Assign a variable `p_not_green` as the probability of the ball not landing in a green pocket
p_not_green <- 1 - p_green

# Calculate the expected outcome of `Y`, the mean outcome per bet in 10,000 bets
10000*(17*p_green+(-1)*p_not_green)
```
Incorrect submission
You are not providing a calculation that gives the correct answer. The expected outcome of a single spin is the sum of the individual outcomes for winning or losing multiplied by their chances of occurring. 

```{r}
# Assign a variable `p_green` as the probability of the ball landing in a green pocket
p_green <- 2 / 38

# Assign a variable `p_not_green` as the probability of the ball not landing in a green pocket
p_not_green <- 1 - p_green

# Calculate the expected outcome of `Y`, the mean outcome per bet in 10,000 bets   ======================================
(17*p_green+(-1)*p_not_green)
```



## Exercise 7. American Roulette per bet standard error

What is the standard error of Y , the average result of 10,000 spins?
Instructions
100 XP

    Compute the standard error of Y , the average result of 10,000 independent spins.

```{r}
# Define the number of bets using the variable 'n'
n <- 10000

# Assign a variable `p_green` as the probability of the ball landing in a green pocket
p_green <- 2 / 38

# Assign a variable `p_not_green` as the probability of the ball not landing in a green pocket
p_not_green <- 1 - p_green

# Compute the standard error of 'Y', the mean outcome per bet from 10,000 bets.
sqrt(n)*abs(17-(-1))*sqrt(p_green*p_not_green)


# Neet to figured where does this comes from
# sqrt(n)*abs(17+1)*sqrt(p_green*p_not_green) =====================================================================
```

Incorrect submission
You are not providing a calculation that gives the correct answer. Don't forget to divide by the square root of the number of bets. 


## Exercise 8. American Roulette winnings per game are positive

What is the probability that your winnings are positive after betting on green 10,000 times?
Instructions
100 XP

    Execute the code that we wrote in previous exercises to determine the average and standard error.
    Use the pnorm function to determine the probability of winning more than $0.

```{r}
# We defined the average using the following code
avg <- 17*p_green + -1*p_not_green

# We defined standard error using this equation
se <- 1/sqrt(n) * (17 - -1)*sqrt(p_green*p_not_green)

# Given this average and standard error, determine the probability of winning more than $0. Print the result to the console.
1-pnorm(0, avg, se)
```


## Exercise 9. American Roulette Monte Carlo again

Create a Monte Carlo simulation that generates 10,000 outcomes of S , the average outcome from 10,000 bets on green.

Compute the average and standard deviation of the resulting list to confirm the results from previous exercises using the Central Limit Theorem.
Instructions
100 XP

    Use the replicate function to model 10,000 iterations of a series of 10,000 bets.
    Each iteration inside replicate should simulate 10,000 bets and determine the average outcome of those 10,000 bets. If you forget to take the mean, DataCamp will crash with a "Session Expired" error.
    Find the average of the 10,000 average outcomes. Print this value to the console.
    Compute the standard deviation of the 10,000 simulations. Print this value to the console.

```{r}
## Make sure you fully follow instructions, including printing values to the console and correctly running the `replicate` loop. If not, you may encounter "Session Expired" errors.

# The variable `n` specifies the number of independent bets on green
n <- 10000

# The variable `B` specifies the number of times we want the simulation to run
B <- 10000

# Use the `set.seed` function to make sure your answer matches the expected result after random number generation
set.seed(1)

# Generate a vector `S` that contains the the average outcomes of 10,000 bets modeled 10,000 times
S <- replicate(B, {
    10000*mean(sample(c(17, -1), n, prob=c(p_green, p_not_green), replace=T))
})




# Compute the average of `S`
mean(S)

# Compute the standard deviation of `S`
sd(S)
```
Incorrect submission
You are not providing a calculation that gives the correct answer. Remember to average the results of 10,000 simulations. Print the result to the console. 

```{r}
## Make sure you fully follow instructions, including printing values to the console and correctly running the `replicate` loop. If not, you may encounter "Session Expired" errors.

# The variable `n` specifies the number of independent bets on green
n <- 10000

# The variable `B` specifies the number of times we want the simulation to run
B <- 10000

# Use the `set.seed` function to make sure your answer matches the expected result after random number generation
set.seed(1)

# Generate a vector `S` that contains the the average outcomes of 10,000 bets modeled 10,000 times
S <- replicate(B, {
    mean(sample(c(17, -1), n, prob=c(p_green, p_not_green), replace=T))
})




# Compute the average of `S`
mean(S)

# Compute the standard deviation of `S`
sd(S)
```


## Exercise 10. American Roulette comparison

In a previous exercise, you found the probability of winning more than $0 after betting on green 10,000 times using the Central Limit Theorem. Then, you used a Monte Carlo simulation to model the average result of betting on green 10,000 times over 10,000 simulated series of bets.

What is the probability of winning more than $0 as estimated by your Monte Carlo simulation? The code to generate the vector S that contains the the average outcomes of 10,000 bets modeled 10,000 times has already been run for you.
Instructions
100 XP

    Calculate the probability of winning more than $0 in the Monte Carlo simulation from the previous exercise using the mean function.
    You do not need to run another simulation: the results of the simulation are in your workspace as the vector S.

```{r}
# Compute the proportion of outcomes in the vector 'S' where you won more than $0
mean(S>0)
```


## Exercise 11. American Roulette comparison analysis

The Monte Carlo result and the CLT approximation are now much closer than when we calculated the probability of winning for 100 bets on green. What could account for this difference?
Instructions
50 XP
Possible Answers

    We are now computing averages instead of sums.
    10,000 Monte Carlo simulations was not enough to provide a good estimate.
    The CLT works better when the sample size is larger.
    It is not closer. The difference is within rounding error.



## End of Assessment: The Central Limit Theorem

This is the end of the programming assignment for this section. Please DO NOT click through to additional assessments from this page. Please DO answer the question on this page. If you do click through, your scores may NOT be recorded.

Click on "Awesome" to get the "points" for this question and then return to the course on edX.

You can close this window and return to Data Science: Probability.
Answer the question
50XP
Possible Answers

    Awesome
    press
    1
    Nope
    press
    2









# Course  /  Section 3: Random Variables, Sampling Models, and the Central Limit Theorem  /  3.3 Assessment: Random Variables, Sampling Models, and the Central


# Questions 1 and 2: SAT testing


## Assessment due Jul 7, 2022 20:19 AWST

The SAT is a standardized college admissions test used in the United States. The following two multi-part questions will ask you some questions about SAT testing.

This is a 6-part question asking you to determine some probabilities of what happens when a student guessed for all of their answers on the SAT. Use the information below to inform your answers for the following questions.

An old version of the SAT college entrance exam had a -0.25 point penalty for every incorrect answer and awarded 1 point for a correct answer. The quantitative test consisted of 44 multiple-choice questions each with 5 answer choices. Suppose a student chooses answers by guessing for all questions on the test.
Question 1a
1/1 point (graded)
What is the probability of guessing correctly for one question?
correct

0.2
Loading

Explanation

The following code can be used to calculate the probability:

p <- 1/5 # one correct choice of 5 options
p

You have used 1 of 10 attempts 


##  Question 1b
1/1 point (graded)
What is the expected value of points for guessing on one question?
correct

0
Loading

Explanation

The following code can be used to calculate the expected value:

          a <- 1
b <- -0.25
mu <- a*p + b*(1-p)
mu
        

You have used 1 of 10 attempts Some

# So the assumption here is that the SAT test is Single Choice instead of multiple choice, also 1 point for correct answer and -0.25 for a incorrect answer
```{r}
1*(1/5) -0.25*(4/5)
```


##  Question 1c
1/1 point (graded)
What is the expected score of guessing on all 44 questions?
correct

0
Loading

Explanation

The following code can be used to calculate the expected score:

          n <- 44
n*mu
        

You have used 1 of 10 attempts Some


##  Question 1d
1/1 point (graded)
What is the standard error of guessing on all 44 questions?
correct

3.32
Loading

Explanation

The following code can be used to calculate the standard error:

          sigma <- sqrt(n) * abs(b-a) * sqrt(p*(1-p))
sigma
        

You have used 1 of 10 attempts Some


```{r}
se <- sqrt(44) * abs(1 - (-0.25))*sqrt(1/5*4/5)

se
```


##  Question 1e
1/1 point (graded)
Use the Central Limit Theorem to determine the probability that a guessing student scores 8 points or higher on the test.
correct

0.00793
Loading

Explanation

The following code can be used to calculate the probability:

          1-pnorm(8, mu, sigma)
        

You have used 1 of 10 attempts Some


```{r}
1 - pnorm(8, 0, se)     # 7.75  0.009727058
```


##  Question 1f
1/1 point (graded)

Set the seed to 21, then run a Monte Carlo simulation of 10,000 students guessing on the test.

(IMPORTANT! If you use R 3.6 or later, you will need to use the command set.seed(x, sample.kind = "Rounding") instead of set.seed(x). Your R version will be printed at the top of the Console window when you start RStudio.)
What is the probability that a guessing student scores 8 points or higher?
correct

0.008
Loading

Explanation

The following code can be used to calculate the probability:

          set.seed(21, sample.kind = "Rounding")
B <- 10000
n <- 44
p <- 0.2
tests <- replicate(B, {
  X <- sample(c(1, -0.25), n, replace = TRUE, prob = c(p, 1-p))
  sum(X)
})
mean(tests >= 8)
        

You have used 1 of 10 attempts Some


# My own code, I read the question statement and established my thoughts with this piece of code, people should definitelly study in the morning instead of night
```{r}
x <- 21

set.seed(x, sample.kind = "Rounding")


n <- 44
B <- 10000


S <- replicate(B, {
  sum(sample(c(1, -0.25), n, prob=c(1/5, 4/5), replace=T))
})



mean(S>8)

```



The SAT was recently changed to reduce the number of multiple choice options from 5 to 4 and also to eliminate the penalty for guessing.

In this two-part question, you'll explore how that affected the expected values for the test.


##  Question 2a
1/1 point (graded)

Suppose that the number of multiple choice options is 4 and that there is no penalty for guessing - that is, an incorrect question gives a score of 0.
What is the expected value of the score when guessing on this new test?
correct

11
Loading

Explanation

The following code can be used to calculate the expected value:

          p <- 1/4
a <- 1
b <- 0
n <- 44
mu <- n * a*p + b*(1-p)
mu
        

You have used 2 of 10 attempts Some


# Read carefully to properly understand teh question for us to solve, he's asking the expected value of the score in this test
```{r}
44*(1*(1/4)+0*(3/4))
```


##  Question 2b
1/1 point (graded)

Consider a range of correct answer probabilities p <- seq(0.25, 0.95, 0.05) representing a range of student skills.
What is the lowest p such that the probability of scoring over 35 exceeds 80%?
correct

0.85
Loading

Explanation

The following code can be used to calculate the value for p:

# ============================================================================================================
```{r}
p <- seq(0.25, 0.95, 0.05)


exp_val <- sapply(p, function(x){
  mu <- n * a*x + b*(1-x)
  sigma <- sqrt(n) * abs(b-a) * sqrt(x*(1-x))
  1-pnorm(35, mu, sigma)
})

min(p[which(exp_val > 0.8)])
```
# ============================================================================================================
        

You have used 4 of 10 attempts Some


```{r}
p <- seq(0.25, 0.95, 0.05)
# Whose guessing skill can make a 95% accuracy in a test, that must be God

p

mu <- 44*(1*p + 0*(1-p))
se <- sqrt(44)*abs(1-0)*sqrt(p*(1-p))

mu
se


pnorm(35, mu, se)
# Compute the standard error of 'Y', the mean outcome per bet from 10,000 bets.
# sqrt(n)*abs(17-(-1))*sqrt(p_green*p_not_green)
```









# Question 3: Betting on Roulette


Assessment due Jul 7, 2022 20:19 AWST

A casino offers a House Special bet on roulette, which is a bet on five pockets (00, 0, 1, 2, 3) out of 38 total pockets. The bet pays out 6 to 1. In other words, a losing bet yields -$1 and a successful bet yields $6. A gambler wants to know the chance of losing money if he places 500 bets on the roulette House Special.

The following 7-part question asks you to do some calculations related to this scenario.


##  Question 3a
1/1 point (graded)
What is the expected value of the payout for one bet?
correct

-0.0789
Loading

Explanation

The expected value can be calculated using the following code:

          
p <- 5/38
a <- 6
b <- -1
mu <- a*p + b*(1-p)
mu
        

You have used 1 of 10 attempts Some


```{r}
# What is the expected value of the payout for one bet?
# ==============================================================================================


6*5/38-1*(1-5/38)
```


##  Question 3b
1/1 point (graded)
What is the standard error of the payout for one bet?
correct

2.37
Loading

Explanation

The standard error can be calculated using the following code:

          sigma <- abs(b-a) * sqrt(p*(1-p))
sigma
        

You have used 1 of 10 attempts Some


```{r}
p <- 5/38



mu <- (6*p + (-1)*(1-p))
se <- sqrt(1)*abs(6-(-1))*sqrt(p*(1-p))


mu
se
```


##  Question 3c
1/1 point (graded)
What is the expected value of the average payout over 500 bets?

Remember there is a difference between expected value of the average and expected value of the sum.
correct

-0.0789
Loading

Explanation

The expected value can be calculated using the following code:

mu

You have used 3 of 10 attempts Some


```{r}
p <- 5/38
n <- 500


mu <- n*(6*p + 0*(1-p))
se <- sqrt(n)*abs(6+1)*sqrt(p*(1-p))


mu
se
```


##  Question 3d
1/1 point (graded)
What is the standard error of the average payout over 500 bets?

Remember there is a difference between the standard error of the average and standard error of the sum.
correct

0.106
Loading

Explanation

The standard error can be calculated using the following code:

          n <- 500
sigma/sqrt(n)
        

You have used 4 of 10 attempts Some


# ==================================================================================================================
# ==================================================================================================================
```{r}
p <- 5/38
n <- 500


mu <- 500*(6*p + (-1)*(1-p))
se <- abs(6+1)*sqrt(p*(1-p))/sqrt(n)
# ========================================================================================================


mu
se
```


##  Question 3e
1/1 point (graded)
What is the expected value of the sum of 500 bets?
correct

-39.5
Loading

Explanation

The expected value can be calculated using the following code:

          n*mu
        

You have used 2 of 10 attempts Some


##  Question 3f
1/1 point (graded)
What is the standard error of the sum of 500 bets?
correct

52.9
Loading

Explanation

The standard error can be calculated using the following code:

          sqrt(n) * sigma
        

##  Question 3g
1/1 point (graded)
Use pnorm() with the expected value of the sum and standard error of the sum to calculate the probability of losing money over 500 bets,
.
correct

0.772
Loading

Explanation

The standard error can be calculated using the following code:

          pnorm(0, n*mu, sqrt(n)*sigma)
        

You have used 1 of 10 attempts Some


```{r}
p <- 5/38
n <- 500


mu <- 500*(6*p + (-1)*(1-p))
se <- sqrt(n)*abs(6+1)*sqrt(p*(1-p))


pnorm(0, mu, se)
```









# Questions on Assessment: Random Variables and Sampling Models?




Ask your questions about the random variables, sampling models, and the CLT assessment here. Remember to search the discussion board before posting to see if someone else has asked the same thing before asking a new question! You're also encouraged to answer each other's questions to help further your own learning.

Some reminders:

        Please be specific in the title and body of your post regarding which question you're asking about to facilitate answering your question.
        Posting snippets of code is okay, but posting full code solutions is not.
        If you do post snippets of code, please format it as code for readability. If you're not sure how to do this, there are instructions in a pinned post in the general discussion forum.

Discussion: Random variables, sampling models, and the CLT assessment
Topic: Section 3 / Assessment: Random variables, sampling models, and the CLT
Filter:
Sort:

    unanswered question
    What question 2b is asking?
    What is the lowest p such that the probability of scoring over 35 exceeds 80%? Sorry, I tried to fetch each mu and se, then apply qnorm() function, but failed. Please help
        Following 
    1 comments
    discussion
    (S+n)/2
    Hi, I don't understand why S(the sum of the dollars of 10000 trials) is added to n(the 1000 people) and then divided by 2.















































```{r}
dim(cars)

#head(cars)


cor(cars$speed, cars$dist)

linearMod <- lm(dist ~ speed, data = cars)
#linearMod
print(linearMod)
```

```{r}
# capture model summary
modsummary <- summary(linearMod)
modsummary
```

```{r}
# model coefficient
summary(linearMod)$coefficients
print(" ")


# get the beta estimate for speed
beta.estimate <- summary(linearMod)$coefficients["speed", "Estimate"]
beta.estimate

# get the std.error for speed
std.error <- modsummary$coefficients["speed", "Std. Error"]
std.error

# calc t statistic
t_value <- beta.estimate/std.error
t_value

# calc p value
p_value <- 2*pt(-abs(t_value), df=nrow(cars)-ncol(cars))
p_value



nrow(cars)-ncol(cars)
```

























































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































